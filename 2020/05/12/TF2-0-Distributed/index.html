
<!DOCTYPE html>
<html lang="en">
    
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Jz Blog">
    <title>Notes for Distributed Tensorflow 2.0 - Jz Blog</title>
    <meta name="author" content="Joddiy Zhang">
    
    
        <link rel="icon" href="https://joddiy.cc/assets/images/favicon.ico">
    
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Joddiy Zhang","sameAs":["https://github.com/joddiy","https://scholar.google.com/citations?user=KH-xv38AAAAJ&hl=en&oi=sra","https://www.linkedin.com/in/joddiyzhang/"],"image":"14108933.jpeg"},"articleBody":"Overview\nParallelism\nModel parallelism\nData parallelism\nSynchronous vs asynchronous training\nTensorFlow Strategy\n\n\nModel Computation Pipelining\nGraph optimization with Grappler\nMetaOptimizer\nPruning optimizer\nFunction optimizer\nCommon subgraph elimination\nDebug stripper\nConstant folding optimizer\nShape optimizer\nAuto mixed precision optimizer\nPin to host optimizer\nArithmetic optimizer \nLayout optimizer\nRemapper optimizer\nLoop optimizer\nDependency optimizer\nMemory optimizer\nAutoparallel optimizer\nScoped allocator optimizer\n\n\nDistributed architecture (unfinished)\nParameter server\nRing-allreduce\nHorovod\n\n1 ParallelismTensorFlow’s basic dataflow graph model can be used in a variety of ways for machine learning applications. However, some neural networks models are so large they cannot fit in memory of a single device (GPU). Google’s Neural Machine Translation system is an example of such a network. \nSuch models need to be split over many devices, carrying out the training in parallel on the devices. There are three method to train a model in parallel on the devices, Model parallelism, Data parallelism, and Model Computation Pipelining. \nModel parallelism uses same data for every device but partitions the model among the devices. The graph is split as several sub-graphs, and assigns these sub-graphs to feasible devices to training. All devices use a same mini-batch to train.\nData parallelism uses the same model for every device, but train the model in each device using different training samples. Each device holds a entire model, but trains with partial samples from the mini-batch.\nModel Computation Pipelining pipelines the computation of seveal same models within one device by running a small number of concurrent steps. \n1.1 Model parallelismModel parallel training, where different portions of the model computation are done on different computational devices simultaneously for the same batch of examples, as the following figure:\n\nIt is challenging to get good performance, because some layers may depend on previous layers which leads to a long waiting time. However, if a model has some components which can run in parallel, it can use this method to improve the efficiency.\n1.2 Data parallelismIn modern deep learning, because the dataset is too big to be fit into the memory, we could only do stochastic gradient descent(SGD) for batches. The shortcoming of SGD is that the estimate of the gradients might not accurately represent the true gradients of using the full dataset. Therefore, it may take much longer to converge.\nData parallelism is a simple technique for speeding up SGD is to parallelize the computation of the gradient for a mini-batch across devices.\nEach device will independently compute the loss the gradients of small batches, the final estimate of the gradients is the weighted average of the gradients calculated from all the small batches(require communication).\nBy using data parallelism, the model can train on a large batch size. For example, the folloing figure shows a typical data parallelism, distributing 32 different images to each of the 256 GPUs running a single model. Together, the total mini-batch size for an iteration is 8,092 images (32 x 256) (Facebook: Training ImageNet in 1 Hour).\n\nMathematically, data parallelism is valid because:\n\n* m1+m2+⋯+mk&#x3D;n.\nWhen m1&#x3D;m2&#x3D;⋯&#x3D;mk&#x3D;nk, we could further have:\n\n1.2.1 Synchronous vs asynchronous trainingIn synchronous training(as following figure), all of the devices train their local model using different parts of data from a single (large) mini-batch. They then communicate their locally calculated gradients (directly or indirectly) to all devices. \nOnly after all devices have successfully computed and sent their gradients is the model updated. The updated model is then sent to all nodes along with splits from the next mini-batch. That is, devices train on non-overlapping splits (subset) of the mini-batch.\n\nIn asynchronous training, no device waits for updates to the model from any other device. The devices can run independently and share results as peers, or communicate through one or more central servers known as “parameter” servers. \n\nIn synchronous training, the parameter servers compute the latest up-to-date version of the model, and send it back to devices. In asynchronous training, parameter servers send gradients to devices that locally compute the new model.\n1.2.2 TensorFlow Strategytf.distribute.Strategy is a TensorFlow API to distribute training across multiple GPUs, multiple machines or TPUs. Using this API. \nIt intends to cover a number of use cases along different axes, including:\n\nsynchronous vs asynchronous training, \nhardware platform(multiple GPUs on one machine, or multiple machines in a network, or on Cloud TPUs).\n\nMirroredStrategytf.distribute.MirroredStrategy supports synchronous distributed training on multiple GPUs on one machine. \nIt creates one replica per GPU device. During training, one mini-batch is split into N parts and each part feed to one GPU device. \nEfficient all-reduce algorithms are used to communicate the variable updates across the devices. By default, it uses NVIDIA NCCL as the all-reduce implementation. Currently, tf.distribute.HierarchicalCopyAllReduce and tf.distribute.ReductionToOneDevice are two options other than tf.distribute.NcclAllReduce which is the default.\nCentralStorageStrategytf.distribute.experimental.CentralStorageStrategy does synchronous training as well. Variables are not mirrored, instead they are placed on the CPU and operations are replicated across all local GPUs. If there is only one GPU, all variables and operations will be placed on that GPU.\nMultiWorkerMirroredStrategytf.distribute.experimental.MultiWorkerMirroredStrategy is very similar to MirroredStrategy. It implements synchronous distributed training across multiple workers, each with potentially multiple GPUs. Similar to MirroredStrategy, it creates copies of all variables in the model on each device across all workers.\nIt uses CollectiveOps as the multi-worker all-reduce communication method used to keep variables in sync. \nA collective op is a single op in the TensorFlow graph which can automatically choose an all-reduce algorithm in the TensorFlow runtime according to hardware, network topology and tensor sizes.\nMultiWorkerMirroredStrategy currently allows two different implementations of collective ops：\n\nCollectiveCommunication.RING, ring algorithms for all-reduce and all-gather.\nCollectiveCommunication.NCCL, ncclAllReduce for all-reduce, and ring algorithms for all-gather.\n\nTPUStrategytf.distribute.experimental.TPUStrategy lets you run your TensorFlow training on Tensor Processing Units (TPUs). \nParameterServerStrategytf.distribute.experimental.ParameterServerStrategy supports parameter servers training on multiple machines. In this setup, some machines are designated as workers and some as parameter servers. Each variable of the model is placed on one parameter server. Computation is replicated across all GPUs of all the workers.\nOneDeviceStrategytf.distribute.OneDeviceStrategy runs on a single device. This strategy will place any variables created in its scope on the specified device. Input distributed through this strategy will be prefetched to the specified device. \nYou can use this strategy to test your code before switching to other strategies which actually distributes to multiple devices&#x2F;machines.\n1.3 Model Computation PipeliningAnother common way to get better utilization for training deep neural networks is to pipeline the computation of the model within the same devices, by running a small number of concurrent steps within the same set of devices.\nIt is somewhat similar to asynchronous data parallelism, except that the parallelism occurs within the same device(s), rather than replicating the computation graph on different devices. \n\nThis allows “filling in the gaps” where computation of a single batch of examples might not be able to fully utilize the full parallelism on all devices at all times during a single step.\n\n2 Graph optimization with GrapplerGraph is the default graph optimization system in the TF runtime to:\n\nAutomatically improve TF performance through graph simplifications &amp; high-level optimizations\nReduce device peak memory usage to enable larger models to run\nImprove hardware utilization by optimizing the mapping of graph nodes to compute resources\n\n\n2.1 MetaOptimizerMetaOptimizer is the top-level driver invoked by runtime or standalone tool, it Runs multiple sub-optimizers in a loop: (* &#x3D; not on by default):\n1234567891011121314151617181920i = 0while i &lt; config.meta_optimizer_iterations (default=2):\tPruning() # Remove nodes not in fanin of outputs, unused functions\tFunction() # Function specialization &amp; inlining, symbolic gradient inlining\tCommonSubgraphElimination() # dedup Subgraph\tDebugStripper ()* # Remove assert, print, check_numerics\tConstFold () # Constant folding and materialization\tShape() # Symbolic shape arithmetic\tAutoMixedPrecision() # Converts data types to float16 where applicable to improve performance\tPinToHost()* # Swaps small operations onto the CPU\tArithmetic() # Node deduping (CSE) &amp; arithmetic simplification\tif i==0: Layout() # Layout optimization for GPU\tRemapper() # Op fusion\tLoop() # Loop Invariant Node Motion*, Stack Push &amp; Dead Node Elimination\tDependency () # Prune/optimize control edges, NoOp/Identity node pruning\tif i==0: Memory() # Swap-out/Swap-in, Recompute*, split large nodes\tCustom() # Run registered custom optimizers (e.g. TensorRT)\tAutoParallel()* # Automatically parallelizes graphs by splitting along the batch dimension.Scopedallocator() # must run last: reduce data movement and to consolidate some operations.i += 1\n\n2.2 Pruning optimizerPrunes nodes that have no effect on the output from the graph. It is usually run first to reduce the size of the graph and speed up processing in other Grappler passes.\nTypically, this optimizer removes some StopGradient nodes and Identity nodes. For example, as the folloing figure, the Identity node is moved to a new branch.\n\n\n\n\n\n\n\n\n\n\n\n\n2.3 Function optimizerOptimizes the function library of a TensorFlow program and inlines function bodies to enable other inter-procedural optimizations.\n2.4 Common subgraph eliminationThis optimizer travels the entire graph to find and dedup same subgraphs.\n2.5 Debug stripperStrips nodes related to debugging operations such as tf.debugging.Assert, tf.debugging.check_numerics, and tf.print from the graph. \nThis optimizer is turned OFF by default.\n2.6 Constant folding optimizerStatically infers the value of tensors when possible by folding constant nodes in the graph and materializes the result using constants.\nThis optimizer has three methods: MaterializeShapes, FoldGraph, and SimplifyGraph.\nMaterializeShapes handles three nodes: Shape, Size, and Rank. Because these three nodes depend on the shape of input tensor, so it doesn’t have relationship with the value of the tensor. MaterializeShapes replaces these three node with Const node.\nFoldGraph folds the node whose all inputs are Const node, because its output can be pre-computed.\nSimplifyGraph handles: \n\nConstant push-down:\nAdd(c1, Add(x, c2)) &#x3D;&gt; Add(x, c1 + c2)\nConvND(c1 * x, c2) &#x3D;&gt; ConvND(x, c1 * c2)\nPartial constfold:\nAddN(c1, x, c2, y) &#x3D;&gt; AddN(c1 + c2, x, y),\nConcat([x, c1, c2, y]) &#x3D; Concat([x, Concat([c1, c2]), y)\nOperations with neutral &amp; absorbing elements:\nx * Ones(s) &#x3D;&gt; Identity(x), if shape(x) &#x3D;&#x3D; output_shape\nx * Ones(s) &#x3D;&gt; BroadcastTo(x, Shape(s)), if shape(s) &#x3D;&#x3D; output_shape\nSame for x + Zeros(s) , x &#x2F; Ones(s), x * Zeros(s) etc.\nZeros(s) - y &#x3D;&gt; Neg(y), if shape(y) &#x3D;&#x3D; output_shape\nOnes(s) &#x2F; y &#x3D;&gt; Recip(y) if shape(y) &#x3D;&#x3D; output_shape\n\n2.7 Shape optimizerOptimizes subgraphs that operate on shape and shape related information.\n2.8 Auto mixed precision optimizerConverts data types to float16 where applicable to improve performance. Currently applies only to GPUs.\n2.9 Pin to host optimizerSwaps small operations onto the CPU. \nThis optimizer is turned OFF by default.\n2.10 Arithmetic optimizerSimplifies arithmetic operations by eliminating common subexpressions and simplifying arithmetic statements.\n\nArithmetic simplifications\nFlattening: a+b+c+d &#x3D;&gt; AddN(a, b, c, d)\nHoisting: AddN(x * a, b * x, x * c) &#x3D;&gt; x * AddN(a+b+c)\nSimplification to reduce number of nodes:\nNumeric: x+x+x &#x3D;&gt; 3*x\nLogic: !(x &gt; y) &#x3D;&gt; x &lt;&#x3D; y\n\n\nBroadcast minimization\nExample: (matrix1 + scalar1) + (matrix2 + scalar2) &#x3D;&gt; (matrix1 + matrix2) + (scalar1 + scalar2)\nBetter use of intrinsics\nMatmul(Transpose(x), y) &#x3D;&gt; Matmul(x, y, transpose_x&#x3D;True)\nRemove redundant ops or op pairs\nTranspose(Transpose(x, perm), inverse_perm)\nBitCast(BitCast(x, dtype1), dtype2) &#x3D;&gt; BitCast(x, dtype2)\nPairs of elementwise involutions f(f(x)) &#x3D;&gt; x (Neg, Conj, Reciprocal, LogicalNot)\nRepeated Idempotent ops f(f(x)) &#x3D;&gt; f(x) (DeepCopy, Identity, CheckNumerics…)\nHoist chains of unary ops at Concat&#x2F;Split&#x2F;SplitV\nConcat([Exp(Cos(x)), Exp(Cos(y)), Exp(Cos(z))]) &#x3D;&gt; Exp(Cos(Concat([x, y, z])))\n[Exp(Cos(y)) for y in Split(x)] &#x3D;&gt; Split(Exp(Cos(x), num_splits)\n\n2.11 Layout optimizerOptimizes tensor layouts to execute data format dependent operations such as convolutions more efficiently.\nFor some nodes including AvgPool, Conv2D, etc, they supports two types of input format, NHWC and NCHW. But at the GPU runtime kernel, the NCHW data foramt is more efficient. This optimize adds a node to transfer the data format before these nodes.\nFor example, the following original graph with all ops in NHWC format\nPhase 1, expand by inserting conversion pairs:\nPhase 2, collapse adjacent conversion pairs:\nThis optimizer only runs at the first iteration.\n2.12 Remapper optimizerRemaps subgraphs onto more efficient implementations by replacing commonly occuring subgraphs with optimized fused monolithic kernels.\nReplaces commonly occurring subgraphs with optimized fused “monolithic” kernels:\n\nConv2D + BiasAdd + &lt;Activation&gt;\nConv2D + FusedBatchNorm + &lt;Activation&gt;\nConv2D + Squeeze + BiasAdd\nMatMul + BiasAdd + &lt;Activation&gt;\n\nSeveral performance advantages:\n\nCompletely eliminates Op scheduling overhead\nImproves temporal and spatial locality of data access\nE.g. MatMul is computed block-wise and bias and activation function can beapplied while data is still “hot” in cache\n\n2.13 Loop optimizerOptimizes the graph control flow by hoisting loop-invariant subgraphs out of loops and by removing redundant stack operations in loops. Also optimizes loops with statically known trip counts and removes statically known dead branches in conditionals.\n\nLoop Invariant Node Motion12345678910for (int i = 0; i &lt; n; i++) &#123;    x = y + z;    a[i] = 6 * i + x * x;&#125;// Motion the y+z and x*xx = y + z;t1 = x * x;for (int i = 0; i &lt; n; i++) &#123;    a[i] = 6 * i + t1;&#125;\nStackPush removal\nRemove StackPushes without consumers\nDead Branch Elimination\nDeduce loop trip count statically\nRemove loop for zero trip count\nRemove control flow nodes for trip count &#x3D;&#x3D; 1\n\n2.14 Dependency optimizerRemoves or rearranges control dependencies to shorten the critical path for a model step or enables other optimizations. Also removes nodes that are effectively no-ops such as Identity.\nA control edge is redundant iff there exists a path of length &gt; 1 from source to control target:\n2.15 Memory optimizerAnalyzes the graph to inspect the peak memory usage for each operation and inserts CPU-GPU memory copy operations for swapping GPU memory to CPU to reduce the peak memory usage.\nMemory optimization based on abstract interpretation\n\nSwap-out &#x2F; Swap-in optimization\nReduces device memory usage by swapping to host memory\nUses memory cost model to estimate peak memory\nUses op cost model to schedule Swap-In at (roughly) the right time\nRecomputation optimization (not on by default)\n\nPeak Memory Characterization:\nSwapping (start early):\nRecomputation:\nThis optimizer only runs at the first iteration.\n2.16 Autoparallel optimizerAutomatically parallelizes graphs by splitting along the batch dimension. \nAutoParallel is similar to the MirroredStrategy, however, the AutoParallel implements the parallel training by modifying the graph, instead of using replicated mulitiply models.\nFor example, the following graph shows a graph, the Dequeue node fetches some samples from the FIFO node, add with Const node and as the input of ApplyGradient node whose logic is var −= add * learning_rate.\nAfter applying AutoParallel with Replica&#x3D;2, the following graph shows, some nodes keep same(FIFO), some nodes are duplicated(add, Dequeue), and some new nodes are added(Div).\nThese two ApplyGradientDescent nodes can run parallelly to compute:\nvar −= add(replica−0)/2 * learning_rate(replica−0)​\tvar −= add(replica−1)/2 * learning_rate(replica−1)\nThis optimizer is turned OFF by default.\n2.17 Scoped allocator optimizerIntroduces scoped allocators to reduce data movement and to consolidate some operations.\nThis optimizer only runs at the last iteration.\n\n3 Distributed architecture (unfinished)\nReference\nhttps://www.oreilly.com/content/distributed-tensorflow/\nhttps://www.tensorflow.org/guide/distributed_training\nhttps://www.tensorflow.org/guide/graph_optimization\nhttps://web.stanford.edu/class/cs245/slides/TFGraphOptimizationsStanford.pdf\nAbadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., … &amp; Ghemawat, S. (2016). Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467.\nSergeev, A., &amp; Del Balso, M. (2018). Horovod: fast and easy distributed deep learning in TensorFlow. arXiv preprint arXiv:1802.05799.\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., … &amp; Kudlur, M. (2016). Tensorflow: A system for large-scale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16) (pp. 265-283).\nMirhoseini, A., Pham, H., Le, Q. V., Steiner, B., Larsen, R., Zhou, Y., … &amp; Dean, J. (2017, August). Device placement optimization with reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 2430-2439). JMLR. org.\nhttps://leimao.github.io/blog/Data-Parallelism-vs-Model-Paralelism/\nGoyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., … &amp; He, K. (2017). Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677.\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/meta_optimizer.cc\n\n","dateCreated":"2020-05-12T00:00:00+08:00","dateModified":"2020-08-12T10:26:00+08:00","datePublished":"2020-05-12T00:00:00+08:00","description":"Distributed Tensorflow","headline":"Notes for Distributed Tensorflow 2.0","image":["https://1.bp.blogspot.com/-cLxpo2BA8oI/XZMQrDSXefI/AAAAAAAAnhs/7FS6r94KFNQFzVt_8Ihx1iyTltt7if_xgCLcBGAsYHQ/s1600/TensorFlow%2B2.0%2BLogo.png","https://www.analyticsindiamag.com/wp-content/uploads/2019/03/tfk1.png"],"mainEntityOfPage":{"@type":"WebPage","@id":"https://joddiy.cc/2020/05/12/TF2-0-Distributed/"},"publisher":{"@type":"Organization","name":"Joddiy Zhang","sameAs":["https://github.com/joddiy","https://scholar.google.com/citations?user=KH-xv38AAAAJ&hl=en&oi=sra","https://www.linkedin.com/in/joddiyzhang/"],"image":"14108933.jpeg","logo":{"@type":"ImageObject","url":"14108933.jpeg"}},"url":"https://joddiy.cc/2020/05/12/TF2-0-Distributed/","keywords":"Tools, Deep Learning, Tensorflow","thumbnailUrl":"https://1.bp.blogspot.com/-cLxpo2BA8oI/XZMQrDSXefI/AAAAAAAAnhs/7FS6r94KFNQFzVt_8Ihx1iyTltt7if_xgCLcBGAsYHQ/s1600/TensorFlow%2B2.0%2BLogo.png"}</script>
    <meta name="description" content="Distributed Tensorflow">
<meta property="og:type" content="blog">
<meta property="og:title" content="Notes for Distributed Tensorflow 2.0">
<meta property="og:url" content="https://joddiy.cc/2020/05/12/TF2-0-Distributed/index.html">
<meta property="og:site_name" content="Jz Blog">
<meta property="og:description" content="Distributed Tensorflow">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://joddiy.cc/1.png">
<meta property="og:image" content="https://joddiy.cc/2.png">
<meta property="og:image" content="https://joddiy.cc/3.png">
<meta property="og:image" content="https://joddiy.cc/4.png">
<meta property="og:image" content="https://joddiy.cc/5.png">
<meta property="og:image" content="https://joddiy.cc/6.png">
<meta property="og:image" content="https://joddiy.cc/7.png">
<meta property="og:image" content="https://joddiy.cc/8.png">
<meta property="og:image" content="https://joddiy.cc/9.png">
<meta property="og:image" content="https://joddiy.cc/10.png">
<meta property="og:image" content="https://joddiy.cc/11.png">
<meta property="og:image" content="https://joddiy.cc/12.png">
<meta property="og:image" content="https://joddiy.cc/13.png">
<meta property="og:image" content="https://joddiy.cc/17.png">
<meta property="og:image" content="https://joddiy.cc/14.png">
<meta property="og:image" content="https://joddiy.cc/15.png">
<meta property="og:image" content="https://joddiy.cc/16.png">
<meta property="og:image" content="https://joddiy.cc/18.png">
<meta property="og:image" content="https://joddiy.cc/19.png">
<meta property="article:published_time" content="2020-05-11T16:00:00.000Z">
<meta property="article:modified_time" content="2020-08-12T02:26:00.000Z">
<meta property="article:author" content="Joddiy Zhang">
<meta property="article:tag" content="Tools">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="Tensorflow">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://joddiy.cc/1.png">
    
    
        
    
    
        <meta property="og:image" content="https://joddiy.cc/assets/images/14108933.jpeg"/>
    
    
        <meta property="og:image" content="https://1.bp.blogspot.com/-cLxpo2BA8oI/XZMQrDSXefI/AAAAAAAAnhs/7FS6r94KFNQFzVt_8Ihx1iyTltt7if_xgCLcBGAsYHQ/s1600/TensorFlow%2B2.0%2BLogo.png"/>
        <meta class="swiftype" name="image" data-type="enum" content="https://1.bp.blogspot.com/-cLxpo2BA8oI/XZMQrDSXefI/AAAAAAAAnhs/7FS6r94KFNQFzVt_8Ihx1iyTltt7if_xgCLcBGAsYHQ/s1600/TensorFlow%2B2.0%2BLogo.png"/>
    
    
        <meta property="og:image" content="https://www.analyticsindiamag.com/wp-content/uploads/2019/03/tfk1.png"/>
        <meta class="swiftype" name="image" data-type="enum" content="https://www.analyticsindiamag.com/wp-content/uploads/2019/03/tfk1.png"/>
    
    
    <!--STYLES-->
    
<link rel="stylesheet" href="/assets/css/style-x8blglznjjnb9pnnwui5zw4h43ysufmsh1b0omicawm4vhqcutzqavokgpne.min.css">

    <!--STYLES END-->
    

    

    
        
            
<link rel="stylesheet" href="/assets/css/gitalk.css">

        
    
</head>

    <body>
        <div id="blog">
            <!-- Define author's picture -->


    
        
            
        
    

<header id="header" data-behavior="4">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a
            class="header-title-link"
            href="/"
            aria-label=""
        >
            Jz Blog
        </a>
    </div>
    
        
            <a
                class="header-right-picture "
                href="#about"
                aria-label="Open the link: /#about"
            >
        
        
            <img class="header-picture" src="/assets/images/14108933.jpeg" alt="Author&#39;s picture"/>
        
        </a>
    
</header>

            <!-- Define author's picture -->



        
    

<nav id="sidebar" data-behavior="4">
    <div class="sidebar-container">
        
            <div class="sidebar-profile">
                <a
                    href="/#about"
                    aria-label="Read more about the author"
                >
                    <img class="sidebar-profile-picture" src="/assets/images/14108933.jpeg" alt="Author&#39;s picture"/>
                </a>
                <h4 class="sidebar-profile-name">Joddiy Zhang</h4>
                
                    <h5 class="sidebar-profile-bio"><p>author.bio</p>
</h5>
                
            </div>
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/"
                            
                            rel="noopener"
                            title="Home"
                        >
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-categories"
                            
                            rel="noopener"
                            title="Categories"
                        >
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-tags"
                            
                            rel="noopener"
                            title="Tags"
                        >
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Tags</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-archives"
                            
                            rel="noopener"
                            title="Archives"
                        >
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="#about"
                            
                            rel="noopener"
                            title="About"
                        >
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://github.com/joddiy"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="GitHub"
                        >
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://scholar.google.com/citations?user=KH-xv38AAAAJ&hl=en&oi=sra"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="Google Scholar"
                        >
                        <i class="sidebar-button-icon fab fa-google" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Google Scholar</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://www.linkedin.com/in/joddiyzhang/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="LinkedIn"
                        >
                        <i class="sidebar-button-icon fab fa-linkedin" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">LinkedIn</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
        <div class="post-header-cover
                    text-center
                    post-header-cover--partial"
             style="background-image:url('https://www.analyticsindiamag.com/wp-content/uploads/2019/03/tfk1.png');"
             data-behavior="4">
            
                <div class="post-header main-content-wrap text-center">
    
        <h1 class="post-title">
            Notes for Distributed Tensorflow 2.0
        </h1>
    
    
        <div class="post-meta">
    <time datetime="2020-05-12T00:00:00+08:00">
	
		    May 12, 2020
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Study-Notes/">Study Notes</a>


    
</div>

    
</div>

            
        </div>

            <div id="main" data-behavior="4"
                 class="hasCover
                        hasCoverMetaIn
                        ">
                
<article class="post">
    
    
    <div class="post-content markdown">
        <div class="main-content-wrap">
            <h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><ul>
<li>Parallelism</li>
<li>Model parallelism</li>
<li>Data parallelism<ol>
<li>Synchronous vs asynchronous training</li>
<li>TensorFlow Strategy</li>
</ol>
</li>
<li>Model Computation Pipelining</li>
<li>Graph optimization with Grappler<ul>
<li>MetaOptimizer</li>
<li>Pruning optimizer</li>
<li>Function optimizer</li>
<li>Common subgraph elimination</li>
<li>Debug stripper</li>
<li>Constant folding optimizer</li>
<li>Shape optimizer</li>
<li>Auto mixed precision optimizer</li>
<li>Pin to host optimizer</li>
<li>Arithmetic optimizer </li>
<li>Layout optimizer</li>
<li>Remapper optimizer</li>
<li>Loop optimizer</li>
<li>Dependency optimizer</li>
<li>Memory optimizer</li>
<li>Autoparallel optimizer</li>
<li>Scoped allocator optimizer</li>
</ul>
</li>
<li>Distributed architecture (unfinished)</li>
<li>Parameter server</li>
<li>Ring-allreduce</li>
<li>Horovod</li>
</ul>
<h2 id="1-Parallelism"><a href="#1-Parallelism" class="headerlink" title="1 Parallelism"></a>1 Parallelism</h2><p>TensorFlow’s basic dataflow graph model can be used in a variety of ways for machine learning applications. However, some neural networks models are so large they cannot fit in memory of a single device (GPU). Google’s Neural Machine Translation system is an example of such a network. </p>
<p>Such models need to be split over many devices, carrying out the training in parallel on the devices. There are three method to train a model in parallel on the devices, <code>Model parallelism</code>, <code>Data parallelism</code>, and <code>Model Computation Pipelining</code>. </p>
<p><code>Model parallelism</code> uses same data for every device but partitions the model among the devices. The graph is split as several sub-graphs, and assigns these sub-graphs to feasible devices to training. All devices use a same mini-batch to train.</p>
<p><code>Data parallelism</code> uses the same model for every device, but train the model in each device using different training samples. Each device holds a entire model, but trains with partial samples from the mini-batch.</p>
<p><code>Model Computation Pipelining</code> pipelines the computation of seveal same models within one device by running a small number of concurrent steps. </p>
<h3 id="1-1-Model-parallelism"><a href="#1-1-Model-parallelism" class="headerlink" title="1.1 Model parallelism"></a>1.1 Model parallelism</h3><p>Model parallel training, where different portions of the model computation are done on different computational devices simultaneously for the same batch of examples, as the following figure:</p>
<p><img src="/1.png" alt="1"></p>
<p>It is challenging to get good performance, because some layers may depend on previous layers which leads to a long waiting time. However, if a model has some components which can run in parallel, it can use this method to improve the efficiency.</p>
<h3 id="1-2-Data-parallelism"><a href="#1-2-Data-parallelism" class="headerlink" title="1.2 Data parallelism"></a>1.2 Data parallelism</h3><p>In modern deep learning, because the dataset is too big to be fit into the memory, we could only do stochastic gradient descent(SGD) for batches. The shortcoming of SGD is that the estimate of the gradients might not accurately represent the true gradients of using the full dataset. Therefore, it may take much longer to converge.</p>
<p>Data parallelism is a simple technique for speeding up SGD is to parallelize the computation of the gradient for a mini-batch across devices.</p>
<p>Each device will independently compute the loss the gradients of small batches, the final estimate of the gradients is the weighted average of the gradients calculated from all the small batches(require communication).</p>
<p>By using data parallelism, the model can train on a large batch size. For example, the folloing figure shows a typical data parallelism, distributing 32 different images to each of the 256 GPUs running a single model. Together, the total mini-batch size for an iteration is 8,092 images (32 x 256) (Facebook: Training ImageNet in 1 Hour).</p>
<p><img src="/2.png" alt="2"></p>
<p>Mathematically, data parallelism is valid because:</p>
<p><img src="/3.png" alt="3"></p>
<p>* m1+m2+⋯+mk&#x3D;n.</p>
<p>When m1&#x3D;m2&#x3D;⋯&#x3D;mk&#x3D;nk, we could further have:</p>
<p><img src="/4.png" alt="4"></p>
<h4 id="1-2-1-Synchronous-vs-asynchronous-training"><a href="#1-2-1-Synchronous-vs-asynchronous-training" class="headerlink" title="1.2.1 Synchronous vs asynchronous training"></a>1.2.1 Synchronous vs asynchronous training</h4><p>In synchronous training(as following figure), all of the devices train their local model using different parts of data from a single (large) mini-batch. They then communicate their locally calculated gradients (directly or indirectly) to all devices. </p>
<p>Only after all devices have successfully computed and sent their gradients is the model updated. The updated model is then sent to all nodes along with splits from the next mini-batch. That is, devices train on non-overlapping splits (subset) of the mini-batch.</p>
<p><img src="/5.png" alt="5"></p>
<p>In asynchronous training, no device waits for updates to the model from any other device. The devices can run independently and share results as peers, or communicate through one or more central servers known as “parameter” servers. </p>
<p><img src="/6.png" alt="6"></p>
<p>In synchronous training, the parameter servers compute the latest up-to-date version of the model, and send it back to devices. In asynchronous training, parameter servers send gradients to devices that locally compute the new model.</p>
<h4 id="1-2-2-TensorFlow-Strategy"><a href="#1-2-2-TensorFlow-Strategy" class="headerlink" title="1.2.2 TensorFlow Strategy"></a>1.2.2 TensorFlow Strategy</h4><p><code>tf.distribute.Strategy</code> is a TensorFlow API to distribute training across multiple GPUs, multiple machines or TPUs. Using this API. </p>
<p>It intends to cover a number of use cases along different axes, including:</p>
<ul>
<li>synchronous vs asynchronous training, </li>
<li>hardware platform(multiple GPUs on one machine, or multiple machines in a network, or on Cloud TPUs).</li>
</ul>
<h5 id="MirroredStrategy"><a href="#MirroredStrategy" class="headerlink" title="MirroredStrategy"></a>MirroredStrategy</h5><p>tf.distribute.MirroredStrategy supports synchronous distributed training on <strong>multiple GPUs on one machine.</strong> </p>
<p>It creates one replica per GPU device. During training, one mini-batch is split into N parts and each part feed to one GPU device. </p>
<p>Efficient <strong>all-reduce</strong> algorithms are used to communicate the variable updates across the devices. By default, it uses NVIDIA NCCL as the all-reduce implementation. Currently, tf.distribute.HierarchicalCopyAllReduce and tf.distribute.ReductionToOneDevice are two options other than tf.distribute.NcclAllReduce which is the default.</p>
<h5 id="CentralStorageStrategy"><a href="#CentralStorageStrategy" class="headerlink" title="CentralStorageStrategy"></a>CentralStorageStrategy</h5><p>tf.distribute.experimental.CentralStorageStrategy does synchronous training as well. <strong>Variables are not mirrored, instead they are placed on the CPU</strong> and operations are replicated across all local GPUs. If there is only one GPU, all variables and operations will be placed on that GPU.</p>
<h5 id="MultiWorkerMirroredStrategy"><a href="#MultiWorkerMirroredStrategy" class="headerlink" title="MultiWorkerMirroredStrategy"></a>MultiWorkerMirroredStrategy</h5><p>tf.distribute.experimental.MultiWorkerMirroredStrategy is very similar to MirroredStrategy. It implements <strong>synchronous distributed training across multiple workers, each with potentially multiple GPUs</strong>. Similar to MirroredStrategy, it creates copies of all variables in the model on each device across all workers.</p>
<p>It uses <strong>CollectiveOps</strong> as the multi-worker all-reduce communication method used to keep variables in sync. </p>
<p>A collective op is a single op in the TensorFlow graph which can automatically choose an all-reduce algorithm in the TensorFlow runtime according to hardware, network topology and tensor sizes.</p>
<p>MultiWorkerMirroredStrategy currently allows two different implementations of collective ops：</p>
<ul>
<li>CollectiveCommunication.RING, ring algorithms for all-reduce and all-gather.</li>
<li>CollectiveCommunication.NCCL, ncclAllReduce for all-reduce, and ring algorithms for all-gather.</li>
</ul>
<h5 id="TPUStrategy"><a href="#TPUStrategy" class="headerlink" title="TPUStrategy"></a>TPUStrategy</h5><p>tf.distribute.experimental.TPUStrategy lets you run your TensorFlow training on Tensor Processing Units (TPUs). </p>
<h5 id="ParameterServerStrategy"><a href="#ParameterServerStrategy" class="headerlink" title="ParameterServerStrategy"></a>ParameterServerStrategy</h5><p>tf.distribute.experimental.ParameterServerStrategy supports parameter servers training on multiple machines. In this setup, some machines are designated as workers and some as parameter servers. <strong>Each variable of the model is placed on one parameter server. Computation is replicated across all GPUs of all the workers.</strong></p>
<h5 id="OneDeviceStrategy"><a href="#OneDeviceStrategy" class="headerlink" title="OneDeviceStrategy"></a>OneDeviceStrategy</h5><p>tf.distribute.OneDeviceStrategy runs on a <strong>single device</strong>. This strategy will place any variables created in its scope on the specified device. Input distributed through this strategy will be prefetched to the specified device. </p>
<p>You can use this strategy to test your code before switching to other strategies which actually distributes to multiple devices&#x2F;machines.</p>
<h3 id="1-3-Model-Computation-Pipelining"><a href="#1-3-Model-Computation-Pipelining" class="headerlink" title="1.3 Model Computation Pipelining"></a>1.3 Model Computation Pipelining</h3><p>Another common way to get better utilization for training deep neural networks is to pipeline the computation of the model within the same devices, by running a small number of concurrent steps within the same set of devices.</p>
<p>It is somewhat similar to asynchronous data parallelism, except that the parallelism occurs within the same device(s), rather than replicating the computation graph on different devices. </p>
<p><img src="/7.png" alt="7"></p>
<p>This allows “filling in the gaps” where computation of a single batch of examples might not be able to fully utilize the full parallelism on all devices at all times during a single step.</p>
<hr>
<h2 id="2-Graph-optimization-with-Grappler"><a href="#2-Graph-optimization-with-Grappler" class="headerlink" title="2 Graph optimization with Grappler"></a>2 Graph optimization with Grappler</h2><p>Graph is the default graph optimization system in the TF runtime to:</p>
<ul>
<li><strong>Automatically improve TF performance</strong> through graph simplifications &amp; high-level optimizations</li>
<li><strong>Reduce device peak memory usage</strong> to enable larger models to run</li>
<li><strong>Improve hardware utilization</strong> by optimizing the mapping of graph nodes to compute resources</li>
</ul>
<p><img src="/8.png" alt="8"></p>
<h3 id="2-1-MetaOptimizer"><a href="#2-1-MetaOptimizer" class="headerlink" title="2.1 MetaOptimizer"></a>2.1 MetaOptimizer</h3><p>MetaOptimizer is the top-level driver invoked by runtime or standalone tool, it Runs multiple sub-optimizers in a loop: (* &#x3D; not on by default):</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> i &lt; config.meta_optimizer_iterations (<span class="keyword">default</span>=<span class="number">2</span>):</span><br><span class="line">	Pruning() # Remove nodes not in fanin of outputs, unused functions</span><br><span class="line">	Function() # Function specialization &amp; inlining, symbolic gradient inlining</span><br><span class="line">	CommonSubgraphElimination() <span class="meta"># dedup Subgraph</span></span><br><span class="line">	DebugStripper ()* # Remove assert, print, check_numerics</span><br><span class="line">	ConstFold () # Constant folding and materialization</span><br><span class="line">	Shape() # Symbolic shape arithmetic</span><br><span class="line">	AutoMixedPrecision() # Converts data types to float16 where applicable to improve performance</span><br><span class="line">	PinToHost()* # Swaps small operations onto the CPU</span><br><span class="line">	Arithmetic() # Node deduping (CSE) &amp; arithmetic simplification</span><br><span class="line">	<span class="keyword">if</span> i==<span class="number">0</span>: Layout() # Layout optimization <span class="keyword">for</span> GPU</span><br><span class="line">	Remapper() # Op fusion</span><br><span class="line">	Loop() # Loop Invariant Node Motion*, Stack Push &amp; Dead Node Elimination</span><br><span class="line">	Dependency () # Prune/optimize control edges, NoOp/Identity node pruning</span><br><span class="line">	<span class="keyword">if</span> i==<span class="number">0</span>: Memory() # Swap-out/Swap-in, Recompute*, split large nodes</span><br><span class="line">	Custom() # Run registered custom optimizers (e.g. TensorRT)</span><br><span class="line">	AutoParallel()* # Automatically parallelizes graphs by splitting along the batch dimension.</span><br><span class="line">Scopedallocator() <span class="meta"># must run last: reduce data movement and to consolidate some operations.</span></span><br><span class="line">i += <span class="number">1</span></span><br></pre></td></tr></table></figure>

<h3 id="2-2-Pruning-optimizer"><a href="#2-2-Pruning-optimizer" class="headerlink" title="2.2 Pruning optimizer"></a>2.2 Pruning optimizer</h3><p>Prunes nodes that have no effect on the output from the graph. It is usually run first to reduce the size of the graph and speed up processing in other Grappler passes.</p>
<p>Typically, this optimizer removes some <code>StopGradient</code> nodes and <code>Identity</code> nodes. For example, as the folloing figure, the Identity node is moved to a new branch.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td><img src="/9.png" alt="9"></td>
<td><img src="/10.png" alt="10"></td>
</tr>
</tbody></table>
<h3 id="2-3-Function-optimizer"><a href="#2-3-Function-optimizer" class="headerlink" title="2.3 Function optimizer"></a>2.3 Function optimizer</h3><p>Optimizes the function library of a TensorFlow program and inlines function bodies to enable other inter-procedural optimizations.</p>
<h3 id="2-4-Common-subgraph-elimination"><a href="#2-4-Common-subgraph-elimination" class="headerlink" title="2.4 Common subgraph elimination"></a>2.4 Common subgraph elimination</h3><p>This optimizer travels the entire graph to find and dedup same subgraphs.</p>
<h3 id="2-5-Debug-stripper"><a href="#2-5-Debug-stripper" class="headerlink" title="2.5 Debug stripper"></a>2.5 Debug stripper</h3><p>Strips nodes related to debugging operations such as tf.debugging.Assert, tf.debugging.check_numerics, and tf.print from the graph. </p>
<p>This optimizer is turned OFF by default.</p>
<h3 id="2-6-Constant-folding-optimizer"><a href="#2-6-Constant-folding-optimizer" class="headerlink" title="2.6 Constant folding optimizer"></a>2.6 Constant folding optimizer</h3><p>Statically infers the value of tensors when possible by folding constant nodes in the graph and materializes the result using constants.</p>
<p>This optimizer has three methods: <code>MaterializeShapes</code>, <code>FoldGraph</code>, and <code>SimplifyGraph</code>.</p>
<p><code>MaterializeShapes</code> handles three nodes: Shape, Size, and Rank. Because these three nodes depend on the shape of input tensor, so it doesn’t have relationship with the value of the tensor. <code>MaterializeShapes</code> replaces these three node with <code>Const</code> node.</p>
<p><code>FoldGraph</code> folds the node whose all inputs are <code>Const</code> node, because its output can be pre-computed.</p>
<p><code>SimplifyGraph</code> handles: </p>
<ul>
<li>Constant push-down:</li>
<li>Add(c1, Add(x, c2)) &#x3D;&gt; Add(x, c1 + c2)</li>
<li>ConvND(c1 * x, c2) &#x3D;&gt; ConvND(x, c1 * c2)</li>
<li>Partial constfold:</li>
<li>AddN(c1, x, c2, y) &#x3D;&gt; AddN(c1 + c2, x, y),</li>
<li>Concat([x, c1, c2, y]) &#x3D; Concat([x, Concat([c1, c2]), y)</li>
<li>Operations with neutral &amp; absorbing elements:</li>
<li>x * Ones(s) &#x3D;&gt; Identity(x), if shape(x) &#x3D;&#x3D; output_shape</li>
<li>x * Ones(s) &#x3D;&gt; BroadcastTo(x, Shape(s)), if shape(s) &#x3D;&#x3D; output_shape</li>
<li>Same for x + Zeros(s) , x &#x2F; Ones(s), x * Zeros(s) etc.</li>
<li>Zeros(s) - y &#x3D;&gt; Neg(y), if shape(y) &#x3D;&#x3D; output_shape</li>
<li>Ones(s) &#x2F; y &#x3D;&gt; Recip(y) if shape(y) &#x3D;&#x3D; output_shape</li>
</ul>
<h3 id="2-7-Shape-optimizer"><a href="#2-7-Shape-optimizer" class="headerlink" title="2.7 Shape optimizer"></a>2.7 Shape optimizer</h3><p>Optimizes subgraphs that operate on shape and shape related information.</p>
<h3 id="2-8-Auto-mixed-precision-optimizer"><a href="#2-8-Auto-mixed-precision-optimizer" class="headerlink" title="2.8 Auto mixed precision optimizer"></a>2.8 Auto mixed precision optimizer</h3><p>Converts data types to float16 where applicable to improve performance. Currently applies only to GPUs.</p>
<h3 id="2-9-Pin-to-host-optimizer"><a href="#2-9-Pin-to-host-optimizer" class="headerlink" title="2.9 Pin to host optimizer"></a>2.9 Pin to host optimizer</h3><p>Swaps small operations onto the CPU. </p>
<p>This optimizer is turned OFF by default.</p>
<h3 id="2-10-Arithmetic-optimizer"><a href="#2-10-Arithmetic-optimizer" class="headerlink" title="2.10 Arithmetic optimizer"></a>2.10 Arithmetic optimizer</h3><p>Simplifies arithmetic operations by eliminating common subexpressions and simplifying arithmetic statements.</p>
<ul>
<li>Arithmetic simplifications</li>
<li>Flattening: a+b+c+d &#x3D;&gt; AddN(a, b, c, d)</li>
<li>Hoisting: AddN(x * a, b * x, x * c) &#x3D;&gt; x * AddN(a+b+c)</li>
<li>Simplification to reduce number of nodes:<ul>
<li>Numeric: x+x+x &#x3D;&gt; 3*x</li>
<li>Logic: !(x &gt; y) &#x3D;&gt; x &lt;&#x3D; y</li>
</ul>
</li>
<li>Broadcast minimization</li>
<li>Example: (matrix1 + scalar1) + (matrix2 + scalar2) &#x3D;&gt; (matrix1 + matrix2) + (scalar1 + scalar2)</li>
<li>Better use of intrinsics</li>
<li>Matmul(Transpose(x), y) &#x3D;&gt; Matmul(x, y, transpose_x&#x3D;True)</li>
<li>Remove redundant ops or op pairs</li>
<li>Transpose(Transpose(x, perm), inverse_perm)</li>
<li>BitCast(BitCast(x, dtype1), dtype2) &#x3D;&gt; BitCast(x, dtype2)</li>
<li>Pairs of elementwise involutions f(f(x)) &#x3D;&gt; x (Neg, Conj, Reciprocal, LogicalNot)</li>
<li>Repeated Idempotent ops f(f(x)) &#x3D;&gt; f(x) (DeepCopy, Identity, CheckNumerics…)</li>
<li>Hoist chains of unary ops at Concat&#x2F;Split&#x2F;SplitV</li>
<li>Concat([Exp(Cos(x)), Exp(Cos(y)), Exp(Cos(z))]) &#x3D;&gt; Exp(Cos(Concat([x, y, z])))</li>
<li>[Exp(Cos(y)) for y in Split(x)] &#x3D;&gt; Split(Exp(Cos(x), num_splits)</li>
</ul>
<h3 id="2-11-Layout-optimizer"><a href="#2-11-Layout-optimizer" class="headerlink" title="2.11 Layout optimizer"></a>2.11 Layout optimizer</h3><p>Optimizes tensor layouts to execute data format dependent operations such as convolutions more efficiently.</p>
<p>For some nodes including AvgPool, Conv2D, etc, they supports two types of input format, <code>NHWC</code> and <code>NCHW</code>. But at the GPU runtime kernel, the <code>NCHW</code> data foramt is more efficient. This optimize adds a node to transfer the data format before these nodes.</p>
<p>For example, the following original graph with all ops in NHWC format<br><img src="/11.png" alt="11"></p>
<p>Phase 1, expand by inserting conversion pairs:<br><img src="/12.png" alt="12"></p>
<p>Phase 2, collapse adjacent conversion pairs:<br><img src="/13.png" alt="13"></p>
<p>This optimizer only runs at the first iteration.</p>
<h3 id="2-12-Remapper-optimizer"><a href="#2-12-Remapper-optimizer" class="headerlink" title="2.12 Remapper optimizer"></a>2.12 Remapper optimizer</h3><p>Remaps subgraphs onto more efficient implementations by replacing commonly occuring subgraphs with optimized fused monolithic kernels.</p>
<p>Replaces commonly occurring subgraphs with optimized fused “monolithic” kernels:</p>
<ul>
<li>Conv2D + BiasAdd + &lt;Activation&gt;</li>
<li>Conv2D + FusedBatchNorm + &lt;Activation&gt;</li>
<li>Conv2D + Squeeze + BiasAdd</li>
<li>MatMul + BiasAdd + &lt;Activation&gt;</li>
</ul>
<p>Several performance advantages:</p>
<ul>
<li>Completely eliminates Op scheduling overhead</li>
<li>Improves temporal and spatial locality of data access</li>
<li>E.g. MatMul is computed block-wise and bias and activation function can be<br>applied while data is still “hot” in cache</li>
</ul>
<h3 id="2-13-Loop-optimizer"><a href="#2-13-Loop-optimizer" class="headerlink" title="2.13 Loop optimizer"></a>2.13 Loop optimizer</h3><p>Optimizes the graph control flow by hoisting loop-invariant subgraphs out of loops and by removing redundant stack operations in loops. Also optimizes loops with statically known trip counts and removes statically known dead branches in conditionals.</p>
<ul>
<li>Loop Invariant Node Motion<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">    x = y + z;</span><br><span class="line">    a[i] = <span class="number">6</span> * i + x * x;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Motion the y+z and x*x</span></span><br><span class="line">x = y + z;</span><br><span class="line">t1 = x * x;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">    a[i] = <span class="number">6</span> * i + t1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>StackPush removal</li>
<li>Remove StackPushes without consumers</li>
<li>Dead Branch Elimination</li>
<li>Deduce loop trip count statically</li>
<li>Remove loop for zero trip count</li>
<li>Remove control flow nodes for trip count &#x3D;&#x3D; 1</li>
</ul>
<h3 id="2-14-Dependency-optimizer"><a href="#2-14-Dependency-optimizer" class="headerlink" title="2.14 Dependency optimizer"></a>2.14 Dependency optimizer</h3><p>Removes or rearranges <strong>control dependencies</strong> to shorten the critical path for a model step or enables other optimizations. Also removes nodes that are effectively no-ops such as Identity.</p>
<p>A control edge is redundant iff there exists a path of length &gt; 1 from source to control target:<br><img src="/17.png" alt="17"></p>
<h3 id="2-15-Memory-optimizer"><a href="#2-15-Memory-optimizer" class="headerlink" title="2.15 Memory optimizer"></a>2.15 Memory optimizer</h3><p>Analyzes the graph to inspect the peak memory usage for each operation and inserts CPU-GPU memory copy operations for swapping GPU memory to CPU to reduce the peak memory usage.</p>
<p>Memory optimization based on abstract interpretation</p>
<ul>
<li>Swap-out &#x2F; Swap-in optimization</li>
<li>Reduces device memory usage by swapping to host memory</li>
<li>Uses memory cost model to estimate peak memory</li>
<li>Uses op cost model to schedule Swap-In at (roughly) the right time</li>
<li>Recomputation optimization (not on by default)</li>
</ul>
<p>Peak Memory Characterization:<br><img src="/14.png" alt="14"></p>
<p>Swapping (start early):<br><img src="/15.png" alt="15"></p>
<p>Recomputation:<br><img src="/16.png" alt="16"></p>
<p>This optimizer only runs at the first iteration.</p>
<h3 id="2-16-Autoparallel-optimizer"><a href="#2-16-Autoparallel-optimizer" class="headerlink" title="2.16 Autoparallel optimizer"></a>2.16 Autoparallel optimizer</h3><p>Automatically parallelizes graphs by splitting along the batch dimension. </p>
<p>AutoParallel is similar to the MirroredStrategy, however, the AutoParallel implements the parallel training by modifying the graph, instead of using replicated mulitiply models.</p>
<p>For example, the following graph shows a graph, the Dequeue node fetches some samples from the FIFO node, add with Const node and as the input of ApplyGradient node whose logic is <code>var −= add * learning_rate</code>.<br><img src="/18.png" alt="18"></p>
<p>After applying AutoParallel with Replica&#x3D;2, the following graph shows, some nodes keep same(FIFO), some nodes are duplicated(add, Dequeue), and some new nodes are added(Div).<br><img src="/19.png" alt="19"></p>
<p>These two ApplyGradientDescent nodes can run parallelly to compute:</p>
<p><code>var −= add(replica−0)/2 * learning_rate(replica−0)</code><br>​	<br><code>var −= add(replica−1)/2 * learning_rate(replica−1)</code></p>
<p>This optimizer is turned OFF by default.</p>
<h3 id="2-17-Scoped-allocator-optimizer"><a href="#2-17-Scoped-allocator-optimizer" class="headerlink" title="2.17 Scoped allocator optimizer"></a>2.17 Scoped allocator optimizer</h3><p>Introduces scoped allocators to reduce data movement and to consolidate some operations.</p>
<p>This optimizer only runs at the last iteration.</p>
<hr>
<h2 id="3-Distributed-architecture-unfinished"><a href="#3-Distributed-architecture-unfinished" class="headerlink" title="3 Distributed architecture (unfinished)"></a>3 Distributed architecture (unfinished)</h2><hr>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.oreilly.com/content/distributed-tensorflow/">https://www.oreilly.com/content/distributed-tensorflow/</a></li>
<li><a target="_blank" rel="noopener" href="https://www.tensorflow.org/guide/distributed_training">https://www.tensorflow.org/guide/distributed_training</a></li>
<li><a target="_blank" rel="noopener" href="https://www.tensorflow.org/guide/graph_optimization">https://www.tensorflow.org/guide/graph_optimization</a></li>
<li><a target="_blank" rel="noopener" href="https://web.stanford.edu/class/cs245/slides/TFGraphOptimizationsStanford.pdf">https://web.stanford.edu/class/cs245/slides/TFGraphOptimizationsStanford.pdf</a></li>
<li>Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., … &amp; Ghemawat, S. (2016). Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467.</li>
<li>Sergeev, A., &amp; Del Balso, M. (2018). Horovod: fast and easy distributed deep learning in TensorFlow. arXiv preprint arXiv:1802.05799.</li>
<li>Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., … &amp; Kudlur, M. (2016). Tensorflow: A system for large-scale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16) (pp. 265-283).</li>
<li>Mirhoseini, A., Pham, H., Le, Q. V., Steiner, B., Larsen, R., Zhou, Y., … &amp; Dean, J. (2017, August). Device placement optimization with reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 2430-2439). JMLR. org.</li>
<li><a target="_blank" rel="noopener" href="https://leimao.github.io/blog/Data-Parallelism-vs-Model-Paralelism/">https://leimao.github.io/blog/Data-Parallelism-vs-Model-Paralelism/</a></li>
<li>Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., … &amp; He, K. (2017). Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677.</li>
<li><a target="_blank" rel="noopener" href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/meta_optimizer.cc">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/meta_optimizer.cc</a></li>
</ol>

            


        </div>
    </div>
    <div id="post-footer" class="post-footer main-content-wrap">
        
            <div class="post-footer-tags">
                <span class="text-color-light text-small">TAGGED IN</span><br/>
                
    <a class="tag tag--primary tag--small t-none-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a> <a class="tag tag--primary tag--small t-none-link" href="/tags/Tensorflow/" rel="tag">Tensorflow</a> <a class="tag tag--primary tag--small t-none-link" href="/tags/Tools/" rel="tag">Tools</a>

            </div>
        
        
            <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2020/05/14/SINGA-ONNX/"
                    data-tooltip="SINGA-ONNX"
                    aria-label="PREVIOUS: SINGA-ONNX"
                >
                    
                        <i class="fa fa-angle-left" aria-hidden="true"></i>
                        <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                    </a>
            </li>
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2020/05/11/C-Plus-Plus/"
                    data-tooltip="C++ Notes"
                    aria-label="NEXT: C++ Notes"
                >
                    
                        <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                        <i class="fa fa-angle-right" aria-hidden="true"></i>
                    </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a
                class="post-action-btn btn btn--default btn-open-shareoptions"
                href="#btn-open-shareoptions"
                aria-label="Share this post"
            >
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://joddiy.cc/2020/05/12/TF2-0-Distributed/"
                    title="Share on Facebook"
                    aria-label="Share on Facebook"
                >
                    <i class="fab fa-facebook" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://twitter.com/intent/tweet?text=https://joddiy.cc/2020/05/12/TF2-0-Distributed/"
                    title="Share on Twitter"
                    aria-label="Share on Twitter"
                >
                    <i class="fab fa-twitter" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://plus.google.com/share?url=https://joddiy.cc/2020/05/12/TF2-0-Distributed/"
                    title="Share on Google+"
                    aria-label="Share on Google+"
                >
                    <i class="fab fa-google-plus" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
                <li class="post-action">
                    <a
                        class="post-action-btn btn btn--default"
                        href="#gitalk"
                        aria-label="Leave a comment"
                    >
                        <i class="fa fa-comment"></i>
                    </a>
                </li>
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


        
        
            
                <div id="gitalk"></div>

            
        
    </div>
</article>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2024 Joddiy Zhang. All Rights Reserved.
    </span>
</footer>

            </div>
            
                <div id="bottom-bar" class="post-bottom-bar" data-behavior="4">
                    <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2020/05/14/SINGA-ONNX/"
                    data-tooltip="SINGA-ONNX"
                    aria-label="PREVIOUS: SINGA-ONNX"
                >
                    
                        <i class="fa fa-angle-left" aria-hidden="true"></i>
                        <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                    </a>
            </li>
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2020/05/11/C-Plus-Plus/"
                    data-tooltip="C++ Notes"
                    aria-label="NEXT: C++ Notes"
                >
                    
                        <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                        <i class="fa fa-angle-right" aria-hidden="true"></i>
                    </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a
                class="post-action-btn btn btn--default btn-open-shareoptions"
                href="#btn-open-shareoptions"
                aria-label="Share this post"
            >
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://joddiy.cc/2020/05/12/TF2-0-Distributed/"
                    title="Share on Facebook"
                    aria-label="Share on Facebook"
                >
                    <i class="fab fa-facebook" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://twitter.com/intent/tweet?text=https://joddiy.cc/2020/05/12/TF2-0-Distributed/"
                    title="Share on Twitter"
                    aria-label="Share on Twitter"
                >
                    <i class="fab fa-twitter" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://plus.google.com/share?url=https://joddiy.cc/2020/05/12/TF2-0-Distributed/"
                    title="Share on Google+"
                    aria-label="Share on Google+"
                >
                    <i class="fab fa-google-plus" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
                <li class="post-action">
                    <a
                        class="post-action-btn btn btn--default"
                        href="#gitalk"
                        aria-label="Leave a comment"
                    >
                        <i class="fa fa-comment"></i>
                    </a>
                </li>
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


                </div>
                
    <div id="share-options-bar" class="share-options-bar" data-behavior="4">
        <i id="btn-close-shareoptions" class="fa fa-times"></i>
        <ul class="share-options">
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://www.facebook.com/sharer/sharer.php?u=https://joddiy.cc/2020/05/12/TF2-0-Distributed/"
                        aria-label="Share on Facebook"
                    >
                        <i class="fab fa-facebook" aria-hidden="true"></i><span>Share on Facebook</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://twitter.com/intent/tweet?text=https://joddiy.cc/2020/05/12/TF2-0-Distributed/"
                        aria-label="Share on Twitter"
                    >
                        <i class="fab fa-twitter" aria-hidden="true"></i><span>Share on Twitter</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://plus.google.com/share?url=https://joddiy.cc/2020/05/12/TF2-0-Distributed/"
                        aria-label="Share on Google+"
                    >
                        <i class="fab fa-google-plus" aria-hidden="true"></i><span>Share on Google+</span>
                    </a>
                </li>
            
        </ul>
    </div>


            
        </div>
        


    
        
    

<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-times"></i>
        </div>
        
            <img id="about-card-picture" src="/assets/images/14108933.jpeg" alt="Author&#39;s picture"/>
        
            <h4 id="about-card-name">Joddiy Zhang</h4>
        
            <div id="about-card-bio"><p>author.bio</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>author.job</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker-alt"></i>
                <br/>
                Singapore
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->

<script src="/assets/js/script-sqrh47zm5nkjgifq4rx38uvns4r2rarrrvwuhjxiztyrddruca5ukl7nw6br.min.js"></script>

<!--SCRIPTS END-->


    
        
<script src="/assets/js/gitalk.js"></script>

        <script type="text/javascript">
          (function() {
            new Gitalk({
              clientID: 'bee9685b2dc9739b6bd5',
              clientSecret: 'a0c683f383a94fae6d021ab932f37f7e56899410',
              repo: 'joddiy.github.io',
              owner: 'joddiy',
              admin: ['joddiy'],
              id: '2020/05/12/TF2-0-Distributed/',
              ...{"language":"en","perPage":10,"distractionFreeMode":false,"enableHotKey":true,"pagerDirection":"first"}
            }).render('gitalk')
          })()
        </script>
    




    </body>
</html>
