
<!DOCTYPE html>
<html lang="en">
    
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Jz Blog">
    <title>RCMD Industry Paper - Jz Blog</title>
    <meta name="author" content="Joddiy Zhang">
    
    
        <link rel="icon" href="http://joddiy.cc/assets/images/favicon.ico">
    
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Joddiy Zhang","sameAs":["https://github.com/joddiy","https://scholar.google.com/citations?user=KH-xv38AAAAJ&hl=en&oi=sra","https://www.linkedin.com/in/joddiyzhang/"],"image":"14108933.jpeg"},"articleBody":"Outline\nFeature Interaction\nAutoRec(2015)\nDeep Crossing(2016)\nPNN(2016)\nNeural CF(2017)\nWide&amp;Deep(2016)\nDeepFM(2017)\nDCN(2017)\nxDeepFM(2018)\nNFM(2017)\nAFM(2017)\nAutoInt(2019)\nEDCN(2021)\nAutoGroup(2020)\n\n\nUser Behaviour\nDIN(2018)\nDIEN(2018)\nDMT(2020)\nDIHN(2022)\nDMIN(2020)\n\n\nLong-term User Behaviour\nSIM(2020)\n\n\nMulti Task\nESMM(2018)\nSTEM(2023)\n\n\nMulti Domain\nSTAR(2021)\n\n\nMulti-Modal\nSEMI(2021)\n\n\nDeep Match\nDSMM(2013)\nYouTubeNet(2016)\nSDM(2019)\nMIND(2019)\nComiRec(2020)\n\n\nBias\nCD2AN(2022)\n\n\nList Wise\nPRM(2019)\n\n\n\nOther Papers\nEvaluation\nInterpretable Evaluation for Offline Evaluation (IEOE)\n\n\n\nAutoRec(2015)\nAutoRec, a new CF model based on the autoencoder paradigm.\nIn rating-based collaborative filtering, we have m users, n items, and a partially observed user-item rating matrix R âˆˆ RmÃ—n.\nOur aim in this work is to design an item-based (user-based) autoencoder which can take as input each partially observed r(i) (r(u)), project it into a low-dimensional latent (hidden) space, and then reconstruct r(i) (r(u)) in the output space to predict missing ratings for purposes of recommendation.\n\nh(r; Î¸) &#x3D; f (W Â· g(Vr + Î¼) + b)\n\n\n\n\nFirst, we account for the fact that each r(i) is partially observed by only updating during backpropagation those weights that are associated with observed inputs, as is common in matrix factorisation and RBM approaches.\nSecond, we regularise the learned parameters so as to prevent overfitting on the observed ratings.\n\n\n\n\nregularisation strength Î» &gt; 0\n\n\n\nDeep Crossing(2016)\nThis paper proposes the Deep Crossing model which is a deep neural network that automatically combines features to produce superior models.\nThe important crossing features are discovered implicitly by the networks, which are comprised of an embedding and stacking layer, as well as a cascade of Residual Units.\n\n\nEmbedding and Stacking Layers, Embedding is applied per individual feature to transform the input features.\n\nResidual Layers\n\n\n\n\nNeural CF(2017)\nIn this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation â€” collaborative filtering â€” on the basis of implicit feedback.\nBy replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural networkbased Col laborative Filtering.\nAmong the various collaborative filtering techniques, matrix factorization (MF) [14, 21] is the most popular one, which projects users and items into a shared latent space, using a vector of latent features to represent a user or an item. Thereafter a userâ€™s interaction on an item is modelled as the inner product of their latent vectors.\nDespite the effectiveness of MF for collaborative filtering, it is well-known that its performance can be hindered by the simple choice of the interaction function inner product.\n\n\nThis paper explores the use of deep neural networks for learning the interaction function from data.\nGeneral Framework\n\nAbove the input layer is the embedding layer; it is a fully connected layer that projects the sparse representation to a dense vector\nFusion of GMF and MLP, Generalized Matrix Factorization (GMF)\n\n\nWe first train GMF and MLP with random initializations until convergence. We then use their model parameters as the initialization for the corresponding parts of NeuMFâ€™s parameters.\n\n\n\n\nPNN(2016)\nIn this paper, we propose a Product-based Neural Networks (PNN) with an embedding layer to learn a distributed representation of the categorical data, a product layer to capture interactive patterns between interfield categories, and further fully connected layers to explore high-order feature interactions.\n\n\nProduct-based Neural Network (PNN):\nbuilds a product layer based on the embedded feature vectors to model the inter-field feature interactions\n\n\nIn this paper, we propose two variants of PNN, namely IPNN and OPNN,Inner Product-based Neural Network (IPNN), which takes a pair of vectors as input and outputs a scalar.Outer Product-based Neural Network (OPNN), which takes a pair of vectors and produces a matrix.\n\n\n\n\nWide&amp;Deep(2016)\nOne challenge in recommender systems, similar to the general search ranking problem, is to achieve both memorization and generalization. \nMemorization can be loosely defined as learning the frequent co-occurrence of items or features and exploiting the correlation available in the historical data. Generalization, on the other hand, is based on transitivity of correlation and explores new feature combinations that have never or rarely occurred in the past.\nMemorization can be achieved effectively using cross-product transformations over sparse features. One limitation of cross-product transformations is that they do not generalize to query-item feature pairs that have not appeared in the training data.\n\n\nThe combined modelâ€™s prediction is:\n\n\nPerformance\n\n\nDeepFM(2017)\nThe proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture.Compared to the latest Wide &amp; Deep model from Google, DeepFM has a shared input to its â€œwideâ€ and â€œdeepâ€ parts, with no need of feature engineering besides raw features.\n\n\nFM ComponentThe output of FM is the summation of an Addition unit and a number of Inner Product units\n\n\nDeep ComponentThe deep component is a feed-forward neural network, which is used to learn high-order feature interactions.\nEmbeddingIt is worth pointing out that FM component and deep component share the same feature embedding\n\n\nRelationship with the other Neural Networks\nFNN, PNN, Wide&amp;Deep\n\n\n\nPerformance\n\n\nDCN(2017)\nIn this paper, we propose the Deep &amp; Cross Network (DCN) which keeps the benefits of a DNN model, and beyond that, it introduces a novel cross network that is more efficient in learning certain bounded-degree feature interactions.\nIn the Kaggle competition, the manually craÂ‰ed features in many winning solutions are low-degree, in an explicit format and effective.\nThe wide-and-deep [4] is a model in this spirit. It takes cross features as inputs to a linear model, and jointly trains the linear model with a DNN model.\n\n\nEmbedding and Stacking Layer: Embedding matrix &amp; embedding vector\nCross NetworkThe cross network is composed of cross layers, with each layer having the following formula:\n\n\nA visualization of one cross layer is shown in Figure 2.\n\n\nPerformance\n\nxDeepFM(2018)\nIn this paper, we propose a novel Compressed Interaction Network (CIN), which aims to generate feature interactions in an explicit fashion and at the vector-wise level.\nIn this paper, we propose a novel Compressed Interaction Network (CIN), which aims to generate feature interactions in an explicit fashion and at the vector-wise level.\nWe thus design a novel compressed interaction network (CIN) to replace the cross network in the DCN.\nEmbedding Layer\n\nImplicit High-order Interactions: Wide&amp;DeepExplicit High-order Interactions: DCN\n\nCompressed Interaction Network (CIN), with the following considerations:(1) interactions are applied at vector-wise level, not at bit-wise level;(2) high-order feature interactions is measured explicitly;(3) the complexity of network will not grow exponentially with the degree of interactions.\nwe formulate the output of field embedding as a matrix X0 âˆˆ RmÃ—D, where the i-th row in X0 is the embedding vector of the i-th field.\n\n\n\nPerformance\n\n\nNFM(2017)\nIn this work, we propose a novel model for sparse data prediction named Neural Factorization Machines (NFMs), which enhances FMs by modelling higher-order and non-linear feature interactions. \nBy devising a new operation in neural network modelling â€” Bilinear Interaction (Bi-Interaction) pooling â€” we subsume FM under the neural network framework for the first time.\n\nExpressiveness Limitation of FM: FM still belongs to the family of (multivariate) linear models.\n\nThe NFM Model\n\nBi-Interaction LayerClearly, the output of Bi-Interaction pooling is a k-dimension vector that encodes the second-order interactions between features in the embedding space.\n\n\n\nAFM(2017)\nWe propose a novel model named Attentional Factorization Machine (AFM), which learns the importance of each feature interaction from data via a neural attention network\n\n\nPair-wise Interaction Layer\n\n\nAttention-based Pooling LayerWe propose to employ the attention mechanism on feature interactions by performing a weighted sum on the interacted vectors:\n\n\n\n\nDIN(2018)\nIn this paper, we propose a novel model: Deep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors\n\nIt is not necessary to compress all the diverse interests of a certain user into the same vector when predicting a candidate ad because only part of userâ€™s interests will influence his&#x2F;her action (to click or not to click).\n\nBesides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters.\n\nIn this paper, we develop a novel mini-batch aware regularization where only parameters of non-zero features appearing in each mini-batch participate in the calculation of L2-norm, making the computation acceptable.\n\nBesides, we design a data adaptive activation function, which generalizes commonly used PReLU[12] by adaptively adjusting the rectified point w.r.t. distribution of inputs and is shown to be helpful for training industrial networks with sparse features.\n\n\nDIN designs a local activation unit to soft-search for relevant user behaviors and takes a weighted sum pooling to obtain the adaptive representation of user interests with respect to a given ad.\n\n\n\nLocal activation unit of Eq.(3) shares similar ideas with attention methods which are developed in NMT task[1]. However different from traditional attention method, the constraint is relaxed in Eq.(3), aiming to reserve the intensity of user interests. That is, normalization with softmax on the output of a(Â·) is abandoned.\n\n\n\nMini-batch Aware RegularizationIn this paper, we introduce an efficient mini-batch aware regularizer, which only calculates the L2-norm over the parameters of sparse features appearing in each mini-batch and makes the computation possible.\nData Adaptive Activation Function\n\n\n\nPerformance\n\n\nDIEN(2018)\nto target item, and obtains adaptive interest representation. However, most interest models including DIN regard the behavior as the interest directly, while latent interest is hard to be fully reflected by explicit behavior.\nBased on all these observations, we propose Deep Interest Evolution Network (DIEN) to improve the performance of CTR prediction. There are two key modules in DIEN, one is for extracting latent temporal interests from explicit user behaviors, and the other one is for modeling interest evolving process.\nWe propose auxiliary loss which uses the next behavior to supervise the learning of current hidden state. We call these hidden states with extra supervision as interest states.\nWe take GRU to model the dependency between behaviors, where the input of GRU is ordered behaviors by their occur time.\nWe propose auxiliary loss, which uses behavior bt+1 to supervise the learning of interest state ht.\nBesides using the real next behavior as positive instance, we also use negative instance that samples from item set except the clicked item.\n\n\n\nInterest Extractor LayerThe formulations of GRU are listed as follows:\n\n\nSo we propose auxiliary loss, which uses behavior bt+1 to supervise the learning of interest state ht. Besides using the real next behavior as positive instance, we also use negative instance that samples from item set except the clicked item.\n\n\nInterest Evolving LayerGRU with attentional update gate (AUGRU): \n\n\n\n\n\nPerformance\n\n\nAutoInt(2019)\nIn this paper, we propose an effective and efficient method called the AutoInt to automatically learn the high-order feature interactions of input features.\n\n\n\nEmbedding LayerOur proposed approach learns effective low-dimensional representations of the sparse and highdimensional input features and is applicable to both the categorical and&#x2F;or numerical input features.\n\n\n\nInteracting LayerWe move to model high-order combinatorial features in the space, in this paper, we tackle this problem with a novel method, the multi-head self-attention mechanism.\n\n\nWe first define the correlation between feature m and feature k under a specific attention head h as follows:\n\n\n\n\n\nwhere Ïˆ is an attention function which defines the similarity between the feature m and k, âŠ• is the concatenation operator, and H is the number of total heads.\nPerformance\n\n\nDMT(2020)\nWe propose Deep Multifaceted Transformers (DMT), a novel framework that can model usersâ€™ multiple types of behavior sequences simultaneously with multiple Transformers. It utilizes Multi-gate Mixture-of-Experts to optimize multiple objectives.\n\n\n\nInput and Embedding LayersUsersâ€™ diverse behavior sequences, the sequence of each type of usersâ€™ behaviors is represented by a variable-length sequence of items.\nDeep Multifaceted Transformers\nTo capture each userâ€™s multifaceted interest, we use three separate Deep Interest Transformers (they have different parameters) to model the userâ€™s click sequence, cart sequence and order sequence, and learn the userâ€™s short-term, middle-term and long-term interest vectors respectively.\nThe basic idea is that usersâ€™ multiple types of behavior sequences on items (e.g., click, add to cart and order) are significantly difierent and they have different timescales.\nDeep Interest TransformerSelf-attention blocks\n\n\n\nPositional encoding\n\nSinusoidal Positional Embedding (pos_sincos)\nLearned Positional Embedding (pos_learn)\n\nMulti-gate Mixture-of-Experts LayersThe multiple objectives may have complex relationships (e.g., independent, related or confiict) with each other, and the commonly used Shared-bottom [23] architecture may harm the learning of multiple objectives. To capture the relation and confiict of multiple tasks, we adopt Multi-gate Mixture-of-Experts (MMoE) [38] for multi-objective ranking.\nFor each task :, as shown in Equation 3, firstly, it exploits a gating network to learn the weights of each expert. Secondly, it calculates the weighted sum of experts outputs. Finally, it feds into a utility network to get the utility for task. The gating networks and utility networks are implemented by multi-layer perceptrons, and their parameters are different for each task.\n\n\n\nBias Deep Neural NetworkIn this paper, we investigate two types of selection bias in e-commerce Recommender Systems: Position bias and Neighboring bias.\n\nPosition bias. The â€œposition biasâ€ means that users tend to click items which are displayed closer to the top of the list.\nNeighboring bias. The â€œneighboring biasâ€ means that the probability of clicking a item may be influenced by its neighboring products.\n\nDMT uses a Bias Deep Neural Network to model the selection bias. The input of the networks are bias features. For the position bias, the input is the number of index (â€œPosition_indexâ€ bias) or page (â€œPosition_pageâ€ bias) of the target item. For the neighboring bias, the input is the categories of the target item and its nearest neighboring items.\n\n\nPerformance\n\nPosition bias:\nFrom this table, we can find that: The Learned Positional Embedding method achieves the best performance compared with other methods. So we use this method in the experiments.\n\n\nMMoE:\nwe find that simultaneously modeling modeling cart sequence can achieve better performance than singly modeling the click sequence. However, if we further adding the order sequence, the performance for click prediction and order prediction doesnâ€™t increase further.\nWe empirically find that: For a product that have a long repurchase period (i.e., computer), a user will tend to click but not purchase the product again in short time after buying it. For a product that have a short repurchase period (i.e., milk), the user may both click and purchase it again in short time. The order sequence may lead to the confiict between click prediction and order prediction and disturb the information in the click or cart sequence.\n\n\n\nSIM(2020)\nHowever, MIMN fails to precisely capture user interests given a specific candidate item when the length of user behavior sequence increases further, say, by 10 times or more.\nIn this paper, we tackle this problem by designing a new modeling paradigm, which we name as Search-based Interest Model (SIM).\nRich user behavior data is proven to be of great value [8]. For example, 23% of users in Taobao, one of the worldâ€™s leading e-commerce site, click with more than 1000 products in last 5 months[8, 10]. The key idea of DIN is searching the effective information from user behaviors to model special interest of user, facing different candidate items. But the searching formula of DIN costs an unacceptable computation and storage facing the long sequential user behavior data as we mentioned above.\n\n\nGeneral Search Unit\n\nHard-search:\nThe hard-search model is non-parametric. Only behavior belongs to the same category as the candidate item will be selected and aggregated as a sub behavior sequence to be sent to the exact search unit.\nSoft-search:\nIn the soft-search model, Wb and Wa are the parameters of weight. ea and ei denote the embedding vectors of target item and i-th behavior bi , respectively. \nTo further speed up the top-K search over ten thousands length of user behaviors, sublinear time maximum inner product search method ALSH[12] is conducted based on the embedding vectors E to search the related top-K behaviors with target item.\nNote that if the user behavior grows to a certain extent, it is impossible to directly fed the whole user behaviors into the model. In that situation, one can randomly sample sets of sub-sequence from the long sequential user behaviors, which still follows the same distribution of the original one.\nExact Search UnitIn the first search stage, top-K related sub user behavior sequence B, and the sequence temporal property for each behavior, specifically, the time intervals D.\nWe take advantage of multi-head attention to capture the diverse user interest:\n\n\n\nOnline Serving SystemWe build an two-level structured index for each user, which we name as user behavior tree (UBT), as illustrated in Figure 3. Briefly speaking, UBT follows the Key-Key-Value data structure: the first key is user id, the second keys are category ids and the last values are the specific behavior items that belong to each category. UBT is implemented as an distributed system, with size reaching up to 22 TB.\n\n\n\nPerformanceSIM achieves significant improvement compared with MIMN because MIMN encodes all unfiltered user historical behaviors into a fixed-length memory which makes it hard to capture diverse long-term interest.\nThe result shows that the user behaviors reserved by hard-search strategy could cover 75% of that from the soft-search strategy.\n\n\n\n\nESMM(2018)\nIn this paper, we model CVR in a brand-new perspective by making good use of sequential pattern of user actions, i.e., impression â†’ click â†’ conversion. The proposed Entire Space Multi-task Model (ESMM) can eliminate the two problems simultaneously by i) modeling CVR directly over the entire space, ii) employing a feature representation transfer learning strategy.\nHowever, there exist several task-specific problems, making CVR modeling challenging. Among them, we report two critical ones encountered in our real practice: \ni) sample selection bias (SSB) problem, As illustrated in Fig.1, conventional CVR models are trained on dataset composed of clicked impressions, while are utilized to make inference on the entire space with samples of all impressions.\nii) data sparsity (DS) problem, In practice, data gathered for training CVR model is generally much less than CTR task.\n\n\nCVR ModelingEq.(2) tells us that with estimation of pCTCVR and pCTR, pCVR can be derived over the entire input space X, which addresses the sample selection bias problem directly.\n\n\n\n\nEntire Space Multi-Task Model\n\nPerformance\n\n\nSTEM(2018)\nIn this paper, we introduce a novel paradigm called Shared and Task-specific EMbeddings (STEM) that aims to incorporate both shared and task-specific embeddings to effectively capture task-specific user preferences.\nA key challenge in MTL is the occurrence of negative transfer, where the performance of certain tasks deteriorates due to conflicts between tasks.Note that, existing methods tend to treat all samples in a task as a whole, overlooking the inherent intricacies within them. \n\n\nHowever, to our surprise, in the zone that receives comparable positive feedback from both tasks, the performance of existing methods is inferior to that of the single-task model.\nIntuitively, users may possess diverse and sometimes even conflicting preferences for items across various tasks, which become more pronounced when there are sufficient signals from multiple tasks, as in the comparable zone here.\nMotivated by the limitations of the shared-embedding paradigm, this paper introduces a Shared and Task-specific EMbeddings (STEM) paradigm. STEM aims to incorporate both shared and task-specific embeddings to learn taskspecific user preferences.\nEvolution of the MTL Model Architectures\n\nAs depicted in Figure 1, both MMoE and PLE demonstrate improved performance when there is overwhelming feedback from either task.\nSTEM-Net\n\nUSER ID and ITEM ID task-specificWe wonder if we can still achieve decent performance lift when only making embeddings of USER ID and ITEM ID task-specific. We evaluate the performance of the corresponding model STEM-Net-(user id, item id) observing that it also achieves competitive performance with STEM-Net.\nWe design two new variants, STEM-Net-(user side) and STEM-Net-(item side), and observe the former one exhibit better performance, indicating that user side features are more effective than item in capturing diverse user preference among tasks.\n\n\nPerformance\n\nEDCN(2021)\nWe propose a novel deep CTR model EDCN. EDCN introduces two advanced modules, namely bridge module and regulation module, which work collaboratively to capture the layer-wise interactive signals and learn discriminative feature distributions for each hidden layer of the parallel networks.\n\n\nIn this paper, we focus on optimizing the models with parallel structure by enhancing the explicit and implicit feature interactions via information sharing.\nInsufficient sharing in hidden layersWe refer such fusion pattern(DCN) as late fusion. With late fusion, explicit and implicit feature interaction networks do not share information in the intermediate hidden layers, which weakens the interactive signals between each other and may easily lead to skewed gradients during the backward propagation.\n\n\nExcessive sharing in network inputTherefore, excessive sharing the network inputs and feeding all the features indiscriminately to the parallel networks may not be a reasonable choice and result in sub-optimal performance.\n\n\nBridge ModuleTo overcome this limitation, we introduce a dense fusion strategy, which is implemented by our proposed bridge module, to capture the layer-wise interactive signals between two parallel networks.\nthe bridge module can be formulated as fğ‘™ &#x3D; ğ‘“ (xğ‘™ , hğ‘™ ), where ğ‘“ (Â·) is a pre-defined interaction function:\n\nPointwise Addition\nHadamard Product\nConcatenation\nAttention Pooling\n\nRegulation Module(SE-Net) Inspired by the gating mechanism used in MMoE [20], we propose a regulation module, implemented by a field-wise gating network to soft-select discriminative feature distributions for each parallel networks.\n\n\n\nPerformance\n\n\nSTAR(2021)\nGenerally, different domains may share some common user groups and items, and each domain may have its own unique user groups and items.\nIn this paper, we propose the Star Topology Adaptive Recommender (STAR) model to train a single model to serve all domains by leveraging data from all domains simultaneously, capturing the characteristics of each domain, and modeling the commonalities between different domains.\n\n\nArchitecture OverviewWe propose Star Topology Adaptive Recommender (STAR) for multi-domain CTR prediction to better utilize the similarity among different domains while capturing the domain distinction. \n\n\nAs shown in Figure 4, STAR consists of three main components:(1) the partitioned normalization (PN) which privatizes normalization for examples from different domains,(2) the star topology fully-connected neural network (star topology FCN),(3) the auxiliary network that treats the domain indicator directly as the input feature and learns its semantic embeddings to capture the domain distinction.\nPartitioned NormalizationAmong all normalization methods, batch normalization (BN) [14] is a representative method that is proved to be crucial to the successful training of very deep neural networks [14, 31].\n\n\nDuring testing, moving averaged statistics of mean ğ¸ and variance ğ‘‰ ğ‘ğ‘Ÿ across all samples are used instead\n\n\nTo capture the unique data characteristic of each domain, we propose partitioned normalization (PN) which privatizes normalization statistics and parameters for different domains.\n\n\nBesides the modification of the scale and bias, PN also let different domains to accumulate the domain-specific moving average of mean ğ¸ğ‘ and variance ğ‘‰ ğ‘ğ‘Ÿğ‘\n\n\nStar Topology FCN\n\nAs depicted in Figure 5, the proposed star topology FCN consists of a shared centered FCN and independent FCNs per domain, thus the total number of FCN is ğ‘€ + 1. \nThe final model of ğ‘-th domain is obtained by combining the shared centered FCN and domain-specific FCN, in which the centered parameters learn general behaviors among all domains, and the domain-specific parameters capture specific behaviors in different domains to facilitate more refined CTR prediction.\nThe final weights ğ‘Š â˜… ğ‘– and bias ğ‘â˜… ğ‘– for the ğ‘-th domain is obtained by:\n\n\n\nAuxiliary NetworkWe argue that a good multi-domain CTR model should have the following characteristic: (1) have informative features regarding the domain characteristic (2) make these features easily and directly influence the final CTR prediction.\nTo this end, we propose an auxiliary network to learn the domain distinction. To augment informative features regarding the domain characteristic, we treat the domain indicator directly as the ID feature input.\nThe domain indicator is first mapped into embedding vector and concatenated with other features. The auxiliary network then computes forward pass with respect to the concatenated features to gets the one-dimensional output ğ‘ ğ‘.\nPerformance\n\n\nAutoGroup(2020)\nAn end-to-end model, AutoGroup, is proposed, which casts the selection of feature interactions as a structural optimization problem.\nThe main contribution of AutoGroup is that it performs both dimensionality reduction and feature selection which are not seen in previous models.\n\nIn real-world scenarios, useful feature interactions are usually sparse relative to the combination space of raw features\n\nModelling with AutoGroup consists of three stages: \n\nthe Automatic Feature Grouping Stage, \nthe Interaction Stage, \nand the MLP Stage.\n\n\n\nAutomatic Feature Grouping StageThe simple strategy of enumerating all feature interactions makes neural networks difficult to achieve the optimal set of network weights.\nThe main idea is to find multiple small sets of features, where the feature interactions in each such identified feature set are effective at a given order.\nAs shown in Figure 1, the selection of feature sets can be viewed as a structural optimization problem: each feature is allowed to be selected by any feature set where each feature set groups related features together such that their interaction is likely to improve prediction performance.\nWe relax the binary choice of a feature being selected by a feature set to a softmax over the two possibilities:(to indicate whether feature fi is selected into feature set   at the pth-order.).\n\n\nTo make the operation differentiable, we relax it with a softmax:\nInteraction StageSpecifically, the representation of a feature set is the weighted sum of representations of the features in this set.\n\n\nIn this paper, we generalize the reformulation (Eq. 7) for the pth-order interaction as:\n\n\nPerformance\n\n\nCD2AN(2022)\nFrom the popularity distribution shift perspective, the normal paradigm trained on exposed items (most are hot items) identifies that recommending popular items more frequently can achieve lower loss, thus injecting popularity information into item property embedding.\n\nExisting work addresses this issue with inverse propensity scoring (IPS) or causal embeddings. \n\nHowever, we argue that not all popularity biases mean bad effects, We propose a co-training disentangled domain adaptation network (CD2AN), which can co-train both biased and unbiased models.\nSpecifically, for popularity distribution shift, CD2AN disentangles item property representation and popularity representation from item property embedding. \nFor long-tail distribution shift, we introduce additional unexposed items (most are long-tail items) to align the distribution of hot and long-tail item property representations.\nFurther, from the instances perspective, we carefully design the item similarity regularization to learn comprehensive item representation, which encourages item pairs with more effective co-occurrences patterns to have more similar item property representations.\nExisting work eliminates this bias effect with inverse propensity scoring (IPS) or causal embeddings. \n(1) IPS, which employs balanced training by re-weighting the interactions [14, 33].\n(2) Causal embeddings, which formulate a causal graph to describe the important cause-effect relations in the recommendation process.\n\n\n\nThe key challenge is how to alleviate popularity and long-tail distribution shifts to obtain an unbiased and comprehensive item representations.\nWe design a feature disentangling module with orthogonal regularization and popularity similarity regularization to separate the popularity representation and the item property representation from the item property embedding injected with the popularity information.\nFeature Disentangling ModuleBased on this training paradigm, there are two main reasons for the popularity bias. (1) Popularity information is injected into the item property embedding, which increases the vector length of popular items, making inner product models score popular items high for every user [41]. (2) Data sparsity makes long-tail items unable to obtain good representations, \n\n\n\nğ¿ğ‘ğ‘œğ‘_ğ‘ ğ‘–ğ‘š encourages the module to separate out the popularity information injected into ğ‘¬ (ğ‘¥) by constraining ğ’—ğ‘ğ‘œğ‘ ğ‘¥ to be similar to ğ’—ğ‘\nğ¿ğ‘œğ‘Ÿğ‘¡â„ğ‘œğ‘”ğ‘œğ‘›ğ‘ğ‘™ encourages orthogonality between ğ’—ğ‘ğ‘œğ‘ ğ‘¥ and ğ’—ğ‘ğ‘Ÿğ‘œ ğ‘¥ , enabling the popularity and property encoders to encode different aspects of the inputs.\nRegularizations for Long-tail ShiftDomain Alignment\nAlthough item property representations ğ’—ğ‘ğ‘Ÿğ‘œ ğ‘¥ are extracted through FDM, we believe that the popularity bias still exists due to data sparsity. Compared with popular items with sufficient interaction, long-tail items cannot be effectively learned. \nInspired by domain adaptation, we additionally introduce unexposed items ğ‘¥ğ‘˜ (most are long-tail items) randomly sampled from the entire item pool, and adopt Maximum Mean Discrepancy (MMD) regularization [13] to align the distribution of hot and long-tail items:\nInstance Alignment\nIntuitively, ğ¿ğ‘€ğ‘€ğ· encourages the cluster center of long-tail items to approach the cluster center of hot items.\nIn the training stage, we additionally introduce the userâ€™s historical interacted items in the item tower. Based on contrastive learning [7, 38], we encourage the items clicked by the same user to be more similar.\n\n\n\n\nCo-trainingHowever, we argue that not all popularity biases mean bad effects. The reason why an item is popular maybe that it is of good quality or in line with the current trend.\nwe co-train unbiased model and biased model to optimize top-k accuracy by adopting the batch softmax loss used in both recommenders [39] and NLP [12]:\n\n\n\nPerformance\n\n\nDIHN(2022)\nIn this paper, we present a new recommendation problem, TriggerInduced Recommendation (TIR), where usersâ€™ instant interest can be explicitly induced with a trigger item and follow-up related target items are recommended accordingly.\nIn TIR scenarios, usersâ€™ instant interest can be explicitly induced with a trigger item.\n\nChallenge 1: Usersâ€™ instant interest induced from a trigger item are inherently noisy,\nChallenge 2: Users always show multiple interests from their historical behaviors.\n\nWe propose a novel recommendation method named Deep Interest Highlight Network (DIHN) for Click-Through Rate (CTR) prediction in TIR scenarios. \nIt has three main components including \n\nUser Intent Network (UIN), which responds to generate a precise probability score to predict userâ€™s intent on the trigger item; \nFusion Embedding Module (FEM), which adaptively fuses trigger item and target item embeddings based on the prediction from UIN; \nHybrid Interest Extracting Module (HIEM), which can effectively highlight usersâ€™ instant interest from their behaviors based on the result of FEM.\n\n\n\n\nUser Intent NetworkReferring to the UIN module in Fig.2, where we utilize three categories of features, ğ‘–.ğ‘’, User Profile, User Behaviors, Trigger to estimate the probability.\n\n\nğ‘¦ âˆˆ {0, 1} is the ground truth label representing whether users clicking the trigger item or not.\n\n\nFusion Embedding ModuleA suitable usersâ€™ behavior modelling solution could be adaptively fuse the trigger item with target item.\n\n\nHybrid Interest Extracting ModuleHard Sequential Modelling\nFollowing the hard-search mode from SIM [13], we propose the Hard Sequential Modelling (HSM), indicating that only behaviors with the same attribute (ğ‘’.ğ‘”., category, destination) as the trigger item.\nSoft Sequential Modelling\nTherefore, from another modelling perspective, we proposed the Soft Sequential Modelling (SSM), which adaptively calculates the representation vector of usersâ€™ behaviors with respect to the fusing result from FEM.\nPerformance\n\n\nSEMI(2021)\nExisting micro-video recommendation methods only focus on usersâ€™ browsing behaviors on micro-videos, but ignore their purchasing intentions in the ecommerce environment.\n\nIn this work, we propose to leverage the product-related user behavior data for micro-video recommendation.\n\nWe propose a novel cross-domain recommendation method, named SEquential Multi-modal Information transfer network (SEMI).\nTo better bridge the behavior pattern gap between micro-video domain and product domain, we further propose a Cross-domain Contrastive Learning (CCL) algorithm to pre-train encoders to describe sequential user behaviors.\nBasic ComponentsFeature Extractor\nWe adopt UniVL [25] to fuse the multi-modal information of microvideos. Specifically, we utilize micro-video titles and frames as inputs and pre-train the UniVL model using about 100 million ecommerce micro-videos. The pre-training tasks of UniVL include masked language modeling, masked frame modeling, video-text alignment, and tag classifications.\nSimilarly, we have the semantic representation for each product ğ‘, which is extracted by the pre-trained UNITER model [9].\nMulti-Head Attention Block\nâ€¦\nMulti-Modal Sequence Encoder\nAs the multi-head attention mechanism is not aware of the sequence order information, we use timestamp embedding.\nCross-Domain Contrastive LearningWe propose a Cross-domain Contrastive Learning (CCL) algorithm to pre-train the micro-video and product multi-modal sequence encoders. \nThe objective of CCL is to encourage the representations of a micro-video sequence and a product sequence to be similar, if they come from the same session of an active user.\nSequential Multi-Modal Information Transfer NetworkThis is challenging because usersâ€™ behaviors in micro-video domain are more sparse than their behaviors in the product domain.\nWe feed [ğ‘‰ ğ‘–ğ‘›ğ‘¡ğ‘Ÿğ‘, ğ‘ƒğ‘–ğ‘›ğ‘¡ğ‘Ÿğ‘] into an additional multi-head attention block to capture the inter-domain dependencies, where [Â·] denotes concatenation.\nWe utilize another multi-head attention blocks over the target micro-video and {ğ‘‰ ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ , ğ‘ƒğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ }.\nPerformance\n\n\nAblation Studies\n\n\n\nIEOE(2021)\nDSSM(2013)\nIn this study we strive to develop a series of new latent semantic models with a deep structure that project queries and documents into a common low-dimensional space where the relevance of a document given a query is readily computed as the distance between them.\n\n\nThe semantic relevance score between a query and a document is then measured as:\n\n\nWe compute the posterior probability of a document given a query from the semantic relevance score between them through a softmax function:\n\n\nYouTubeNet(2016)\nIn this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning.\n\n\n\nSDM(2019)\nIn this paper, we propose a new sequential deep matching (SDM) model to capture usersâ€™ dynamic preferences by combining short-term sessions and long-term behaviors.\nWe tackle the following two inherent problems in realworld applications: (1) there could exist multiple interest tendencies in one session. (2) long-term preferences may not be effectively fused with current session interests.\n\n\nWe propose to encode behavior sequences with two corresponding components: multi-head self-attention module to capture multiple types of interests and long-short term gated fusion module to incorporate long-term preferences.\n\n\n\nMIND(2019)\nComiRec(2020)\n","dateCreated":"2024-01-01T11:36:17+08:00","dateModified":"2024-06-21T11:48:36+08:00","datePublished":"2024-01-01T11:36:17+08:00","description":"Paper summary for rcmd industry papers","headline":"RCMD Industry Paper","image":["https://cdn.iconscout.com/icon/premium/png-256-thumb/personalized-recommendations-6020966-5047679.png","https://realpython.com/cdn-cgi/image/width=1920,format=auto/https://files.realpython.com/media/Build-a-Recommendation-Engine-With-Collaborative-Filtering_Watermarked.451abc4ecb9f.jpg"],"mainEntityOfPage":{"@type":"WebPage","@id":"http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/"},"publisher":{"@type":"Organization","name":"Joddiy Zhang","sameAs":["https://github.com/joddiy","https://scholar.google.com/citations?user=KH-xv38AAAAJ&hl=en&oi=sra","https://www.linkedin.com/in/joddiyzhang/"],"image":"14108933.jpeg","logo":{"@type":"ImageObject","url":"14108933.jpeg"}},"url":"http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/","keywords":"RCMD","thumbnailUrl":"https://cdn.iconscout.com/icon/premium/png-256-thumb/personalized-recommendations-6020966-5047679.png"}</script>
    <meta name="description" content="Paper summary for rcmd industry papers">
<meta property="og:type" content="blog">
<meta property="og:title" content="RCMD Industry Paper">
<meta property="og:url" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/index.html">
<meta property="og:site_name" content="Jz Blog">
<meta property="og:description" content="Paper summary for rcmd industry papers">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/1.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/2.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/3.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/4.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/5.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/6.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/7.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/8.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/9.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/10.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/11.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/12.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/13.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/14.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/15.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/16.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/17.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/19.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/20.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/18.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/21.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/22.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/23.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/24.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/25.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/26.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/27.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/28.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/29.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/30.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/30.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/31.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/32.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/33.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/34.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/35.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/36.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/37.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/38.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/39.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/40.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/41.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/43.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/42.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/44.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/45.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/46.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/47.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/48.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/49.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/50.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/51.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/52.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/53.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/54.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/55.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/58.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/56.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/57.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/59.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/60.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/61.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/63.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/64.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/65.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/66.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/67.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/68.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/69.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/70.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/72.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/73.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/71.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/74.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/75.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/76.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/77.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/79.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/78.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/80.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/81.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/82.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/83.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/84.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/85.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/86.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/87.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/88.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/89.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/90.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/91.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/92.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/93.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/95.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/96.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/97.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/98.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/99.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/100.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/101.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/102.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/103.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/104.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/105.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/106.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/107.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/108.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/109.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/110.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/111.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/112.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/113.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/114.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/115.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/116.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/117.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/118.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/119.png">
<meta property="og:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/120.png">
<meta property="article:published_time" content="2024-01-01T03:36:17.000Z">
<meta property="article:modified_time" content="2024-06-21T03:48:36.718Z">
<meta property="article:author" content="Joddiy Zhang">
<meta property="article:tag" content="RCMD">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/1.png">
    
    
        
    
    
        <meta property="og:image" content="http://joddiy.cc/assets/images/14108933.jpeg"/>
    
    
        <meta property="og:image" content="https://cdn.iconscout.com/icon/premium/png-256-thumb/personalized-recommendations-6020966-5047679.png"/>
        <meta class="swiftype" name="image" data-type="enum" content="https://cdn.iconscout.com/icon/premium/png-256-thumb/personalized-recommendations-6020966-5047679.png"/>
    
    
        <meta property="og:image" content="https://realpython.com/cdn-cgi/image/width=1920,format=auto/https://files.realpython.com/media/Build-a-Recommendation-Engine-With-Collaborative-Filtering_Watermarked.451abc4ecb9f.jpg"/>
        <meta class="swiftype" name="image" data-type="enum" content="https://realpython.com/cdn-cgi/image/width=1920,format=auto/https://files.realpython.com/media/Build-a-Recommendation-Engine-With-Collaborative-Filtering_Watermarked.451abc4ecb9f.jpg"/>
    
    
    <!--STYLES-->
    
<link rel="stylesheet" href="/assets/css/style-x8blglznjjnb9pnnwui5zw4h43ysufmsh1b0omicawm4vhqcutzqavokgpne.min.css">

    <!--STYLES END-->
    

    

    
        
    
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

    <body>
        <div id="blog">
            <!-- Define author's picture -->


    
        
            
        
    

<header id="header" data-behavior="4">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a
            class="header-title-link"
            href="/"
            aria-label=""
        >
            Jz Blog
        </a>
    </div>
    
        
            <a
                class="header-right-picture "
                href="#about"
                aria-label="Open the link: /#about"
            >
        
        
            <img class="header-picture" src="/assets/images/14108933.jpeg" alt="Author&#39;s picture"/>
        
        </a>
    
</header>

            <!-- Define author's picture -->



        
    

<nav id="sidebar" data-behavior="4">
    <div class="sidebar-container">
        
            <div class="sidebar-profile">
                <a
                    href="/#about"
                    aria-label="Read more about the author"
                >
                    <img class="sidebar-profile-picture" src="/assets/images/14108933.jpeg" alt="Author&#39;s picture"/>
                </a>
                <h4 class="sidebar-profile-name">Joddiy Zhang</h4>
                
                    <h5 class="sidebar-profile-bio"><p><a href="mailto:&#106;&#111;&#x64;&#x64;&#x69;&#121;&#122;&#x68;&#x61;&#x6e;&#x67;&#64;&#103;&#x6d;&#97;&#x69;&#108;&#46;&#x63;&#x6f;&#x6d;">&#106;&#111;&#x64;&#x64;&#x69;&#121;&#122;&#x68;&#x61;&#x6e;&#x67;&#64;&#103;&#x6d;&#97;&#x69;&#108;&#46;&#x63;&#x6f;&#x6d;</a></p>
</h5>
                
            </div>
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/"
                            
                            rel="noopener"
                            title="Home"
                        >
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-categories"
                            
                            rel="noopener"
                            title="Categories"
                        >
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-tags"
                            
                            rel="noopener"
                            title="Tags"
                        >
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Tags</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-archives"
                            
                            rel="noopener"
                            title="Archives"
                        >
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="#about"
                            
                            rel="noopener"
                            title="About"
                        >
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://github.com/joddiy"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="GitHub"
                        >
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://scholar.google.com/citations?user=KH-xv38AAAAJ&hl=en&oi=sra"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="Google Scholar"
                        >
                        <i class="sidebar-button-icon fab fa-google" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Google Scholar</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://www.linkedin.com/in/joddiyzhang/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="LinkedIn"
                        >
                        <i class="sidebar-button-icon fab fa-linkedin" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">LinkedIn</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
        <div class="post-header-cover
                    text-center
                    post-header-cover--partial"
             style="background-image:url('https://realpython.com/cdn-cgi/image/width=1920,format=auto/https://files.realpython.com/media/Build-a-Recommendation-Engine-With-Collaborative-Filtering_Watermarked.451abc4ecb9f.jpg');"
             data-behavior="4">
            
        </div>

            <div id="main" data-behavior="4"
                 class="hasCover
                        hasCoverMetaOut
                        ">
                
<article class="post">
    
    
        <div class="post-header main-content-wrap text-center">
    
        <h1 class="post-title">
            RCMD Industry Paper
        </h1>
    
    
        <div class="post-meta">
    <time datetime="2024-01-01T11:36:17+08:00">
	
		    Jan 01, 2024
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Research/">Research</a>


    
</div>

    
</div>

    
    <div class="post-content markdown">
        <div class="main-content-wrap">
            <h1 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h1><ul>
<li>Feature Interaction<ul>
<li><a href="#AutoRec">AutoRec(2015)</a></li>
<li><a href="#Deep-Crossing">Deep Crossing(2016)</a></li>
<li><a href="#PNN">PNN(2016)</a></li>
<li><a href="#Neural-CF">Neural CF(2017)</a></li>
<li><a href="#Wide-Deep">Wide&amp;Deep(2016)</a></li>
<li><a href="#DeepFM">DeepFM(2017)</a></li>
<li><a href="#DCN">DCN(2017)</a></li>
<li><a href="#xDeepFM">xDeepFM(2018)</a></li>
<li><a href="#NFM">NFM(2017)</a></li>
<li><a href="#AFM">AFM(2017)</a></li>
<li><a href="#AutoInt">AutoInt(2019)</a></li>
<li><a href="#EDCN">EDCN(2021)</a></li>
<li><a href="#AutoGroup">AutoGroup(2020)</a></li>
</ul>
</li>
<li>User Behaviour<ul>
<li><a href="#DIN">DIN(2018)</a></li>
<li><a href="#DIEN">DIEN(2018)</a></li>
<li><a href="#DMT">DMT(2020)</a></li>
<li><a href="#DIHN">DIHN(2022)</a></li>
<li><a href="#DMIN">DMIN(2020)</a></li>
</ul>
</li>
<li>Long-term User Behaviour<ul>
<li><a href="#SIM">SIM(2020)</a></li>
</ul>
</li>
<li>Multi Task<ul>
<li><a href="#ESMM">ESMM(2018)</a></li>
<li><a href="#STEM">STEM(2023)</a></li>
</ul>
</li>
<li>Multi Domain<ul>
<li><a href="#ESMM">STAR(2021)</a></li>
</ul>
</li>
<li>Multi-Modal<ul>
<li><a href="#SEMI">SEMI(2021)</a></li>
</ul>
</li>
<li>Deep Match<ul>
<li><a href="#DSSM">DSMM(2013)</a></li>
<li><a href="#YouTubeNet">YouTubeNet(2016)</a></li>
<li><a href="#SDM">SDM(2019)</a></li>
<li><a href="#MIND">MIND(2019)</a></li>
<li><a href="#ComiRec">ComiRec(2020)</a></li>
</ul>
</li>
<li>Bias<ul>
<li><a href="#CD2AN">CD2AN(2022)</a></li>
</ul>
</li>
<li>List Wise<ul>
<li><a href="#PRM">PRM(2019)</a></li>
</ul>
</li>
</ul>
<h1 id="Other-Papers"><a href="#Other-Papers" class="headerlink" title="Other Papers"></a>Other Papers</h1><ul>
<li>Evaluation<ul>
<li><a href="#IEOE">Interpretable Evaluation for Offline Evaluation (IEOE)</a></li>
</ul>
</li>
</ul>
<h1 id="AutoRec"><a href="#AutoRec" class="headerlink" title="AutoRec"></a>AutoRec</h1><p>(2015)</p>
<p>AutoRec, a new CF model based on the autoencoder paradigm.</p>
<p>In rating-based collaborative filtering, we have m users, n items, and a partially observed user-item rating matrix R âˆˆ RmÃ—n.</p>
<p>Our aim in this work is to design an item-based (user-based) autoencoder which can take as input each partially observed r(i) (r(u)), project it into a low-dimensional latent (hidden) space, and then reconstruct r(i) (r(u)) in the output space to predict missing ratings for purposes of recommendation.</p>
<blockquote>
<p>h(r; Î¸) &#x3D; f (W Â· g(Vr + Î¼) + b)</p>
</blockquote>
<div style="display: flex; justify-content: space-evenly;"><img src="1.png" alt="AutoRec" height="300"></div>

<ul>
<li>First, we account for the fact that each r(i) is partially observed by only updating during backpropagation those weights that are associated with observed inputs, as is common in matrix factorisation and RBM approaches.</li>
<li>Second, we regularise the learned parameters so as to prevent overfitting on the observed ratings.</li>
</ul>
<div style="display: flex; justify-content: space-evenly;"><img src="2.png" alt="" height="50"></div>

<blockquote>
<p>regularisation strength Î» &gt; 0</p>
</blockquote>
<div style="display: flex; justify-content: space-evenly;"><img src="3.png" alt="" height="200"></div>

<h1 id="Deep-Crossing"><a href="#Deep-Crossing" class="headerlink" title="Deep Crossing"></a>Deep Crossing</h1><p>(2016)</p>
<p>This paper proposes the Deep Crossing model which is a deep neural network that automatically combines features to produce superior models.</p>
<p>The important crossing features are discovered implicitly by the networks, which are comprised of an embedding and stacking layer, as well as a cascade of Residual Units.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="4.png" alt="" height="200"></div>

<h4 id="Embedding-and-Stacking-Layers-Embedding-is-applied-per-individual-feature-to-transform-the-input-features"><a href="#Embedding-and-Stacking-Layers-Embedding-is-applied-per-individual-feature-to-transform-the-input-features" class="headerlink" title="Embedding and Stacking Layers, Embedding is applied per individual feature to transform the input features."></a>Embedding and Stacking Layers, Embedding is applied per individual feature to transform the input features.</h4><div style="display: flex; justify-content: space-evenly;"><img src="5.png" alt="" height="50"></div>

<h4 id="Residual-Layers"><a href="#Residual-Layers" class="headerlink" title="Residual Layers"></a>Residual Layers</h4><!-- <div style="display: flex; justify-content: space-evenly;"><img src="6.png" alt="" height="50"></div> -->

<div style="display: flex; justify-content: space-evenly;"><img src="7.png" alt="" height="200"></div>


<h1 id="Neural-CF"><a href="#Neural-CF" class="headerlink" title="Neural CF"></a>Neural CF</h1><p>(2017)</p>
<p>In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation â€” collaborative filtering â€” on the basis of implicit feedback.</p>
<p>By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural networkbased Col laborative Filtering.</p>
<p>Among the various collaborative filtering techniques, matrix factorization (MF) [14, 21] is the most popular one, which projects users and items into a shared latent space, using a vector of latent features to represent a user or an item. Thereafter a userâ€™s interaction on an item is modelled as the inner product of their latent vectors.</p>
<p>Despite the effectiveness of MF for collaborative filtering, it is well-known that its performance can be hindered by the simple choice of the interaction function inner product.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="8.png" alt="" height="300"></div>

<p>This paper explores the use of deep neural networks for learning the interaction function from data.</p>
<h4 id="General-Framework"><a href="#General-Framework" class="headerlink" title="General Framework"></a>General Framework</h4><div style="display: flex; justify-content: space-evenly;"><img src="9.png" alt="" height="300"></div>

<p>Above the input layer is the embedding layer; it is a fully connected layer that projects the sparse representation to a dense vector</p>
<p>Fusion of GMF and MLP, Generalized Matrix Factorization (GMF)</p>
<div style="display: flex; justify-content: space-evenly;"><img src="10.png" alt="" height="300"></div>

<p>We first train GMF and MLP with random initializations until convergence. We then use their model parameters as the initialization for the corresponding parts of NeuMFâ€™s parameters.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="11.png" alt="" height="300"></div>



<h1 id="PNN"><a href="#PNN" class="headerlink" title="PNN"></a>PNN</h1><p>(2016)</p>
<p>In this paper, we propose a Product-based Neural Networks (PNN) with an embedding layer to learn a distributed representation of the categorical data, a product layer to capture interactive patterns between interfield categories, and further fully connected layers to explore high-order feature interactions.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="12.png" alt="" height="400"></div>

<h4 id="Product-based-Neural-Network-PNN"><a href="#Product-based-Neural-Network-PNN" class="headerlink" title="Product-based Neural Network (PNN):"></a>Product-based Neural Network (PNN):</h4><!-- (i) starts from an embedding layer without pretraining as used in [12] -->
<p>builds a product layer based on the embedded feature vectors to model the inter-field feature interactions</p>
<!-- (iii) further distills the high-order feature patterns with fully connected MLPs. -->

<p>In this paper, we propose two variants of PNN, namely IPNN and OPNN,<br>Inner Product-based Neural Network (IPNN), which takes a pair of vectors as input and outputs a scalar.<br>Outer Product-based Neural Network (OPNN), which takes a pair of vectors and produces a matrix.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="13.png" alt="" height="200"></div>



<h1 id="Wide-Deep"><a href="#Wide-Deep" class="headerlink" title="Wide&amp;Deep"></a>Wide&amp;Deep</h1><p>(2016)</p>
<p>One challenge in recommender systems, similar to the general search ranking problem, is to achieve both memorization and generalization. </p>
<p>Memorization can be loosely defined as learning the frequent co-occurrence of items or features and exploiting the correlation available in the historical data. Generalization, on the other hand, is based on transitivity of correlation and explores new feature combinations that have never or rarely occurred in the past.</p>
<p>Memorization can be achieved effectively using cross-product transformations over sparse features. One limitation of cross-product transformations is that they do not generalize to query-item feature pairs that have not appeared in the training data.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="14.png" alt="" height="300"></div>

<p>The combined modelâ€™s prediction is:</p>
<div style="display: flex; justify-content: space-evenly;"><img src="15.png" alt="" height="50"></div>

<h4 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h4><div style="display: flex; justify-content: space-evenly;"><img src="16.png" alt="" height="200"></div>


<h1 id="DeepFM"><a href="#DeepFM" class="headerlink" title="DeepFM"></a>DeepFM</h1><p>(2017)</p>
<p>The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture.<br>Compared to the latest Wide &amp; Deep model from Google, DeepFM has a shared input to its â€œwideâ€ and â€œdeepâ€ parts, with no need of feature engineering besides raw features.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="17.png" alt="" height="300"></div>

<h4 id="FM-Component"><a href="#FM-Component" class="headerlink" title="FM Component"></a>FM Component</h4><p>The output of FM is the summation of an Addition unit and a number of Inner Product units</p>
<div style="display: flex; justify-content: space-evenly;"><img src="19.png" alt="" height="80"></div>

<h4 id="Deep-Component"><a href="#Deep-Component" class="headerlink" title="Deep Component"></a>Deep Component</h4><p>The deep component is a feed-forward neural network, which is used to learn high-order feature interactions.</p>
<h4 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h4><p>It is worth pointing out that FM component and deep component share the same feature embedding</p>
<div style="display: flex; justify-content: space-evenly;"><img src="20.png" alt="" height="40"></div>

<p>Relationship with the other Neural Networks</p>
<h4 id="FNN-PNN-Wide-Deep"><a href="#FNN-PNN-Wide-Deep" class="headerlink" title="FNN, PNN, Wide&amp;Deep"></a>FNN, PNN, Wide&amp;Deep</h4><div style="display: flex; justify-content: space-evenly;"><img src="18.png" alt="" height="200"></div>

<div style="display: flex; justify-content: space-evenly;"><img src="21.png" alt="" height="150"></div>

<p>Performance</p>
<div style="display: flex; justify-content: space-evenly;"><img src="22.png" alt="" height="200"></div>

<h1 id="DCN"><a href="#DCN" class="headerlink" title="DCN"></a>DCN</h1><p>(2017)</p>
<p>In this paper, we propose the Deep &amp; Cross Network (DCN) which keeps the benefits of a DNN model, and beyond that, it introduces a novel cross network that is more efficient in learning certain bounded-degree feature interactions.</p>
<p>In the Kaggle competition, the manually craÂ‰ed features in many winning solutions are low-degree, in an explicit format and effective.</p>
<p>The wide-and-deep [4] is a model in this spirit. It takes cross features as inputs to a linear model, and jointly trains the linear model with a DNN model.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="23.png" alt="" height="500"></div>

<p>Embedding and Stacking Layer: Embedding matrix &amp; embedding vector</p>
<h4 id="Cross-Network"><a href="#Cross-Network" class="headerlink" title="Cross Network"></a>Cross Network</h4><p>The cross network is composed of cross layers, with each layer having the following formula:</p>
<div style="display: flex; justify-content: space-evenly;"><img src="24.png" alt="" height="50"></div>

<p>A visualization of one cross layer is shown in Figure 2.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="25.png" alt="" height="300"></div>

<h4 id="Performance-1"><a href="#Performance-1" class="headerlink" title="Performance"></a>Performance</h4><div style="display: flex; justify-content: space-evenly;"><img src="26.png" alt="" height="200"></div>

<h1 id="xDeepFM"><a href="#xDeepFM" class="headerlink" title="xDeepFM"></a>xDeepFM</h1><p>(2018)</p>
<p>In this paper, we propose a novel Compressed Interaction Network (CIN), which aims to generate feature interactions in an explicit fashion and at the vector-wise level.</p>
<p>In this paper, we propose a novel Compressed Interaction Network (CIN), which aims to generate feature interactions in an explicit fashion and at the vector-wise level.</p>
<p>We thus design a novel compressed interaction network (CIN) to replace the cross network in the DCN.</p>
<h4 id="Embedding-Layer"><a href="#Embedding-Layer" class="headerlink" title="Embedding Layer"></a>Embedding Layer</h4><div style="display: flex; justify-content: space-evenly;"><img src="27.png" alt="" height="200"></div>

<h4 id="Implicit-High-order-Interactions-Wide-Deep"><a href="#Implicit-High-order-Interactions-Wide-Deep" class="headerlink" title="Implicit High-order Interactions: Wide&amp;Deep"></a>Implicit High-order Interactions: Wide&amp;Deep</h4><h4 id="Explicit-High-order-Interactions-DCN"><a href="#Explicit-High-order-Interactions-DCN" class="headerlink" title="Explicit High-order Interactions: DCN"></a>Explicit High-order Interactions: DCN</h4><div style="display: flex; justify-content: space-evenly;"><img src="28.png" alt="" height="300"></div>

<p>Compressed Interaction Network (CIN), with the following considerations:<br>(1) interactions are applied at vector-wise level, not at bit-wise level;<br>(2) high-order feature interactions is measured explicitly;<br>(3) the complexity of network will not grow exponentially with the degree of interactions.</p>
<p>we formulate the output of field embedding as a matrix X0 âˆˆ RmÃ—D, where the i-th row in X0 is the embedding vector of the i-th field.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="29.png" alt="" height="400"></div>


<h4 id="Performance-2"><a href="#Performance-2" class="headerlink" title="Performance"></a>Performance</h4><div style="display: flex; justify-content: space-evenly;"><img src="30.png" alt="" height="300"></div>


<h1 id="NFM"><a href="#NFM" class="headerlink" title="NFM"></a>NFM</h1><p>(2017)</p>
<p>In this work, we propose a novel model for sparse data prediction named Neural Factorization Machines (NFMs), which enhances FMs by modelling higher-order and non-linear feature interactions. </p>
<p>By devising a new operation in neural network modelling â€” Bilinear Interaction (Bi-Interaction) pooling â€” we subsume FM under the neural network framework for the first time.</p>
<blockquote>
<p>Expressiveness Limitation of FM: FM still belongs to the family of (multivariate) linear models.</p>
</blockquote>
<h4 id="The-NFM-Model"><a href="#The-NFM-Model" class="headerlink" title="The NFM Model"></a>The NFM Model</h4><div style="display: flex; justify-content: space-evenly;"><img src="30.png" alt="" height="300"></div>

<h4 id="Bi-Interaction-Layer"><a href="#Bi-Interaction-Layer" class="headerlink" title="Bi-Interaction Layer"></a>Bi-Interaction Layer</h4><p>Clearly, the output of Bi-Interaction pooling is a k-dimension vector that encodes the second-order interactions between features in the embedding space.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="31.png" alt="" height="80"></div>


<h1 id="AFM"><a href="#AFM" class="headerlink" title="AFM"></a>AFM</h1><p>(2017)</p>
<p>We propose a novel model named Attentional Factorization Machine (AFM), which learns the importance of each feature interaction from data via a neural attention network</p>
<div style="display: flex; justify-content: space-evenly;"><img src="32.png" alt="" height="300"></div>

<h4 id="Pair-wise-Interaction-Layer"><a href="#Pair-wise-Interaction-Layer" class="headerlink" title="Pair-wise Interaction Layer"></a>Pair-wise Interaction Layer</h4><div style="display: flex; justify-content: space-evenly;"><img src="33.png" alt="" height="50"></div>


<h4 id="Attention-based-Pooling-Layer"><a href="#Attention-based-Pooling-Layer" class="headerlink" title="Attention-based Pooling Layer"></a>Attention-based Pooling Layer</h4><p>We propose to employ the attention mechanism on feature interactions by performing a weighted sum on the interacted vectors:</p>
<div style="display: flex; justify-content: space-evenly;"><img src="34.png" alt="" height="50"></div>

<div style="display: flex; justify-content: space-evenly;"><img src="35.png" alt="" height="100"></div>

<h1 id="DIN"><a href="#DIN" class="headerlink" title="DIN"></a>DIN</h1><p>(2018)</p>
<p>In this paper, we propose a novel model: Deep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors</p>
<blockquote>
<p>It is not necessary to compress all the diverse interests of a certain user into the same vector when predicting a candidate ad because only part of userâ€™s interests will influence his&#x2F;her action (to click or not to click).</p>
</blockquote>
<p>Besides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters.</p>
<ul>
<li><p>In this paper, we develop a novel mini-batch aware regularization where only parameters of non-zero features appearing in each mini-batch participate in the calculation of L2-norm, making the computation acceptable.</p>
</li>
<li><p>Besides, we design a data adaptive activation function, which generalizes commonly used PReLU[12] by adaptively adjusting the rectified point w.r.t. distribution of inputs and is shown to be helpful for training industrial networks with sparse features.</p>
</li>
</ul>
<p>DIN designs a local activation unit to soft-search for relevant user behaviors and takes a weighted sum pooling to obtain the adaptive representation of user interests with respect to a given ad.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="36.png" alt="" height="400"></div>


<p>Local activation unit of Eq.(3) shares similar ideas with attention methods which are developed in NMT task[1]. However different from traditional attention method, the constraint is relaxed in Eq.(3), aiming to reserve the intensity of user interests. That is, normalization with softmax on the output of a(Â·) is abandoned.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="37.png" alt="" height="80"></div>


<h4 id="Mini-batch-Aware-Regularization"><a href="#Mini-batch-Aware-Regularization" class="headerlink" title="Mini-batch Aware Regularization"></a>Mini-batch Aware Regularization</h4><p>In this paper, we introduce an efficient mini-batch aware regularizer, which only calculates the L2-norm over the parameters of sparse features appearing in each mini-batch and makes the computation possible.</p>
<h4 id="Data-Adaptive-Activation-Function"><a href="#Data-Adaptive-Activation-Function" class="headerlink" title="Data Adaptive Activation Function"></a>Data Adaptive Activation Function</h4><div style="display: flex; justify-content: space-evenly;"><img src="38.png" alt="" height="200"></div>
<div style="display: flex; justify-content: space-evenly;"><img src="39.png" alt="" height="80"></div>
<div style="display: flex; justify-content: space-evenly;"><img src="40.png" alt="" height="80"></div>

<h4 id="Performance-3"><a href="#Performance-3" class="headerlink" title="Performance"></a>Performance</h4><div style="display: flex; justify-content: space-evenly;"><img src="41.png" alt="" height="200"></div>


<h1 id="DIEN"><a href="#DIEN" class="headerlink" title="DIEN"></a>DIEN</h1><p>(2018)</p>
<p>to target item, and obtains adaptive interest representation. However, most interest models including DIN regard the behavior as the interest directly, while latent interest is hard to be fully reflected by explicit behavior.</p>
<p>Based on all these observations, we propose Deep Interest Evolution Network (DIEN) to improve the performance of CTR prediction. There are two key modules in DIEN, one is for extracting latent temporal interests from explicit user behaviors, and the other one is for modeling interest evolving process.</p>
<p>We propose auxiliary loss which uses the next behavior to supervise the learning of current hidden state. We call these hidden states with extra supervision as interest states.</p>
<p>We take GRU to model the dependency between behaviors, where the input of GRU is ordered behaviors by their occur time.</p>
<p>We propose auxiliary loss, which uses behavior bt+1 to supervise the learning of interest state ht.</p>
<p>Besides using the real next behavior as positive instance, we also use negative instance that samples from item set except the clicked item.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="43.png" alt="" height="400"></div>


<h4 id="Interest-Extractor-Layer"><a href="#Interest-Extractor-Layer" class="headerlink" title="Interest Extractor Layer"></a>Interest Extractor Layer</h4><p>The formulations of GRU are listed as follows:</p>
<div style="display: flex; justify-content: space-evenly;"><img src="42.png" alt="" height="150"></div>

<p>So we propose auxiliary loss, which uses behavior bt+1 to supervise the learning of interest state ht. Besides using the real next behavior as positive instance, we also use negative instance that samples from item set except the clicked item.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="44.png" alt="" height="150"></div>

<h4 id="Interest-Evolving-Layer"><a href="#Interest-Evolving-Layer" class="headerlink" title="Interest Evolving Layer"></a>Interest Evolving Layer</h4><p>GRU with attentional update gate (AUGRU): </p>
<div style="display: flex; justify-content: space-evenly;"><img src="45.png" alt="" height="80"></div>

<div style="display: flex; justify-content: space-evenly;"><img src="46.png" alt="" height="80"></div>


<h4 id="Performance-4"><a href="#Performance-4" class="headerlink" title="Performance"></a>Performance</h4><div style="display: flex; justify-content: space-evenly;"><img src="47.png" alt="" height="200"></div>


<h1 id="AutoInt"><a href="#AutoInt" class="headerlink" title="AutoInt"></a>AutoInt</h1><p>(2019)</p>
<p>In this paper, we propose an effective and efficient method called the AutoInt to automatically learn the high-order feature interactions of input features.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="48.png" alt="" height="300"></div>


<h4 id="Embedding-Layer-1"><a href="#Embedding-Layer-1" class="headerlink" title="Embedding Layer"></a>Embedding Layer</h4><p>Our proposed approach learns effective low-dimensional representations of the sparse and highdimensional input features and is applicable to both the categorical and&#x2F;or numerical input features.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="49.png" alt="" height="200"></div>


<h4 id="Interacting-Layer"><a href="#Interacting-Layer" class="headerlink" title="Interacting Layer"></a>Interacting Layer</h4><p>We move to model high-order combinatorial features in the space, in this paper, we tackle this problem with a novel method, the multi-head self-attention mechanism.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="50.png" alt="" height="300"></div>

<p>We first define the correlation between feature m and feature k under a specific attention head h as follows:</p>
<div style="display: flex; justify-content: space-evenly;"><img src="51.png" alt="" height="100"></div>
<div style="display: flex; justify-content: space-evenly;"><img src="52.png" alt="" height="70"></div>
<div style="display: flex; justify-content: space-evenly;"><img src="53.png" alt="" height="50"></div>


<p>where Ïˆ is an attention function which defines the similarity between the feature m and k, âŠ• is the concatenation operator, and H is the number of total heads.</p>
<h4 id="Performance-5"><a href="#Performance-5" class="headerlink" title="Performance"></a>Performance</h4><div style="display: flex; justify-content: space-evenly;"><img src="54.png" alt="" height="300"></div>


<h1 id="DMT"><a href="#DMT" class="headerlink" title="DMT"></a>DMT</h1><p>(2020)</p>
<p>We propose Deep Multifaceted Transformers (DMT), a novel framework that can model usersâ€™ multiple types of behavior sequences simultaneously with multiple Transformers. It utilizes Multi-gate Mixture-of-Experts to optimize multiple objectives.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="55.png" alt="" height="450"></div>


<h4 id="Input-and-Embedding-Layers"><a href="#Input-and-Embedding-Layers" class="headerlink" title="Input and Embedding Layers"></a>Input and Embedding Layers</h4><p>Usersâ€™ diverse behavior sequences, the sequence of each type of usersâ€™ behaviors is represented by a variable-length sequence of items.</p>
<p>Deep Multifaceted Transformers</p>
<p>To capture each userâ€™s multifaceted interest, we use three separate Deep Interest Transformers (they have different parameters) to model the userâ€™s click sequence, cart sequence and order sequence, and learn the userâ€™s short-term, middle-term and long-term interest vectors respectively.</p>
<p>The basic idea is that usersâ€™ multiple types of behavior sequences on items (e.g., click, add to cart and order) are significantly difierent and they have different timescales.</p>
<h4 id="Deep-Interest-Transformer"><a href="#Deep-Interest-Transformer" class="headerlink" title="Deep Interest Transformer"></a>Deep Interest Transformer</h4><p>Self-attention blocks</p>
<div style="display: flex; justify-content: space-evenly;"><img src="58.png" alt="" height="80"></div>
<div style="display: flex; justify-content: space-evenly;"><img src="56.png" alt="" height="80"></div>

<p>Positional encoding</p>
<ul>
<li>Sinusoidal Positional Embedding (pos_sincos)</li>
<li>Learned Positional Embedding (pos_learn)</li>
</ul>
<h4 id="Multi-gate-Mixture-of-Experts-Layers"><a href="#Multi-gate-Mixture-of-Experts-Layers" class="headerlink" title="Multi-gate Mixture-of-Experts Layers"></a>Multi-gate Mixture-of-Experts Layers</h4><p>The multiple objectives may have complex relationships (e.g., independent, related or confiict) with each other, and the commonly used Shared-bottom [23] architecture may harm the learning of multiple objectives. To capture the relation and confiict of multiple tasks, we adopt Multi-gate Mixture-of-Experts (MMoE) [38] for multi-objective ranking.</p>
<p>For each task :, as shown in Equation 3, firstly, it exploits a gating network to learn the weights of each expert. Secondly, it calculates the weighted sum of experts outputs. Finally, it feds into a utility network to get the utility for task. The gating networks and utility networks are implemented by multi-layer perceptrons, and their parameters are different for each task.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="57.png" alt="" height="150"></div>


<h4 id="Bias-Deep-Neural-Network"><a href="#Bias-Deep-Neural-Network" class="headerlink" title="Bias Deep Neural Network"></a>Bias Deep Neural Network</h4><p>In this paper, we investigate two types of selection bias in e-commerce Recommender Systems: Position bias and Neighboring bias.</p>
<ul>
<li>Position bias. The â€œposition biasâ€ means that users tend to click items which are displayed closer to the top of the list.</li>
<li>Neighboring bias. The â€œneighboring biasâ€ means that the probability of clicking a item may be influenced by its neighboring products.</li>
</ul>
<p>DMT uses a Bias Deep Neural Network to model the selection bias. The input of the networks are bias features. For the position bias, the input is the number of index (â€œPosition_indexâ€ bias) or page (â€œPosition_pageâ€ bias) of the target item. For the neighboring bias, the input is the categories of the target item and its nearest neighboring items.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="59.png" alt="" height="40"></div>

<h4 id="Performance-6"><a href="#Performance-6" class="headerlink" title="Performance"></a>Performance</h4><div style="display: flex; justify-content: space-evenly;"><img src="60.png" alt="" height="300"></div>

<p>Position bias:</p>
<p>From this table, we can find that: The Learned Positional Embedding method achieves the best performance compared with other methods. So we use this method in the experiments.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="61.png" alt="" height="200"></div>

<p>MMoE:</p>
<p>we find that simultaneously modeling modeling cart sequence can achieve better performance than singly modeling the click sequence. However, if we further adding the order sequence, the performance for click prediction and order prediction doesnâ€™t increase further.</p>
<p>We empirically find that: For a product that have a long repurchase period (i.e., computer), a user will tend to click but not purchase the product again in short time after buying it. For a product that have a short repurchase period (i.e., milk), the user may both click and purchase it again in short time. The order sequence may lead to the confiict between click prediction and order prediction and disturb the information in the click or cart sequence.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="63.png" alt="" height="300"></div>


<h1 id="SIM"><a href="#SIM" class="headerlink" title="SIM"></a>SIM</h1><p>(2020)</p>
<p>However, MIMN fails to precisely capture user interests given a specific candidate item when the length of user behavior sequence increases further, say, by 10 times or more.</p>
<p>In this paper, we tackle this problem by designing a new modeling paradigm, which we name as Search-based Interest Model (SIM).</p>
<p>Rich user behavior data is proven to be of great value [8]. For example, 23% of users in Taobao, one of the worldâ€™s leading e-commerce site, click with more than 1000 products in last 5 months[8, 10]. The key idea of DIN is searching the effective information from user behaviors to model special interest of user, facing different candidate items. But the searching formula of DIN costs an unacceptable computation and storage facing the long sequential user behavior data as we mentioned above.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="64.png" alt="" height="500"></div>

<h4 id="General-Search-Unit"><a href="#General-Search-Unit" class="headerlink" title="General Search Unit"></a>General Search Unit</h4><div style="display: flex; justify-content: space-evenly;"><img src="65.png" alt="" height="70"></div>

<p>Hard-search:</p>
<p>The hard-search model is non-parametric. Only behavior belongs to the same category as the candidate item will be selected and aggregated as a sub behavior sequence to be sent to the exact search unit.</p>
<p>Soft-search:</p>
<p>In the soft-search model, Wb and Wa are the parameters of weight. ea and ei denote the embedding vectors of target item and i-th behavior bi , respectively. </p>
<p>To further speed up the top-K search over ten thousands length of user behaviors, sublinear time maximum inner product search method ALSH[12] is conducted based on the embedding vectors E to search the related top-K behaviors with target item.</p>
<p>Note that if the user behavior grows to a certain extent, it is impossible to directly fed the whole user behaviors into the model. In that situation, one can randomly sample sets of sub-sequence from the long sequential user behaviors, which still follows the same distribution of the original one.</p>
<h4 id="Exact-Search-Unit"><a href="#Exact-Search-Unit" class="headerlink" title="Exact Search Unit"></a>Exact Search Unit</h4><p>In the first search stage, top-K related sub user behavior sequence B, and the sequence temporal property for each behavior, specifically, the time intervals D.</p>
<p>We take advantage of multi-head attention to capture the diverse user interest:</p>
<div style="display: flex; justify-content: space-evenly;"><img src="66.png" alt="" height="70"></div>


<h4 id="Online-Serving-System"><a href="#Online-Serving-System" class="headerlink" title="Online Serving System"></a>Online Serving System</h4><p>We build an two-level structured index for each user, which we name as user behavior tree (UBT), as illustrated in Figure 3. Briefly speaking, UBT follows the Key-Key-Value data structure: the first key is user id, the second keys are category ids and the last values are the specific behavior items that belong to each category. UBT is implemented as an distributed system, with size reaching up to 22 TB.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="67.png" alt="" height="400"></div>


<h4 id="Performance-7"><a href="#Performance-7" class="headerlink" title="Performance"></a>Performance</h4><p>SIM achieves significant improvement compared with MIMN because MIMN encodes all unfiltered user historical behaviors into a fixed-length memory which makes it hard to capture diverse long-term interest.</p>
<p>The result shows that the user behaviors reserved by hard-search strategy could cover 75% of that from the soft-search strategy.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="68.png" alt="" height="300"></div>
<div style="display: flex; justify-content: space-evenly;"><img src="69.png" alt="" height="300"></div>


<h1 id="ESMM"><a href="#ESMM" class="headerlink" title="ESMM"></a>ESMM</h1><p>(2018)</p>
<p>In this paper, we model CVR in a brand-new perspective by making good use of sequential pattern of user actions, i.e., impression â†’ click â†’ conversion. The proposed Entire Space Multi-task Model (ESMM) can eliminate the two problems simultaneously by i) modeling CVR directly over the entire space, ii) employing a feature representation transfer learning strategy.</p>
<p>However, there exist several task-specific problems, making CVR modeling challenging. Among them, we report two critical ones encountered in our real practice: </p>
<p>i) sample selection bias (SSB) problem, As illustrated in Fig.1, conventional CVR models are trained on dataset composed of clicked impressions, while are utilized to make inference on the entire space with samples of all impressions.</p>
<p>ii) data sparsity (DS) problem, In practice, data gathered for training CVR model is generally much less than CTR task.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="70.png" alt="" height="300"></div>

<h4 id="CVR-Modeling"><a href="#CVR-Modeling" class="headerlink" title="CVR Modeling"></a>CVR Modeling</h4><p>Eq.(2) tells us that with estimation of pCTCVR and pCTR, pCVR can be derived over the entire input space X, which addresses the sample selection bias problem directly.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="72.png" alt="" height="80"></div>
<div style="display: flex; justify-content: space-evenly;"><img src="73.png" alt="" height="60"></div>


<h4 id="Entire-Space-Multi-Task-Model"><a href="#Entire-Space-Multi-Task-Model" class="headerlink" title="Entire Space Multi-Task Model"></a>Entire Space Multi-Task Model</h4><div style="display: flex; justify-content: space-evenly;"><img src="71.png" alt="" height="500"></div>

<h4 id="Performance-8"><a href="#Performance-8" class="headerlink" title="Performance"></a>Performance</h4><div style="display: flex; justify-content: space-evenly;"><img src="74.png" alt="" height="250"></div>


<h1 id="STEM"><a href="#STEM" class="headerlink" title="STEM"></a>STEM</h1><p>(2018)</p>
<p>In this paper, we introduce a novel paradigm called Shared and Task-specific EMbeddings (STEM) that aims to incorporate both shared and task-specific embeddings to effectively capture task-specific user preferences.</p>
<p>A key challenge in MTL is the occurrence of negative transfer, where the performance of certain tasks deteriorates due to conflicts between tasks.<br>Note that, existing methods tend to treat all samples in a task as a whole, overlooking the inherent intricacies within them. </p>
<div style="display: flex; justify-content: space-evenly;"><img src="75.png" alt="" height="400"></div>

<p>However, to our surprise, in the zone that receives comparable positive feedback from both tasks, the performance of existing methods is inferior to that of the single-task model.</p>
<p>Intuitively, users may possess diverse and sometimes even conflicting preferences for items across various tasks, which become more pronounced when there are sufficient signals from multiple tasks, as in the comparable zone here.</p>
<p>Motivated by the limitations of the shared-embedding paradigm, this paper introduces a Shared and Task-specific EMbeddings (STEM) paradigm. STEM aims to incorporate both shared and task-specific embeddings to learn taskspecific user preferences.</p>
<h4 id="Evolution-of-the-MTL-Model-Architectures"><a href="#Evolution-of-the-MTL-Model-Architectures" class="headerlink" title="Evolution of the MTL Model Architectures"></a>Evolution of the MTL Model Architectures</h4><div style="display: flex; justify-content: space-evenly;"><img src="76.png" alt="" height="250"></div>

<p>As depicted in Figure 1, both MMoE and PLE demonstrate improved performance when there is overwhelming feedback from either task.</p>
<h4 id="STEM-Net"><a href="#STEM-Net" class="headerlink" title="STEM-Net"></a>STEM-Net</h4><div style="display: flex; justify-content: space-evenly;"><img src="77.png" alt="" height="350"></div>

<h4 id="USER-ID-and-ITEM-ID-task-specific"><a href="#USER-ID-and-ITEM-ID-task-specific" class="headerlink" title="USER ID and ITEM ID task-specific"></a>USER ID and ITEM ID task-specific</h4><p>We wonder if we can still achieve decent performance lift when only making embeddings of USER ID and ITEM ID task-specific. We evaluate the performance of the corresponding model STEM-Net-(user id, item id) observing that it also achieves competitive performance with STEM-Net.</p>
<p>We design two new variants, STEM-Net-(user side) and STEM-Net-(item side), and observe the former one exhibit better performance, indicating that user side features are more effective than item in capturing diverse user preference among tasks.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="79.png" alt="" height="650"></div>

<h4 id="Performance-9"><a href="#Performance-9" class="headerlink" title="Performance"></a>Performance</h4><div style="display: flex; justify-content: space-evenly;"><img src="78.png" alt="" height="250"></div>

<h1 id="EDCN"><a href="#EDCN" class="headerlink" title="EDCN"></a>EDCN</h1><p>(2021)</p>
<p>We propose a novel deep CTR model EDCN. EDCN introduces two advanced modules, namely bridge module and regulation module, which work collaboratively to capture the layer-wise interactive signals and learn discriminative feature distributions for each hidden layer of the parallel networks.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="80.png" alt="" height="300"></div>

<p>In this paper, we focus on optimizing the models with parallel structure by enhancing the explicit and implicit feature interactions via information sharing.</p>
<h4 id="Insufficient-sharing-in-hidden-layers"><a href="#Insufficient-sharing-in-hidden-layers" class="headerlink" title="Insufficient sharing in hidden layers"></a>Insufficient sharing in hidden layers</h4><p>We refer such fusion pattern(DCN) as late fusion. With late fusion, explicit and implicit feature interaction networks do not share information in the intermediate hidden layers, which weakens the interactive signals between each other and may easily lead to skewed gradients during the backward propagation.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="81.png" alt="" height="300"></div>

<h4 id="Excessive-sharing-in-network-input"><a href="#Excessive-sharing-in-network-input" class="headerlink" title="Excessive sharing in network input"></a>Excessive sharing in network input</h4><p>Therefore, excessive sharing the network inputs and feeding all the features indiscriminately to the parallel networks may not be a reasonable choice and result in sub-optimal performance.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="82.png" alt="" height="600"></div>

<h4 id="Bridge-Module"><a href="#Bridge-Module" class="headerlink" title="Bridge Module"></a>Bridge Module</h4><p>To overcome this limitation, we introduce a dense fusion strategy, which is implemented by our proposed bridge module, to capture the layer-wise interactive signals between two parallel networks.</p>
<p>the bridge module can be formulated as fğ‘™ &#x3D; ğ‘“ (xğ‘™ , hğ‘™ ), where ğ‘“ (Â·) is a pre-defined interaction function:</p>
<ul>
<li>Pointwise Addition</li>
<li>Hadamard Product</li>
<li>Concatenation</li>
<li>Attention Pooling</li>
</ul>
<h4 id="Regulation-Module"><a href="#Regulation-Module" class="headerlink" title="Regulation Module"></a>Regulation Module</h4><p>(SE-Net) Inspired by the gating mechanism used in MMoE [20], we propose a regulation module, implemented by a field-wise gating network to soft-select discriminative feature distributions for each parallel networks.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="83.png" alt="" height="80"></div>
<div style="display: flex; justify-content: space-evenly;"><img src="84.png" alt="" height="50"></div>

<h4 id="Performance-10"><a href="#Performance-10" class="headerlink" title="Performance"></a>Performance</h4><div style="display: flex; justify-content: space-evenly;"><img src="85.png" alt="" height="400"></div>


<h1 id="STAR"><a href="#STAR" class="headerlink" title="STAR"></a>STAR</h1><p>(2021)</p>
<p>Generally, different domains may share some common user groups and items, and each domain may have its own unique user groups and items.</p>
<p>In this paper, we propose the Star Topology Adaptive Recommender (STAR) model to train a single model to serve all domains by leveraging data from all domains simultaneously, capturing the characteristics of each domain, and modeling the commonalities between different domains.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="86.png" alt="" height="400"></div>

<h4 id="Architecture-Overview"><a href="#Architecture-Overview" class="headerlink" title="Architecture Overview"></a>Architecture Overview</h4><p>We propose Star Topology Adaptive Recommender (STAR) for multi-domain CTR prediction to better utilize the similarity among different domains while capturing the domain distinction. </p>
<div style="display: flex; justify-content: space-evenly;"><img src="87.png" alt="" height="600"></div>

<p>As shown in Figure 4, STAR consists of three main components:<br>(1) the partitioned normalization (PN) which privatizes normalization for examples from different domains,<br>(2) the star topology fully-connected neural network (star topology FCN),<br>(3) the auxiliary network that treats the domain indicator directly as the input feature and learns its semantic embeddings to capture the domain distinction.</p>
<h4 id="Partitioned-Normalization"><a href="#Partitioned-Normalization" class="headerlink" title="Partitioned Normalization"></a>Partitioned Normalization</h4><p>Among all normalization methods, batch normalization (BN) [14] is a representative method that is proved to be crucial to the successful training of very deep neural networks [14, 31].</p>
<div style="display: flex; justify-content: space-evenly;"><img src="88.png" alt="" height="80"></div>

<p>During testing, moving averaged statistics of mean ğ¸ and variance ğ‘‰ ğ‘ğ‘Ÿ across all samples are used instead</p>
<div style="display: flex; justify-content: space-evenly;"><img src="89.png" alt="" height="80"></div>

<p>To capture the unique data characteristic of each domain, we propose partitioned normalization (PN) which privatizes normalization statistics and parameters for different domains.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="90.png" alt="" height="70"></div>

<p>Besides the modification of the scale and bias, PN also let different domains to accumulate the domain-specific moving average of mean ğ¸ğ‘ and variance ğ‘‰ ğ‘ğ‘Ÿğ‘</p>
<div style="display: flex; justify-content: space-evenly;"><img src="91.png" alt="" height="80"></div>

<h4 id="Star-Topology-FCN"><a href="#Star-Topology-FCN" class="headerlink" title="Star Topology FCN"></a>Star Topology FCN</h4><div style="display: flex; justify-content: space-evenly;"><img src="92.png" alt="" height="400"></div>

<p>As depicted in Figure 5, the proposed star topology FCN consists of a shared centered FCN and independent FCNs per domain, thus the total number of FCN is ğ‘€ + 1. </p>
<p>The final model of ğ‘-th domain is obtained by combining the shared centered FCN and domain-specific FCN, in which the centered parameters learn general behaviors among all domains, and the domain-specific parameters capture specific behaviors in different domains to facilitate more refined CTR prediction.</p>
<p>The final weights ğ‘Š â˜… ğ‘– and bias ğ‘â˜… ğ‘– for the ğ‘-th domain is obtained by:</p>
<div style="display: flex; justify-content: space-evenly;"><img src="93.png" alt="" height="50"></div>


<h4 id="Auxiliary-Network"><a href="#Auxiliary-Network" class="headerlink" title="Auxiliary Network"></a>Auxiliary Network</h4><p>We argue that a good multi-domain CTR model should have the following characteristic: (1) have informative features regarding the domain characteristic (2) make these features easily and directly influence the final CTR prediction.</p>
<p>To this end, we propose an auxiliary network to learn the domain distinction. To augment informative features regarding the domain characteristic, we treat the domain indicator directly as the ID feature input.</p>
<p>The domain indicator is first mapped into embedding vector and concatenated with other features. The auxiliary network then computes forward pass with respect to the concatenated features to gets the one-dimensional output ğ‘ ğ‘.</p>
<h4 id="Performance-11"><a href="#Performance-11" class="headerlink" title="Performance"></a>Performance</h4><div style="display: flex; justify-content: space-evenly;"><img src="95.png" alt="" height="400"></div>


<h1 id="AutoGroup"><a href="#AutoGroup" class="headerlink" title="AutoGroup"></a>AutoGroup</h1><p>(2020)</p>
<p>An end-to-end model, AutoGroup, is proposed, which casts the selection of feature interactions as a structural optimization problem.</p>
<p>The main contribution of AutoGroup is that it performs both dimensionality reduction and feature selection which are not seen in previous models.</p>
<blockquote>
<p>In real-world scenarios, useful feature interactions are usually sparse relative to the combination space of raw features</p>
</blockquote>
<p>Modelling with AutoGroup consists of three stages: </p>
<ul>
<li>the Automatic Feature Grouping Stage, </li>
<li>the Interaction Stage, </li>
<li>and the MLP Stage.</li>
</ul>
<div style="display: flex; justify-content: space-evenly;"><img src="96.png" alt="" height="400"></div>

<h4 id="Automatic-Feature-Grouping-Stage"><a href="#Automatic-Feature-Grouping-Stage" class="headerlink" title="Automatic Feature Grouping Stage"></a>Automatic Feature Grouping Stage</h4><p>The simple strategy of enumerating all feature interactions makes neural networks difficult to achieve the optimal set of network weights.</p>
<p>The main idea is to find multiple small sets of features, where the feature interactions in each such identified feature set are effective at a given order.</p>
<p>As shown in Figure 1, the selection of feature sets can be viewed as a structural optimization problem: each feature is allowed to be selected by any feature set where each feature set groups related features together such that their interaction is likely to improve prediction performance.</p>
<p>We relax the binary choice of a feature being selected by a feature set to a softmax over the two possibilities:<br>(to indicate whether feature fi is selected into feature set <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.991ex;" xmlns="http://www.w3.org/2000/svg" width="2.054ex" height="2.809ex" role="img" focusable="false" viewBox="0 -803.3 907.7 1241.4" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-1-TEX-I-1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path><path id="MJX-1-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><use data-c="1D460" xlink:href="#MJX-1-TEX-I-1D460"></use></g><g data-mml-node="TeXAtom" transform="translate(502,490.8) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D45D" xlink:href="#MJX-1-TEX-I-1D45D"></use></g></g><g data-mml-node="TeXAtom" transform="translate(502,-293.8) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D457" xlink:href="#MJX-1-TEX-I-1D457"></use></g></g></g></g></g></svg></mjx-container>  at the pth-order.).</p>
<div style="display: flex; justify-content: space-evenly;"><img src="97.png" alt="" height="60"></div>

<p>To make the operation differentiable, we relax it with a softmax:</p>
<h4 id="Interaction-Stage"><a href="#Interaction-Stage" class="headerlink" title="Interaction Stage"></a>Interaction Stage</h4><p>Specifically, the representation of a feature set is the weighted sum of representations of the features in this set.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="98.png" alt="" height="60"></div>

<p>In this paper, we generalize the reformulation (Eq. 7) for the pth-order interaction as:</p>
<div style="display: flex; justify-content: space-evenly;"><img src="99.png" alt="" height="60"></div>

<h4 id="Performance-12"><a href="#Performance-12" class="headerlink" title="Performance"></a>Performance</h4><div style="display: flex; justify-content: space-evenly;"><img src="100.png" alt="" height="200"></div>


<h1 id="CD2AN"><a href="#CD2AN" class="headerlink" title="CD2AN"></a>CD2AN</h1><p>(2022)</p>
<p>From the popularity distribution shift perspective, the normal paradigm trained on exposed items (most are hot items) identifies that recommending popular items more frequently can achieve lower loss, thus injecting popularity information into item property embedding.</p>
<blockquote>
<p>Existing work addresses this issue with inverse propensity scoring (IPS) or causal embeddings. </p>
</blockquote>
<p>However, we argue that not all popularity biases mean bad effects, We propose a co-training disentangled domain adaptation network (CD2AN), which can co-train both biased and unbiased models.</p>
<p>Specifically, for popularity distribution shift, CD2AN disentangles item property representation and popularity representation from item property embedding. </p>
<p>For long-tail distribution shift, we introduce additional unexposed items (most are long-tail items) to align the distribution of hot and long-tail item property representations.</p>
<p>Further, from the instances perspective, we carefully design the item similarity regularization to learn comprehensive item representation, which encourages item pairs with more effective co-occurrences patterns to have more similar item property representations.</p>
<p>Existing work eliminates this bias effect with inverse propensity scoring (IPS) or causal embeddings. </p>
<p>(1) IPS, which employs balanced training by re-weighting the interactions [14, 33].</p>
<p>(2) Causal embeddings, which formulate a causal graph to describe the important cause-effect relations in the recommendation process.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="101.png" alt="" height="300"></div>


<p>The key challenge is how to alleviate popularity and long-tail distribution shifts to obtain an unbiased and comprehensive item representations.</p>
<p>We design a feature disentangling module with orthogonal regularization and popularity similarity regularization to separate the popularity representation and the item property representation from the item property embedding injected with the popularity information.</p>
<h4 id="Feature-Disentangling-Module"><a href="#Feature-Disentangling-Module" class="headerlink" title="Feature Disentangling Module"></a>Feature Disentangling Module</h4><p>Based on this training paradigm, there are two main reasons for the popularity bias. (1) Popularity information is injected into the item property embedding, which increases the vector length of popular items, making inner product models score popular items high for every user [41]. (2) Data sparsity makes long-tail items unable to obtain good representations, </p>
<div style="display: flex; justify-content: space-evenly;"><img src="102.png" alt="" height="80"></div>
<div style="display: flex; justify-content: space-evenly;"><img src="103.png" alt="" height="200"></div>

<p>ğ¿ğ‘ğ‘œğ‘_ğ‘ ğ‘–ğ‘š encourages the module to separate out the popularity information injected into ğ‘¬ (ğ‘¥) by constraining ğ’—ğ‘ğ‘œğ‘ ğ‘¥ to be similar to ğ’—ğ‘</p>
<p>ğ¿ğ‘œğ‘Ÿğ‘¡â„ğ‘œğ‘”ğ‘œğ‘›ğ‘ğ‘™ encourages orthogonality between ğ’—ğ‘ğ‘œğ‘ ğ‘¥ and ğ’—ğ‘ğ‘Ÿğ‘œ ğ‘¥ , enabling the popularity and property encoders to encode different aspects of the inputs.</p>
<h4 id="Regularizations-for-Long-tail-Shift"><a href="#Regularizations-for-Long-tail-Shift" class="headerlink" title="Regularizations for Long-tail Shift"></a>Regularizations for Long-tail Shift</h4><p>Domain Alignment</p>
<p>Although item property representations ğ’—ğ‘ğ‘Ÿğ‘œ ğ‘¥ are extracted through FDM, we believe that the popularity bias still exists due to data sparsity. Compared with popular items with sufficient interaction, long-tail items cannot be effectively learned. </p>
<p>Inspired by domain adaptation, we additionally introduce unexposed items ğ‘¥ğ‘˜ (most are long-tail items) randomly sampled from the entire item pool, and adopt Maximum Mean Discrepancy (MMD) regularization [13] to align the distribution of hot and long-tail items:</p>
<p>Instance Alignment</p>
<p>Intuitively, ğ¿ğ‘€ğ‘€ğ· encourages the cluster center of long-tail items to approach the cluster center of hot items.</p>
<p>In the training stage, we additionally introduce the userâ€™s historical interacted items in the item tower. Based on contrastive learning [7, 38], we encourage the items clicked by the same user to be more similar.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="104.png" alt="" height="150"></div>
<div style="display: flex; justify-content: space-evenly;"><img src="105.png" alt="" height="60"></div>


<h4 id="Co-training"><a href="#Co-training" class="headerlink" title="Co-training"></a>Co-training</h4><p>However, we argue that not all popularity biases mean bad effects. The reason why an item is popular maybe that it is of good quality or in line with the current trend.</p>
<p>we co-train unbiased model and biased model to optimize top-k accuracy by adopting the batch softmax loss used in both recommenders [39] and NLP [12]:</p>
<div style="display: flex; justify-content: space-evenly;"><img src="106.png" alt="" height="150"></div>


<h4 id="Performance-13"><a href="#Performance-13" class="headerlink" title="Performance"></a>Performance</h4><div style="display: flex; justify-content: space-evenly;"><img src="107.png" alt="" height="300"></div>


<h1 id="DIHN"><a href="#DIHN" class="headerlink" title="DIHN"></a>DIHN</h1><p>(2022)</p>
<p>In this paper, we present a new recommendation problem, TriggerInduced Recommendation (TIR), where usersâ€™ instant interest can be explicitly induced with a trigger item and follow-up related target items are recommended accordingly.</p>
<p>In TIR scenarios, usersâ€™ instant interest can be explicitly induced with a trigger item.</p>
<ul>
<li>Challenge 1: Usersâ€™ instant interest induced from a trigger item are inherently noisy,</li>
<li>Challenge 2: Users always show multiple interests from their historical behaviors.</li>
</ul>
<p>We propose a novel recommendation method named Deep Interest Highlight Network (DIHN) for Click-Through Rate (CTR) prediction in TIR scenarios. </p>
<p>It has three main components including </p>
<ol>
<li>User Intent Network (UIN), which responds to generate a precise probability score to predict userâ€™s intent on the trigger item; </li>
<li>Fusion Embedding Module (FEM), which adaptively fuses trigger item and target item embeddings based on the prediction from UIN; </li>
<li>Hybrid Interest Extracting Module (HIEM), which can effectively highlight usersâ€™ instant interest from their behaviors based on the result of FEM.</li>
</ol>
<div style="display: flex; justify-content: space-evenly;"><img src="108.png" alt="" height="400"></div>


<h4 id="User-Intent-Network"><a href="#User-Intent-Network" class="headerlink" title="User Intent Network"></a>User Intent Network</h4><p>Referring to the UIN module in Fig.2, where we utilize three categories of features, ğ‘–.ğ‘’, User Profile, User Behaviors, Trigger to estimate the probability.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="109.png" alt="" height="80"></div>

<p>ğ‘¦ âˆˆ {0, 1} is the ground truth label representing whether users clicking the trigger item or not.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="110.png" alt="" height="80"></div>

<h4 id="Fusion-Embedding-Module"><a href="#Fusion-Embedding-Module" class="headerlink" title="Fusion Embedding Module"></a>Fusion Embedding Module</h4><p>A suitable usersâ€™ behavior modelling solution could be adaptively fuse the trigger item with target item.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="111.png" alt="" height="60"></div>

<h4 id="Hybrid-Interest-Extracting-Module"><a href="#Hybrid-Interest-Extracting-Module" class="headerlink" title="Hybrid Interest Extracting Module"></a>Hybrid Interest Extracting Module</h4><p>Hard Sequential Modelling</p>
<p>Following the hard-search mode from SIM [13], we propose the Hard Sequential Modelling (HSM), indicating that only behaviors with the same attribute (ğ‘’.ğ‘”., category, destination) as the trigger item.</p>
<p>Soft Sequential Modelling</p>
<p>Therefore, from another modelling perspective, we proposed the Soft Sequential Modelling (SSM), which adaptively calculates the representation vector of usersâ€™ behaviors with respect to the fusing result from FEM.</p>
<h4 id="Performance-14"><a href="#Performance-14" class="headerlink" title="Performance"></a>Performance</h4><div style="display: flex; justify-content: space-evenly;"><img src="112.png" alt="" height="250"></div>


<h1 id="SEMI"><a href="#SEMI" class="headerlink" title="SEMI"></a>SEMI</h1><p>(2021)</p>
<p>Existing micro-video recommendation methods only focus on usersâ€™ browsing behaviors on micro-videos, but ignore their purchasing intentions in the ecommerce environment.</p>
<blockquote>
<p>In this work, we propose to leverage the product-related user behavior data for micro-video recommendation.</p>
</blockquote>
<p>We propose a novel cross-domain recommendation method, named SEquential Multi-modal Information transfer network (SEMI).</p>
<p>To better bridge the behavior pattern gap between micro-video domain and product domain, we further propose a Cross-domain Contrastive Learning (CCL) algorithm to pre-train encoders to describe sequential user behaviors.</p>
<h4 id="Basic-Components"><a href="#Basic-Components" class="headerlink" title="Basic Components"></a>Basic Components</h4><p>Feature Extractor</p>
<p>We adopt UniVL [25] to fuse the multi-modal information of microvideos. Specifically, we utilize micro-video titles and frames as inputs and pre-train the UniVL model using about 100 million ecommerce micro-videos. The pre-training tasks of UniVL include masked language modeling, masked frame modeling, video-text alignment, and tag classifications.</p>
<p>Similarly, we have the semantic representation for each product ğ‘, which is extracted by the pre-trained UNITER model [9].</p>
<p>Multi-Head Attention Block</p>
<p>â€¦</p>
<p>Multi-Modal Sequence Encoder</p>
<p>As the multi-head attention mechanism is not aware of the sequence order information, we use timestamp embedding.</p>
<h4 id="Cross-Domain-Contrastive-Learning"><a href="#Cross-Domain-Contrastive-Learning" class="headerlink" title="Cross-Domain Contrastive Learning"></a>Cross-Domain Contrastive Learning</h4><p>We propose a Cross-domain Contrastive Learning (CCL) algorithm to pre-train the micro-video and product multi-modal sequence encoders. </p>
<p>The objective of CCL is to encourage the representations of a micro-video sequence and a product sequence to be similar, if they come from the same session of an active user.</p>
<h4 id="Sequential-Multi-Modal-Information-Transfer-Network"><a href="#Sequential-Multi-Modal-Information-Transfer-Network" class="headerlink" title="Sequential Multi-Modal Information Transfer Network"></a>Sequential Multi-Modal Information Transfer Network</h4><p>This is challenging because usersâ€™ behaviors in micro-video domain are more sparse than their behaviors in the product domain.</p>
<p>We feed [ğ‘‰ ğ‘–ğ‘›ğ‘¡ğ‘Ÿğ‘, ğ‘ƒğ‘–ğ‘›ğ‘¡ğ‘Ÿğ‘] into an additional multi-head attention block to capture the inter-domain dependencies, where [Â·] denotes concatenation.</p>
<p>We utilize another multi-head attention blocks over the target micro-video and {ğ‘‰ ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ , ğ‘ƒğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ }.</p>
<h4 id="Performance-15"><a href="#Performance-15" class="headerlink" title="Performance"></a>Performance</h4><div style="display: flex; justify-content: space-evenly;"><img src="113.png" alt="" height="500"></div>


<h4 id="Ablation-Studies"><a href="#Ablation-Studies" class="headerlink" title="Ablation Studies"></a>Ablation Studies</h4><div style="display: flex; justify-content: space-evenly;"><img src="114.png" alt="" height="400"></div>



<h1 id="IEOE"><a href="#IEOE" class="headerlink" title="IEOE"></a>IEOE</h1><p>(2021)</p>
<h1 id="DSSM"><a href="#DSSM" class="headerlink" title="DSSM"></a>DSSM</h1><p>(2013)</p>
<p>In this study we strive to develop a series of new latent semantic models with a deep structure that project queries and documents into a common low-dimensional space where the relevance of a document given a query is readily computed as the distance between them.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="115.png" alt="" height="350"></div>

<p>The semantic relevance score between a query and a document is then measured as:</p>
<div style="display: flex; justify-content: space-evenly;"><img src="116.png" alt="" height="50"></div>

<p>We compute the posterior probability of a document given a query from the semantic relevance score between them through a softmax function:</p>
<div style="display: flex; justify-content: space-evenly;"><img src="117.png" alt="" height="50"></div>

<h1 id="YouTubeNet"><a href="#YouTubeNet" class="headerlink" title="YouTubeNet"></a>YouTubeNet</h1><p>(2016)</p>
<p>In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="118.png" alt="" height="450"></div>


<h1 id="SDM"><a href="#SDM" class="headerlink" title="SDM"></a>SDM</h1><p>(2019)</p>
<p>In this paper, we propose a new sequential deep matching (SDM) model to capture usersâ€™ dynamic preferences by combining short-term sessions and long-term behaviors.</p>
<p>We tackle the following two inherent problems in realworld applications: (1) there could exist multiple interest tendencies in one session. (2) long-term preferences may not be effectively fused with current session interests.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="119.png" alt="" height="450"></div>

<p>We propose to encode behavior sequences with two corresponding components: multi-head self-attention module to capture multiple types of interests and long-short term gated fusion module to incorporate long-term preferences.</p>
<div style="display: flex; justify-content: space-evenly;"><img src="120.png" alt="" height="450"></div>


<h1 id="MIND"><a href="#MIND" class="headerlink" title="MIND"></a>MIND</h1><p>(2019)</p>
<h1 id="ComiRec"><a href="#ComiRec" class="headerlink" title="ComiRec"></a>ComiRec</h1><p>(2020)</p>

            


        </div>
    </div>
    <div id="post-footer" class="post-footer main-content-wrap">
        
            <div class="post-footer-tags">
                <span class="text-color-light text-small">TAGGED IN</span><br/>
                
    <a class="tag tag--primary tag--small t-none-link" href="/tags/RCMD/" rel="tag">RCMD</a>

            </div>
        
        
            <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2024/06/10/RCMD-LLM-Paper/"
                    data-tooltip="RCMD LLM Paper"
                    aria-label="PREVIOUS: RCMD LLM Paper"
                >
                    
                        <i class="fa fa-angle-left" aria-hidden="true"></i>
                        <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                    </a>
            </li>
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2022/08/24/20220824-Shell/"
                    data-tooltip="20220824-Shell"
                    aria-label="NEXT: 20220824-Shell"
                >
                    
                        <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                        <i class="fa fa-angle-right" aria-hidden="true"></i>
                    </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a
                class="post-action-btn btn btn--default btn-open-shareoptions"
                href="#btn-open-shareoptions"
                aria-label="Share this post"
            >
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://www.facebook.com/sharer/sharer.php?u=http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/"
                    title="Share on Facebook"
                    aria-label="Share on Facebook"
                >
                    <i class="fab fa-facebook" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://twitter.com/intent/tweet?text=http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/"
                    title="Share on Twitter"
                    aria-label="Share on Twitter"
                >
                    <i class="fab fa-twitter" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://plus.google.com/share?url=http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/"
                    title="Share on Google+"
                    aria-label="Share on Google+"
                >
                    <i class="fab fa-google-plus" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


        
        
            
        
    </div>
</article>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2024 Joddiy Zhang. All Rights Reserved.
    </span>
</footer>

            </div>
            
                <div id="bottom-bar" class="post-bottom-bar" data-behavior="4">
                    <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2024/06/10/RCMD-LLM-Paper/"
                    data-tooltip="RCMD LLM Paper"
                    aria-label="PREVIOUS: RCMD LLM Paper"
                >
                    
                        <i class="fa fa-angle-left" aria-hidden="true"></i>
                        <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                    </a>
            </li>
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2022/08/24/20220824-Shell/"
                    data-tooltip="20220824-Shell"
                    aria-label="NEXT: 20220824-Shell"
                >
                    
                        <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                        <i class="fa fa-angle-right" aria-hidden="true"></i>
                    </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a
                class="post-action-btn btn btn--default btn-open-shareoptions"
                href="#btn-open-shareoptions"
                aria-label="Share this post"
            >
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://www.facebook.com/sharer/sharer.php?u=http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/"
                    title="Share on Facebook"
                    aria-label="Share on Facebook"
                >
                    <i class="fab fa-facebook" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://twitter.com/intent/tweet?text=http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/"
                    title="Share on Twitter"
                    aria-label="Share on Twitter"
                >
                    <i class="fab fa-twitter" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://plus.google.com/share?url=http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/"
                    title="Share on Google+"
                    aria-label="Share on Google+"
                >
                    <i class="fab fa-google-plus" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


                </div>
                
    <div id="share-options-bar" class="share-options-bar" data-behavior="4">
        <i id="btn-close-shareoptions" class="fa fa-times"></i>
        <ul class="share-options">
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://www.facebook.com/sharer/sharer.php?u=http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/"
                        aria-label="Share on Facebook"
                    >
                        <i class="fab fa-facebook" aria-hidden="true"></i><span>Share on Facebook</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://twitter.com/intent/tweet?text=http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/"
                        aria-label="Share on Twitter"
                    >
                        <i class="fab fa-twitter" aria-hidden="true"></i><span>Share on Twitter</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://plus.google.com/share?url=http://joddiy.cc/2024/01/01/RCMD-Industry-Paper/"
                        aria-label="Share on Google+"
                    >
                        <i class="fab fa-google-plus" aria-hidden="true"></i><span>Share on Google+</span>
                    </a>
                </li>
            
        </ul>
    </div>


            
        </div>
        


    
        
    

<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-times"></i>
        </div>
        
            <img id="about-card-picture" src="/assets/images/14108933.jpeg" alt="Author&#39;s picture"/>
        
            <h4 id="about-card-name">Joddiy Zhang</h4>
        
            <div id="about-card-bio"><p><a href="mailto:&#x6a;&#111;&#100;&#x64;&#x69;&#121;&#x7a;&#104;&#97;&#x6e;&#x67;&#64;&#x67;&#109;&#x61;&#x69;&#x6c;&#x2e;&#x63;&#x6f;&#x6d;">&#x6a;&#111;&#100;&#x64;&#x69;&#121;&#x7a;&#104;&#97;&#x6e;&#x67;&#64;&#x67;&#109;&#x61;&#x69;&#x6c;&#x2e;&#x63;&#x6f;&#x6d;</a></p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>Machine Learning Engineer</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker-alt"></i>
                <br/>
                Singapore
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->

<script src="/assets/js/script-sqrh47zm5nkjgifq4rx38uvns4r2rarrrvwuhjxiztyrddruca5ukl7nw6br.min.js"></script>

<!--SCRIPTS END-->


    




    </body>
</html>
