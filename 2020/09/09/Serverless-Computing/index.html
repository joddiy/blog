
<!DOCTYPE html>
<html lang="en">
    
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Jz Blog">
    <title>Serverless Computing - Jz Blog</title>
    <meta name="author" content="Joddiy Zhang">
    
    
        <link rel="icon" href="http://joddiy.cc/assets/images/favicon.ico">
    
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Joddiy Zhang","sameAs":["https://github.com/joddiy","https://scholar.google.com/citations?user=KH-xv38AAAAJ&hl=en&oi=sra","https://www.linkedin.com/in/joddiyzhang/"],"image":"14108933.jpeg"},"articleBody":"Serverless Computing: One Step Forward, Two Steps Back\n\n\n\nThree Design Patterns\nEmbarrassingly parallel functionsIn some applications, each function invocation is an independent task and never needs to communicate with other functions. such “map” functions, which can directly exploit Lambda’s auto-scaling features to scale.\n\nOrchestration functionsA second class of use cases leverages serverless functions simply to orchestrate calls to proprietary autoscaling services, such as large-scale analytics. For example, using Lambda functions to preprocess eventstreams before funneling them to Athena via S3.\n\nFunction CompositionThe third category consists of collections of functions that are composed to build applications and thus need to pass along outputs and inputs.\n\n\nServerless Is Too Less\n\n\n\n\nLimited LifetimesAfter 15 minutes, function invocations are shut down by the Lambda infrastructure.There is no way to ensure that subsequent invocations are run on the same VM.\n\n\n\n\nI&#x2F;O Bottlenecks Recent studies show that a single Lambda function can achieve on average 538Mbps network bandwidth. Worse, AWS appears to attempt to pack Lambda functions from the same user together on a single VM, so the limited bandwidth is shared by multiple functions.\n\nCommunication Through Slow Storage Two Lambda functions can only communicate through an autoscaling intermediary service. Hence maintaining state across client calls requires writing the state out to slow storage, and reading it back on every subsequent call.\n\n\nA number of discussion follow directly.\nFaaS is a Data-Shipping Architecture.\nFaaS hinders Distributed Computing(slow and expensive storage).\nCase Studies: Model Training(1). Lambda (465 minutes):\nEach Lambda is allocated the maximum lifetime (15 min) and 640MB RAM and runs as many training iterations as possible.\nEach iteration in Lambda took 3.08 seconds: 2.49 to fetch a 100MB batch from S3 and 0.59 seconds to run the AdamOptimizer.\nWe trained the model over 10 full passes of the training data, which translates to 31 sequential lambda executions, each of which runs for 15 minutes, or 465 minutes total latency. This costs $0.29.\n(2). Tensorflow (22 minutes):\nFor comparison, we trained the same model on an m4.large EC2 instance, which has 8GB of RAM and 2vCPUs. In this setting, each iteration is significantly faster (0.14 seconds): 0.04 seconds to fetch data from an EBS volume and 0.1 seconds to run the optimizer.\nThe same training process takes about 1300 seconds (just under 22 minutes), which translates to a cost of $0.04.\nLambda’s limited resources and data-shipping architecture mean that running this algorithm on Lambda is 21× slower and 7.3× more expensive than running on EC2.\nCase Studies: Prediction ServingOnly on CPU, and we limited all experiments here to 10-message batches.\nIf the model was retrieved on every invocation, the average latency over 1,000 batch invocations for the Lambda application was 559ms per batch with S3 and was 447ms with SQS queue.\nAn EC2 machine (not function) to receive SQS message batches—this showed a latency of 13ms per batch averaged over 1,000 batches—27× faster than our “optimized” Lambda implementation. \nAn EC2 machine (not function) with ZeroMQ had a per batch latency of 2.8ms—127× faster than the optimized Lambda implementation.\nSQS request rate alone would cost $1,584 per hour. \nCase Studies: Distributed Computingtry alternative solutions to achieve distributed computation.\n\n\nwe measure the cost of 1KB argument for a Lambda function invocation — this incurs both I&#x2F;O and function overheads (303ms). \nThe cost of explicit I&#x2F;O from Lambda to S3 is 108ms and to DynamoDB is 11ms.\nThe cost of explicit I&#x2F;O from EC2 to S3 is 106ms and to DynamoDB is 11ms.\nThe cost of explicit I&#x2F;O from EC2 to 0MQ is 290us.\n\n\n\n\n\nCirrus: a Serverless Framework for End-to-end MLWorkflowsThis work proposes Cirrus, a distributed ML training framework that addresses these challenges by leveraging serverless computing.\nEnd-to-end MLWorkflow ChallengesOver-provisioning\n\nThe heterogeneity of the different tasks in an MLworkflowleads to a significant resource imbalance during the execution of a training workflow.\n\nExplicit resource management\n\nThe established approach of exposing low-level VM resources, such as storage and CPUs, puts a significant burden on ML developers who are faced with the challenge of provisioning, configuring, and managing these resources for each of their ML workloads.\n\nServerless Computingmajor limitations of existing serverless environments:\nSmall local memory and storage\n\nAWS lambdas can only access at most 3GB of local RAM and 512MB of local disk\n\nLow bandwidth and lack of P2P communication\n\nWe find that the largest AWS Lambda can only sustain 60MB&#x2F;s of bandwidth. AWS Lambdas do not allow peer-to-peer communication. Thus, common communication strategies used for datacenter ML, such as tree-structured or ring-structured AllReduce communication [43], become impossible to implement efficiently in such environments.\n\nShort-lived and unpredictable launch times\n\nAWS lambdas can take up to several minutes to start after being launched. This means that during training, lambdas start at unpredictable times and can finish in the middle of training. This requires ML runtimes for lambdas to tolerate the frequent departure and arrival of workers.\n\nLack of fast shared storage\n\nshared storage needs to be low-latency, high-throughput, and optimized for the type of communications in ML workloads.\n\nCIRRUS DESIGN\n\n\n\n(1) Worker runtime\n\n\nThe worker runtime provides two APIs. \nData iterator API: prefetches and buffers minibatches in the lambda’s local memory in parallel with the worker’s computations to mitigate the high latency (&gt;10ms) of accessing S3.\nData store client API: data compression, sparse transfers of data, asynchronous communication and sharding across multiple nodes.\n(2) Distributed data store\nIntermediate stored data(in cloud VMs) to be shared by all workers.\nIt achieves latencies as low as 300μs versus ≈ 10ms for AWS S3.\n\n\nCirrus Implementation(1) Client backend\nLambdas that are launched during training are relaunched automatically when their lifetime terminates (every 15 minutes).\nThe backend keeps a pool of threads that can be used for responding to requests for new lambda tasks.\n(2) Distributed data store\nCirrus’s distributed data store provides an interface supporting a key-value store interface (set&#x2F;get) and a parameter-server interface (send gradient &#x2F; get model).\nseveral optimizations:\n\nmultithreaded server that distributes work across many cores\ndata compression for the gradientand models\nsparse gradient and model data structures\n\n(3) Worker runtime\n\n\nFor data access, the runtime provides a minibatch-based iterator backed by a local memory ring-buffer that allows workers to access training minibatches with low latency. \nIn addition, it provides an efficient API to communicate with the distributed data store.\nEVALUATION\n\nTensorflow was executed on a 32-core node (performed better than on 1 Titan V GPU) and Cirrus ran in 10 lambdas.\nProblem\nevaluation not fair.\nDistributed data store how to attend 300μs not clear.\nonly focus on throughput not on latency(only example for training).\n\nNarrowing the Gap Between Serverless and its State with Storage Functions\n\nShredder Design\n\nInternally, Shredder consists of three layers: a networking layer, a storage layer, and a function layer. \nEach CPU core runs all three layers, but CPU cores followa shared-nothing design; the state of these layers is partitioned across CPU cores to avoid contention and synchronization overheads.\nThe storage layer hosts all tenants’ data in memory and has a get()&#x2F;put() key-value interface.\nThe function layer matches incoming requests to their storage function code and context, and it executes the operation within a per-core instance of the V8 runtime. \nEach V8 runtime has a set of embedded trusted access methods to avoid expensive calls between the function runtime and the storage layer.\nIsolation and Context Management\n\nShredder relies on V8’s Contexts to isolate tenantprovided code.\nZero-copy Data Access\n\nLocal get() and put() operations are optimized to avoid copying records into and out of storage whenever possible.\nEliminating Boundary Crossings with CSACSA is portable across hardware architectures, and it can be translated into highly efficient machine code.\nKey idea\nSeparating storage from functions.\nV8 runtime per-core to ensure isolation.\nZero-copy Data Access\nTrusted CSA code within the V8 runtime is given read-only access to the data storage.\n\nEvaluation\n\nCost of Isolation; CPU Scalability; Tenant Scalability;\nProblem\nthe Zero-copy Data Access only supports local memory.\nfor remote get, it only discuss the throughput not the lantency.\nV8 JavaScript runtime is heavy.\nbind compute with memory limits the scalability.\n\nCentralized Core-granular Scheduling for Serverless FunctionsExisting serverless platforms lack deployment and scheduling mechanisms.\nScheduling granularity\n\nserver-level scheduling: \nUnpredictable performance due to sharing of physical and virtual resources among function invocations.\ndistributed task-scheduling:\nScalability challenge stems from latency of scheduling and task migration.\ncentralized and core-granularity scheduling:\ncore-granularity improves performance predictability and the centralized design maintains a global view of cluster resources.\nA centralized core-granular scheduler\n\nThe centralized scheduler runs on a multicore server. We distinguish between cores on the scheduler server, called scheduler cores, and cores on worker servers, called worker cores. \nEach scheduler core maintains a list of references to idle worker cores.\ni -&gt; iv, when comes a request, it chooses a worker core to execute. After finished, return it.v -&gt; vii, if no available worker cores, borrow from others.\nProblem\nno evaluation\nnot specify to ML\n\nNote:problem:Platform: \n\nvirtualization technologies\ncontext switch\ndistributed approach(scalability)\nstate locally\n\nUser:\n\n640M RAM and 15min limit\nstate from the slow external store\nevent granularity(sub-graph)\n\nCommon:\n\nwarmed up run-time\nwork assignment(event-driven trigger)\n\n\n\n\n\n\n\nBurst:1, 预测机制，提前copy weights到function附近位置（存储资源比计算资源便宜）2, 拆分机制，可以把graph拆分成多个stage（function不宜过大，利用stage加载时间去cover中间结果传输时间）4, 基于ONNX来拆分和执行stage\n\n\n将graph分成stage:\n\n如果传输中间结果的时间，小于，加载stage的时间，可以认为，传输时间不产生费用（和latency）\n选取数据量小的中间结果进行传输\n拆分的stage，满足现有的lambda的内存要求\n模型未开始阶段，先预测需要多少的function，提前把各个stage的weight传输过去\n\nstateful:\n\n预加载到内存\n确保传输的中间结果比较小\n加入，模型压缩，模型蒸馏？\n\ntrade-off between lantency, throughput and cost per request.\nImportant point: \n\ntail-lantency\n\ncpu 是否 batch 越大越好\n预测所需资源（cpu, memory, therefore time）\n提出一种度量，来描述burst\n全serverless, 包含 frontend\n先考虑技术（拆分model+调度执行），再考虑应用场景（1. request burst\n分两步自动化：\n\n用state machine统一管理好，已经创建好的，functions\n自动化创建functions\n预测workload，并自动拆分onnx model\n\n","dateCreated":"2020-09-09T04:13:11+08:00","dateModified":"2020-11-25T14:57:55+08:00","datePublished":"2020-09-09T04:13:11+08:00","description":"Paper summary for Serverless Computing","headline":"Serverless Computing","image":["https://1.bp.blogspot.com/-cLxpo2BA8oI/XZMQrDSXefI/AAAAAAAAnhs/7FS6r94KFNQFzVt_8Ihx1iyTltt7if_xgCLcBGAsYHQ/s1600/TensorFlow%2B2.0%2BLogo.png","https://www.eetasia.com/wp-content/uploads/sites/2/images/c806bc6b-a80c-4ed7-808e-3cc362811e28.jpg"],"mainEntityOfPage":{"@type":"WebPage","@id":"http://joddiy.cc/2020/09/09/Serverless-Computing/"},"publisher":{"@type":"Organization","name":"Joddiy Zhang","sameAs":["https://github.com/joddiy","https://scholar.google.com/citations?user=KH-xv38AAAAJ&hl=en&oi=sra","https://www.linkedin.com/in/joddiyzhang/"],"image":"14108933.jpeg","logo":{"@type":"ImageObject","url":"14108933.jpeg"}},"url":"http://joddiy.cc/2020/09/09/Serverless-Computing/","keywords":"Tools, Deep Learning","thumbnailUrl":"https://1.bp.blogspot.com/-cLxpo2BA8oI/XZMQrDSXefI/AAAAAAAAnhs/7FS6r94KFNQFzVt_8Ihx1iyTltt7if_xgCLcBGAsYHQ/s1600/TensorFlow%2B2.0%2BLogo.png"}</script>
    <meta name="description" content="Paper summary for Serverless Computing">
<meta property="og:type" content="blog">
<meta property="og:title" content="Serverless Computing">
<meta property="og:url" content="http://joddiy.cc/2020/09/09/Serverless-Computing/index.html">
<meta property="og:site_name" content="Jz Blog">
<meta property="og:description" content="Paper summary for Serverless Computing">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://joddiy.cc/2020/09/09/Serverless-Computing/1.png">
<meta property="og:image" content="http://joddiy.cc/2020/09/09/Serverless-Computing/2.png">
<meta property="og:image" content="http://joddiy.cc/2020/09/09/Serverless-Computing/3.png">
<meta property="og:image" content="http://joddiy.cc/2020/09/09/Serverless-Computing/4.png">
<meta property="og:image" content="http://joddiy.cc/2020/09/09/Serverless-Computing/5.png">
<meta property="og:image" content="http://joddiy.cc/2020/09/09/Serverless-Computing/6.png">
<meta property="og:image" content="http://joddiy.cc/2020/09/09/Serverless-Computing/7.png">
<meta property="og:image" content="http://joddiy.cc/2020/09/09/Serverless-Computing/8.png">
<meta property="og:image" content="http://joddiy.cc/2020/09/09/Serverless-Computing/11.png">
<meta property="og:image" content="http://joddiy.cc/2020/09/09/Serverless-Computing/9.png">
<meta property="og:image" content="http://joddiy.cc/2020/09/09/Serverless-Computing/10.png">
<meta property="og:image" content="http://joddiy.cc/2020/09/09/Serverless-Computing/13.png">
<meta property="article:published_time" content="2020-09-08T20:13:11.000Z">
<meta property="article:modified_time" content="2020-11-25T06:57:55.000Z">
<meta property="article:author" content="Joddiy Zhang">
<meta property="article:tag" content="Tools">
<meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://joddiy.cc/2020/09/09/Serverless-Computing/1.png">
    
    
        
    
    
        <meta property="og:image" content="http://joddiy.cc/assets/images/14108933.jpeg"/>
    
    
        <meta property="og:image" content="https://1.bp.blogspot.com/-cLxpo2BA8oI/XZMQrDSXefI/AAAAAAAAnhs/7FS6r94KFNQFzVt_8Ihx1iyTltt7if_xgCLcBGAsYHQ/s1600/TensorFlow%2B2.0%2BLogo.png"/>
        <meta class="swiftype" name="image" data-type="enum" content="https://1.bp.blogspot.com/-cLxpo2BA8oI/XZMQrDSXefI/AAAAAAAAnhs/7FS6r94KFNQFzVt_8Ihx1iyTltt7if_xgCLcBGAsYHQ/s1600/TensorFlow%2B2.0%2BLogo.png"/>
    
    
        <meta property="og:image" content="https://www.eetasia.com/wp-content/uploads/sites/2/images/c806bc6b-a80c-4ed7-808e-3cc362811e28.jpg"/>
        <meta class="swiftype" name="image" data-type="enum" content="https://www.eetasia.com/wp-content/uploads/sites/2/images/c806bc6b-a80c-4ed7-808e-3cc362811e28.jpg"/>
    
    
    <!--STYLES-->
    
<link rel="stylesheet" href="/assets/css/style-x8blglznjjnb9pnnwui5zw4h43ysufmsh1b0omicawm4vhqcutzqavokgpne.min.css">

    <!--STYLES END-->
    

    

    
        
    
</head>

    <body>
        <div id="blog">
            <!-- Define author's picture -->


    
        
            
        
    

<header id="header" data-behavior="4">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a
            class="header-title-link"
            href="/"
            aria-label=""
        >
            Jz Blog
        </a>
    </div>
    
        
            <a
                class="header-right-picture "
                href="#about"
                aria-label="Open the link: /#about"
            >
        
        
            <img class="header-picture" src="/assets/images/14108933.jpeg" alt="Author&#39;s picture"/>
        
        </a>
    
</header>

            <!-- Define author's picture -->



        
    

<nav id="sidebar" data-behavior="4">
    <div class="sidebar-container">
        
            <div class="sidebar-profile">
                <a
                    href="/#about"
                    aria-label="Read more about the author"
                >
                    <img class="sidebar-profile-picture" src="/assets/images/14108933.jpeg" alt="Author&#39;s picture"/>
                </a>
                <h4 class="sidebar-profile-name">Joddiy Zhang</h4>
                
                    <h5 class="sidebar-profile-bio"><p><a href="mailto:&#x6a;&#x6f;&#x64;&#100;&#105;&#x79;&#x7a;&#104;&#x61;&#x6e;&#103;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#x63;&#x6f;&#x6d;">&#x6a;&#x6f;&#x64;&#100;&#105;&#x79;&#x7a;&#104;&#x61;&#x6e;&#103;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#x63;&#x6f;&#x6d;</a></p>
</h5>
                
            </div>
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/"
                            
                            rel="noopener"
                            title="Home"
                        >
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-categories"
                            
                            rel="noopener"
                            title="Categories"
                        >
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-tags"
                            
                            rel="noopener"
                            title="Tags"
                        >
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Tags</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-archives"
                            
                            rel="noopener"
                            title="Archives"
                        >
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="#about"
                            
                            rel="noopener"
                            title="About"
                        >
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://github.com/joddiy"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="GitHub"
                        >
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://scholar.google.com/citations?user=KH-xv38AAAAJ&hl=en&oi=sra"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="Google Scholar"
                        >
                        <i class="sidebar-button-icon fab fa-google" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Google Scholar</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://www.linkedin.com/in/joddiyzhang/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="LinkedIn"
                        >
                        <i class="sidebar-button-icon fab fa-linkedin" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">LinkedIn</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
        <div class="post-header-cover
                    text-center
                    post-header-cover--partial"
             style="background-image:url('https://www.eetasia.com/wp-content/uploads/sites/2/images/c806bc6b-a80c-4ed7-808e-3cc362811e28.jpg');"
             data-behavior="4">
            
                <div class="post-header main-content-wrap text-center">
    
        <h1 class="post-title">
            Serverless Computing
        </h1>
    
    
        <div class="post-meta">
    <time datetime="2020-09-09T04:13:11+08:00">
	
		    Sep 09, 2020
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Study-Notes/">Study Notes</a>


    
</div>

    
</div>

            
        </div>

            <div id="main" data-behavior="4"
                 class="hasCover
                        hasCoverMetaIn
                        ">
                
<article class="post">
    
    
    <div class="post-content markdown">
        <div class="main-content-wrap">
            <h1 id="Serverless-Computing-One-Step-Forward-Two-Steps-Back"><a href="#Serverless-Computing-One-Step-Forward-Two-Steps-Back" class="headerlink" title="Serverless Computing: One Step Forward, Two Steps Back"></a>Serverless Computing: One Step Forward, Two Steps Back</h1><!-- A FaaS offering by itself is of little value, since each function execution is isolated and ephemeral. Building applications on FaaS requires data management in both persistent and temporary storage.

As a result, cloud providers are quick to emphasize that serverless is not only FaaS. It is FaaS supported by a **“standard library”: the various multitenanted, autoscaling services provided by the vendor.** -->

<!-- ### Forward

It is autoscaling. The workload automatically drives the allocation and deallocation of resources.

## Two Major Backward.

First, they painfully ignore the importance of efficient data processing. 

Second, they stymie the development of distributed systems. -->

<h2 id="Three-Design-Patterns"><a href="#Three-Design-Patterns" class="headerlink" title="Three Design Patterns"></a>Three Design Patterns</h2><ol>
<li><p>Embarrassingly parallel functions<br>In some applications, each function invocation is an independent task and never needs to communicate with other functions. such “map” functions, which can directly exploit Lambda’s auto-scaling features to scale.</p>
</li>
<li><p>Orchestration functions<br>A second class of use cases leverages serverless functions simply to orchestrate calls to proprietary autoscaling services, such as large-scale analytics. For example, using Lambda functions to preprocess event<br>streams before funneling them to Athena via S3.</p>
</li>
<li><p>Function Composition<br>The third category consists of collections of functions that are composed to build applications and thus need to pass along outputs and inputs.</p>
</li>
</ol>
<h2 id="Serverless-Is-Too-Less"><a href="#Serverless-Is-Too-Less" class="headerlink" title="Serverless Is Too Less"></a>Serverless Is Too Less</h2><!-- Serverless computing today is at best a simple and powerful way to run embarrassingly parallel computations or harness proprietary services. -->

<!-- **The limitations in today’s FaaS offerings:** -->

<ol>
<li>Limited Lifetimes<br>After 15 minutes, function invocations are shut down by the Lambda infrastructure.<br>There is no way to ensure that subsequent invocations are run on the same VM.</li>
</ol>
<!-- Hence functions must be written assuming that state will not be recoverable across invocations. -->

<ol start="2">
<li><p>I&#x2F;O Bottlenecks<br> Recent studies show that a single Lambda function can achieve on average 538Mbps network bandwidth.<br> Worse, AWS appears to attempt to pack Lambda functions from the same user together on a single VM, so the limited bandwidth is shared by multiple functions.</p>
</li>
<li><p>Communication Through Slow Storage<br> Two Lambda functions can only communicate through an autoscaling intermediary service.<br> Hence maintaining state across client calls requires writing the state out to slow storage, and reading it back on every subsequent call.</p>
</li>
</ol>
<p><strong>A number of discussion follow directly.</strong></p>
<p>FaaS is a Data-Shipping Architecture.</p>
<p>FaaS hinders Distributed Computing(slow and expensive storage).</p>
<h2 id="Case-Studies-Model-Training"><a href="#Case-Studies-Model-Training" class="headerlink" title="Case Studies: Model Training"></a>Case Studies: Model Training</h2><p>(1). Lambda (465 minutes):</p>
<p>Each Lambda is allocated the maximum lifetime (15 min) and 640MB RAM and runs as many training iterations as possible.</p>
<p>Each iteration in Lambda took <strong>3.08 seconds</strong>: 2.49 to fetch a 100MB batch from S3 and 0.59 seconds to run the AdamOptimizer.</p>
<p>We trained the model over 10 full passes of the training data, which translates to <strong>31 sequential lambda executions</strong>, each of which runs for 15 minutes, or 465 minutes total latency. This costs $0.29.</p>
<p>(2). Tensorflow (22 minutes):</p>
<p>For comparison, we trained the same model on an m4.large EC2 instance, which has 8GB of RAM and 2vCPUs. In this setting, each iteration is significantly faster <strong>(0.14 seconds)</strong>: 0.04 seconds to fetch data from an EBS volume and 0.1 seconds to run the optimizer.</p>
<p>The same training process takes about 1300 seconds (just under 22 minutes), which translates to a cost of $0.04.</p>
<p>Lambda’s limited resources and data-shipping architecture mean that running this algorithm on Lambda is 21× slower and 7.3× more expensive than running on EC2.</p>
<h2 id="Case-Studies-Prediction-Serving"><a href="#Case-Studies-Prediction-Serving" class="headerlink" title="Case Studies: Prediction Serving"></a>Case Studies: Prediction Serving</h2><p>Only on CPU, and we limited all experiments here to 10-message batches.</p>
<p>If the model was retrieved on every invocation, the average latency over 1,000 batch invocations for the Lambda application was <strong>559ms</strong> per batch with S3 and was <strong>447ms</strong> with SQS queue.</p>
<p>An EC2 machine (not function) to receive SQS message batches—this showed a latency of <strong>13ms</strong> per batch averaged over 1,000 batches—27× faster than our “optimized” Lambda implementation. </p>
<p>An EC2 machine (not function) with ZeroMQ had a per batch latency of <strong>2.8ms</strong>—127× faster than the optimized Lambda implementation.</p>
<p>SQS request rate alone would cost $1,584 per hour. </p>
<h2 id="Case-Studies-Distributed-Computing"><a href="#Case-Studies-Distributed-Computing" class="headerlink" title="Case Studies: Distributed Computing"></a>Case Studies: Distributed Computing</h2><p>try alternative solutions to achieve distributed computation.</p>
<img src="1.png" style="width:800px;"/>

<p>we measure the cost of 1KB argument for a Lambda function invocation — this incurs both I&#x2F;O and function overheads (303ms). </p>
<p>The cost of explicit I&#x2F;O from Lambda to S3 is 108ms and to DynamoDB is 11ms.</p>
<p>The cost of explicit I&#x2F;O from EC2 to S3 is 106ms and to DynamoDB is 11ms.</p>
<p>The cost of explicit I&#x2F;O from EC2 to 0MQ is 290us.</p>
<!-- We see that latencies from EC2 are almost identical, so the overhead is in the storage service costs, not in Lambda.

Finally we show the latency of (acked) messaging using Python functions directly addressing each other via ZeroMQ. This last cost is close to typical intra-rack datacenter network measurements; studies from even a few years ago report average inter-rack measurements around 1.26ms

In sum, communicating via cloud storage is not a reasonable replacement for directly-addressed networking, even with direct I/O.

DynamoDB as a fine-grained communication medium is incredibly expensive: Supporting a cluster of 1,000 nodes costs at minimum $450 per hour.
 -->




<h1 id="Cirrus-a-Serverless-Framework-for-End-to-end-MLWorkflows"><a href="#Cirrus-a-Serverless-Framework-for-End-to-end-MLWorkflows" class="headerlink" title="Cirrus: a Serverless Framework for End-to-end MLWorkflows"></a>Cirrus: a Serverless Framework for End-to-end MLWorkflows</h1><p>This work proposes Cirrus, a distributed ML training framework that addresses these challenges by leveraging serverless computing.</p>
<h2 id="End-to-end-MLWorkflow-Challenges"><a href="#End-to-end-MLWorkflow-Challenges" class="headerlink" title="End-to-end MLWorkflow Challenges"></a>End-to-end MLWorkflow Challenges</h2><p><strong>Over-provisioning</strong></p>
<blockquote>
<p>The heterogeneity of the different tasks in an MLworkflowleads to a significant resource imbalance during the execution of a training workflow.</p>
</blockquote>
<p><strong>Explicit resource management</strong></p>
<blockquote>
<p>The established approach of exposing low-level VM resources, such as storage and CPUs, puts a significant burden on ML developers who are faced with the challenge of provisioning, configuring, and managing these resources for each of their ML workloads.</p>
</blockquote>
<h2 id="Serverless-Computing"><a href="#Serverless-Computing" class="headerlink" title="Serverless Computing"></a>Serverless Computing</h2><p>major limitations of existing serverless environments:</p>
<p><strong>Small local memory and storage</strong></p>
<blockquote>
<p>AWS lambdas can only access at most 3GB of local RAM and 512MB of local disk</p>
</blockquote>
<p><strong>Low bandwidth and lack of P2P communication</strong></p>
<blockquote>
<p>We find that the largest AWS Lambda can only sustain 60MB&#x2F;s of bandwidth. AWS Lambdas do not allow peer-to-peer communication. Thus, common communication strategies used for datacenter ML, such as tree-structured or ring-structured AllReduce communication [43], become impossible to implement efficiently in such environments.</p>
</blockquote>
<p><strong>Short-lived and unpredictable launch times</strong></p>
<blockquote>
<p>AWS lambdas can take up to several minutes to start after being launched. This means that during training, lambdas start at unpredictable times and can finish in the middle of training. This requires ML runtimes for lambdas to tolerate the frequent departure and arrival of workers.</p>
</blockquote>
<p><strong>Lack of fast shared storage</strong></p>
<blockquote>
<p>shared storage needs to be low-latency, high-throughput, and optimized for the type of communications in ML workloads.</p>
</blockquote>
<h2 id="CIRRUS-DESIGN"><a href="#CIRRUS-DESIGN" class="headerlink" title="CIRRUS DESIGN"></a>CIRRUS DESIGN</h2><img src="2.png" style="width:400px;"/>

<!-- Design Principles:
 - Adaptive, fine-grained resource allocation.
 - Stateless server-side backend.
 - End-to-end serverless API.
 - High scalability. -->

<p>(1) Worker runtime</p>
<!-- The system runtime meets two goals: 1) lightweight, and 2) high-performance. -->

<p>The worker runtime provides two APIs. </p>
<p>Data iterator API: prefetches and buffers minibatches in the lambda’s local memory in parallel with the worker’s computations to mitigate the high latency (&gt;10ms) of accessing S3.</p>
<p>Data store client API: data compression, sparse transfers of data, asynchronous communication and sharding across multiple nodes.</p>
<p>(2) Distributed data store</p>
<p>Intermediate stored data(in cloud VMs) to be shared by all workers.</p>
<p>It achieves latencies as low as 300μs versus ≈ 10ms for AWS S3.</p>
<!-- ## API Example

<img src="3.png" style="width:800px;"/>

The first step in the workflow with Cirrus is to load the dataset and upload it to S3. 

Cirrus parses the data, automatically creates partitions for it and then uploads it to S3. The front-end partitions datasets in blocks of roughly 10MB. 

We chose this size because data partitions in Cirrus are the granularity of data workers transfer from S3. We have found this size allows lambda workers to achieve good network transfer bandwidth. In addition, this keeps the size of each worker’s minibatch cache small. -->

<h2 id="Cirrus-Implementation"><a href="#Cirrus-Implementation" class="headerlink" title="Cirrus Implementation"></a>Cirrus Implementation</h2><p>(1) Client backend</p>
<p>Lambdas that are launched during training are relaunched automatically when their lifetime terminates (every 15 minutes).</p>
<p>The backend keeps a pool of threads that can be used for responding to requests for new lambda tasks.</p>
<p>(2) Distributed data store</p>
<p>Cirrus’s distributed data store provides an interface supporting a key-value store interface (set&#x2F;get) and a parameter-server interface (send gradient &#x2F; get model).</p>
<p>several optimizations:</p>
<ul>
<li>multithreaded server that distributes work across many cores</li>
<li>data compression for the gradientand models</li>
<li>sparse gradient and model data structures</li>
</ul>
<p>(3) Worker runtime</p>
<img src="4.png" style="width:400px;"/>

<p>For data access, the runtime provides a minibatch-based iterator backed by a local memory ring-buffer that allows workers to access training minibatches with low latency. </p>
<p>In addition, it provides an efficient API to communicate with the distributed data store.</p>
<h2 id="EVALUATION"><a href="#EVALUATION" class="headerlink" title="EVALUATION"></a>EVALUATION</h2><img src="5.png" style="width:800px;"/>

<p>Tensorflow was executed on a 32-core node (performed better than on 1 Titan V GPU) and Cirrus ran in 10 lambdas.</p>
<h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h2><ol>
<li>evaluation not fair.</li>
<li>Distributed data store how to attend 300μs not clear.</li>
<li>only focus on throughput not on latency(only example for training).</li>
</ol>
<h1 id="Narrowing-the-Gap-Between-Serverless-and-its-State-with-Storage-Functions"><a href="#Narrowing-the-Gap-Between-Serverless-and-its-State-with-Storage-Functions" class="headerlink" title="Narrowing the Gap Between Serverless and its State with Storage Functions"></a>Narrowing the Gap Between Serverless and its State with Storage Functions</h1><!-- ## Contributions
We identify the gap between serverless computing and state storage; we propose storage functions to address this gap.

We propose a novel technique that relies on an V8 intermediate representation (CSA) to avoid runtime boundary crossing cost.

We show how embedding compute within storage. -->

<h2 id="Shredder-Design"><a href="#Shredder-Design" class="headerlink" title="Shredder Design"></a>Shredder Design</h2><img src="6.png" style="width:400px;"/>

<p>Internally, Shredder consists of three layers: a networking layer, a storage layer, and a function layer. </p>
<p>Each CPU core runs all three layers, but CPU cores followa shared-nothing design; the state of these layers is partitioned across CPU cores to avoid contention and synchronization overheads.</p>
<p>The <strong>storage layer</strong> hosts all tenants’ data in memory and has a get()&#x2F;put() key-value interface.</p>
<p>The <strong>function layer</strong> matches incoming requests to their storage function code and context, and it executes the operation within a per-core instance of the V8 runtime. </p>
<p>Each <strong>V8 runtime</strong> has a set of embedded trusted access methods to avoid expensive calls between the function runtime and the storage layer.</p>
<h2 id="Isolation-and-Context-Management"><a href="#Isolation-and-Context-Management" class="headerlink" title="Isolation and Context Management"></a>Isolation and Context Management</h2><img src="7.png" style="width:400px;"/>

<p>Shredder relies on V8’s Contexts to isolate tenantprovided code.</p>
<h2 id="Zero-copy-Data-Access"><a href="#Zero-copy-Data-Access" class="headerlink" title="Zero-copy Data Access"></a>Zero-copy Data Access</h2><img src="8.png" style="width:400px;"/>

<p>Local get() and put() operations are optimized to avoid copying records into and out of storage whenever possible.</p>
<h2 id="Eliminating-Boundary-Crossings-with-CSA"><a href="#Eliminating-Boundary-Crossings-with-CSA" class="headerlink" title="Eliminating Boundary Crossings with CSA"></a>Eliminating Boundary Crossings with CSA</h2><p>CSA is portable across hardware architectures, and it can be translated into highly efficient machine code.</p>
<h2 id="Key-idea"><a href="#Key-idea" class="headerlink" title="Key idea"></a>Key idea</h2><ol>
<li>Separating storage from functions.</li>
<li>V8 runtime per-core to ensure isolation.</li>
<li>Zero-copy Data Access</li>
<li>Trusted CSA code within the V8 runtime is given read-only access to the data storage.</li>
</ol>
<h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><img src="11.png" style="width:800px;"/>

<p>Cost of Isolation; CPU Scalability; Tenant Scalability;</p>
<h2 id="Problem-1"><a href="#Problem-1" class="headerlink" title="Problem"></a>Problem</h2><ol>
<li>the Zero-copy Data Access only supports local memory.</li>
<li>for remote get, it only discuss the throughput not the lantency.</li>
<li>V8 JavaScript runtime is heavy.</li>
<li>bind compute with memory limits the scalability.</li>
</ol>
<h1 id="Centralized-Core-granular-Scheduling-for-Serverless-Functions"><a href="#Centralized-Core-granular-Scheduling-for-Serverless-Functions" class="headerlink" title="Centralized Core-granular Scheduling for Serverless Functions"></a>Centralized Core-granular Scheduling for Serverless Functions</h1><p>Existing serverless platforms lack deployment and scheduling mechanisms.</p>
<h2 id="Scheduling-granularity"><a href="#Scheduling-granularity" class="headerlink" title="Scheduling granularity"></a>Scheduling granularity</h2><img src="9.png" style="width:800px;"/>

<p>server-level scheduling: </p>
<p>Unpredictable performance due to sharing of physical and virtual resources among function invocations.</p>
<p>distributed task-scheduling:</p>
<p>Scalability challenge stems from latency of scheduling and task migration.</p>
<p>centralized and core-granularity scheduling:</p>
<p>core-granularity improves performance predictability and the centralized design maintains a global view of cluster resources.</p>
<h2 id="A-centralized-core-granular-scheduler"><a href="#A-centralized-core-granular-scheduler" class="headerlink" title="A centralized core-granular scheduler"></a>A centralized core-granular scheduler</h2><img src="10.png" style="width:400px;"/>

<p>The centralized scheduler runs on a multicore server. We distinguish between cores on the scheduler server, called scheduler cores, and cores on worker servers, called worker cores. </p>
<p>Each scheduler core maintains a list of references to idle worker cores.</p>
<p>i -&gt; iv, when comes a request, it chooses a worker core to execute. After finished, return it.<br>v -&gt; vii, if no available worker cores, borrow from others.</p>
<h2 id="Problem-2"><a href="#Problem-2" class="headerlink" title="Problem"></a>Problem</h2><ol>
<li>no evaluation</li>
<li>not specify to ML</li>
</ol>
<h1 id="Note"><a href="#Note" class="headerlink" title="Note:"></a>Note:</h1><h2 id="problem"><a href="#problem" class="headerlink" title="problem:"></a>problem:</h2><p>Platform: </p>
<ul>
<li>virtualization technologies</li>
<li>context switch</li>
<li>distributed approach(scalability)</li>
<li>state locally</li>
</ul>
<p>User:</p>
<ul>
<li>640M RAM and 15min limit</li>
<li>state from the slow external store</li>
<li>event granularity(sub-graph)</li>
</ul>
<p>Common:</p>
<ul>
<li>warmed up run-time</li>
<li>work assignment(event-driven trigger)</li>
</ul>
<!-- 调度的粒度，整个server，单个core，单个graph -->

<!-- 在现有的平台下提优化方案，还是设计新的平台 -->

<!-- 三种并行方式 -->

<p>Burst:<br>1, 预测机制，提前copy weights到function附近位置（存储资源比计算资源便宜）<br>2, 拆分机制，可以把graph拆分成多个stage（function不宜过大，利用stage加载时间去cover中间结果传输时间）<br>4, 基于ONNX来拆分和执行stage</p>
<img src="13.png" style="width:800px;"/>

<p>将graph分成stage:</p>
<ul>
<li>如果传输中间结果的时间，小于，加载stage的时间，可以认为，传输时间不产生费用（和latency）</li>
<li>选取数据量小的中间结果进行传输</li>
<li>拆分的stage，满足现有的lambda的内存要求</li>
<li>模型未开始阶段，先预测需要多少的function，提前把各个stage的weight传输过去</li>
</ul>
<p>stateful:</p>
<ul>
<li>预加载到内存</li>
<li>确保传输的中间结果比较小</li>
<li>加入，模型压缩，模型蒸馏？</li>
</ul>
<p>trade-off between lantency, throughput and cost per request.</p>
<p>Important point: </p>
<ul>
<li>tail-lantency</li>
</ul>
<p>cpu 是否 batch 越大越好</p>
<p>预测所需资源（cpu, memory, therefore time）</p>
<p>提出一种度量，来描述burst</p>
<p>全serverless, 包含 frontend</p>
<p>先考虑技术（拆分model+调度执行），再考虑应用场景（1. request burst</p>
<p>分两步自动化：</p>
<ol>
<li>用state machine统一管理好，已经创建好的，functions</li>
<li>自动化创建functions</li>
<li>预测workload，并自动拆分onnx model</li>
</ol>

            


        </div>
    </div>
    <div id="post-footer" class="post-footer main-content-wrap">
        
            <div class="post-footer-tags">
                <span class="text-color-light text-small">TAGGED IN</span><br/>
                
    <a class="tag tag--primary tag--small t-none-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a> <a class="tag tag--primary tag--small t-none-link" href="/tags/Tools/" rel="tag">Tools</a>

            </div>
        
        
            <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2020/10/07/Serverless-Paper-Draft/"
                    data-tooltip="Serverless Paper Draft"
                    aria-label="PREVIOUS: Serverless Paper Draft"
                >
                    
                        <i class="fa fa-angle-left" aria-hidden="true"></i>
                        <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                    </a>
            </li>
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2020/09/02/Model-Serving/"
                    data-tooltip="Model Serving"
                    aria-label="NEXT: Model Serving"
                >
                    
                        <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                        <i class="fa fa-angle-right" aria-hidden="true"></i>
                    </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a
                class="post-action-btn btn btn--default btn-open-shareoptions"
                href="#btn-open-shareoptions"
                aria-label="Share this post"
            >
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://www.facebook.com/sharer/sharer.php?u=http://joddiy.cc/2020/09/09/Serverless-Computing/"
                    title="Share on Facebook"
                    aria-label="Share on Facebook"
                >
                    <i class="fab fa-facebook" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://twitter.com/intent/tweet?text=http://joddiy.cc/2020/09/09/Serverless-Computing/"
                    title="Share on Twitter"
                    aria-label="Share on Twitter"
                >
                    <i class="fab fa-twitter" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://plus.google.com/share?url=http://joddiy.cc/2020/09/09/Serverless-Computing/"
                    title="Share on Google+"
                    aria-label="Share on Google+"
                >
                    <i class="fab fa-google-plus" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


        
        
            
        
    </div>
</article>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2024 Joddiy Zhang. All Rights Reserved.
    </span>
</footer>

            </div>
            
                <div id="bottom-bar" class="post-bottom-bar" data-behavior="4">
                    <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2020/10/07/Serverless-Paper-Draft/"
                    data-tooltip="Serverless Paper Draft"
                    aria-label="PREVIOUS: Serverless Paper Draft"
                >
                    
                        <i class="fa fa-angle-left" aria-hidden="true"></i>
                        <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                    </a>
            </li>
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2020/09/02/Model-Serving/"
                    data-tooltip="Model Serving"
                    aria-label="NEXT: Model Serving"
                >
                    
                        <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                        <i class="fa fa-angle-right" aria-hidden="true"></i>
                    </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a
                class="post-action-btn btn btn--default btn-open-shareoptions"
                href="#btn-open-shareoptions"
                aria-label="Share this post"
            >
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://www.facebook.com/sharer/sharer.php?u=http://joddiy.cc/2020/09/09/Serverless-Computing/"
                    title="Share on Facebook"
                    aria-label="Share on Facebook"
                >
                    <i class="fab fa-facebook" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://twitter.com/intent/tweet?text=http://joddiy.cc/2020/09/09/Serverless-Computing/"
                    title="Share on Twitter"
                    aria-label="Share on Twitter"
                >
                    <i class="fab fa-twitter" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://plus.google.com/share?url=http://joddiy.cc/2020/09/09/Serverless-Computing/"
                    title="Share on Google+"
                    aria-label="Share on Google+"
                >
                    <i class="fab fa-google-plus" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


                </div>
                
    <div id="share-options-bar" class="share-options-bar" data-behavior="4">
        <i id="btn-close-shareoptions" class="fa fa-times"></i>
        <ul class="share-options">
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://www.facebook.com/sharer/sharer.php?u=http://joddiy.cc/2020/09/09/Serverless-Computing/"
                        aria-label="Share on Facebook"
                    >
                        <i class="fab fa-facebook" aria-hidden="true"></i><span>Share on Facebook</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://twitter.com/intent/tweet?text=http://joddiy.cc/2020/09/09/Serverless-Computing/"
                        aria-label="Share on Twitter"
                    >
                        <i class="fab fa-twitter" aria-hidden="true"></i><span>Share on Twitter</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://plus.google.com/share?url=http://joddiy.cc/2020/09/09/Serverless-Computing/"
                        aria-label="Share on Google+"
                    >
                        <i class="fab fa-google-plus" aria-hidden="true"></i><span>Share on Google+</span>
                    </a>
                </li>
            
        </ul>
    </div>


            
        </div>
        


    
        
    

<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-times"></i>
        </div>
        
            <img id="about-card-picture" src="/assets/images/14108933.jpeg" alt="Author&#39;s picture"/>
        
            <h4 id="about-card-name">Joddiy Zhang</h4>
        
            <div id="about-card-bio"><p><a href="mailto:&#x6a;&#111;&#100;&#100;&#x69;&#x79;&#122;&#x68;&#x61;&#110;&#x67;&#64;&#103;&#x6d;&#x61;&#105;&#x6c;&#x2e;&#99;&#x6f;&#x6d;">&#x6a;&#111;&#100;&#100;&#x69;&#x79;&#122;&#x68;&#x61;&#110;&#x67;&#64;&#103;&#x6d;&#x61;&#105;&#x6c;&#x2e;&#99;&#x6f;&#x6d;</a></p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>Machine Learning Engineer</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker-alt"></i>
                <br/>
                Singapore
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->

<script src="/assets/js/script-sqrh47zm5nkjgifq4rx38uvns4r2rarrrvwuhjxiztyrddruca5ukl7nw6br.min.js"></script>

<!--SCRIPTS END-->


    




    </body>
</html>
