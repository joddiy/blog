
<!DOCTYPE html>
<html lang="en">
    
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Jz Blog">
    <title>Distributed training with TensorFlow - Jz Blog</title>
    <meta name="author" content="Joddiy Zhang">
    
    
        <link rel="icon" href="https://joddiy.github.io/assets/images/favicon.ico">
    
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Joddiy Zhang","sameAs":["https://github.com/joddiy","https://scholar.google.com/citations?user=KH-xv38AAAAJ&hl=en&oi=sra","https://www.linkedin.com/in/joddiyzhang/"],"image":"14108933.jpeg"},"articleBody":"Distributed training with TensorFlowdoc link\nOverviewtf.distribute.Strategy is a TensorFlow API to distribute training across multiple GPUs, multiple machines or TPUs. Using this API, you can distribute your existing models and training code with minimal code changes.\n\nTypes of strategiestf.distribute.Strategy intends to cover a number of use cases along different axes. Some of these combinations are currently supported and others will be added in the future. Some of these axes are:\n\nSynchronous vs asynchronous training: These are two common ways of distributing training with data parallelism. In sync training, all workers train over different slices of input data in sync, and aggregating gradients at each step. In async training, all workers are independently training over the input data and updating variables asynchronously. Typically sync training is supported via all-reduce and async through parameter server architecture.\n\nHardware platform: You may want to scale your training onto multiple GPUs on one machine, or multiple machines in a network (with 0 or more GPUs each), or on Cloud TPUs.\n\n\n\n\n\nTraining API\nMirroredStrategy\nTPUStrategy\nMultiWorkerMirroredStrategy\n\n\n\nKeras API\nSupported\nExperimental support\nExperimental support\n\n\nCustom training loop\nExperimental support\nExperimental support\nSupport planned post 2.0\n\n\nEstimator API\nLimited Support\nNot supported\nLimited Support\n\n\n\n\n\nCentralStorageStrategy\nParameterServerStrategy\nOneDeviceStrategy\n\n\n\nExperimental support\nSupported planned post 2.0\nSupported\n\n\nSupport\nplanned post 2.0\nNo support yet\n\n\nLimited Support\nLimited Support\nLimited Support\n\n\nMirroredStrategytf.distribute.MirroredStrategy supports synchronous distributed training on multiple GPUs on one machine. It creates one replica per GPU device. Each variable in the model is mirrored across all the replicas. Together, these variables form a single conceptual variable called MirroredVariable. These variables are kept in sync with each other by applying identical updates.\nEfficient all-reduce algorithms are used to communicate the variable updates across the devices.\nAll-reduce aggregates tensors across all the devices by adding them up, and makes them available on each device. It’s a fused algorithm that is very efficient and can reduce the overhead of synchronization significantly. \nThere are many all-reduce algorithms and implementations available, depending on the type of communication available between devices. By default, it uses NVIDIA NCCL as the all-reduce implementation. You can choose from a few other options we provide, or write your own.\nHere is the simplest way of creating MirroredStrategy:\n1mirrored_strategy = tf.distribute.MirroredStrategy()\n\nThis will create a MirroredStrategy instance which will use all the GPUs that are visible to TensorFlow, and use NCCL as the cross device communication.\nIf you wish to use only some of the GPUs on your machine, you can do so like this:\n1mirrored_strategy = tf.distribute.MirroredStrategy(devices=[&quot;/gpu:0&quot;, &quot;/gpu:1&quot;])\n\nCurrently, tf.distribute.HierarchicalCopyAllReduce and tf.distribute.ReductionToOneDevice are two options other than tf.distribute.NcclAllReduce which is the default.\n12mirrored_strategy = tf.distribute.MirroredStrategy(    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n\nCentralStorageStrategytf.distribute.experimental.CentralStorageStrategy does synchronous training as well. Variables are not mirrored, instead they are placed on the CPU and operations are replicated across all local GPUs. If there is only one GPU, all variables and operations will be placed on that GPU.\nCreate an instance of CentralStorageStrategy by:\n1central_storage_strategy = tf.distribute.experimental.CentralStorageStrategy()\n\nMultiWorkerMirroredStrategytf.distribute.experimental.MultiWorkerMirroredStrategy is very similar to MirroredStrategy. It implements synchronous distributed training across multiple workers, each with potentially multiple GPUs. Similar to MirroredStrategy, it creates copies of all variables in the model on each device across all workers.\nIt uses CollectiveOps as the multi-worker all-reduce communication method used to keep variables in sync. \nA collective op is a single op in the TensorFlow graph which can automatically choose an all-reduce algorithm in the TensorFlow runtime according to hardware, network topology and tensor sizes.\nIt also implements additional performance optimizations. For example, it includes a static optimization that converts multiple all-reductions on small tensors into fewer all-reductions on larger tensors.\nHow?\n\n\nMultiWorkerMirroredStrategy currently allows you to choose between two different implementations of collective ops. CollectiveCommunication.RING implements ring-based collectives using gRPC as the communication layer. CollectiveCommunication.NCCL uses Nvidia’s NCCL to implement collectives.\n12multiworker_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(    tf.distribute.experimental.CollectiveCommunication.NCCL)\n\n\nTPUStrategytf.distribute.experimental.TPUStrategy lets you run your TensorFlow training on Tensor Processing Units (TPUs). \n12345cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(    tpu=tpu_address)tf.config.experimental_connect_to_cluster(cluster_resolver)tf.tpu.experimental.initialize_tpu_system(cluster_resolver)tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\n\nParameterServerStrategytf.distribute.experimental.ParameterServerStrategy supports parameter servers training on multiple machines. In this setup, some machines are designated as workers and some as parameter servers. Each variable of the model is placed on one parameter server. Computation is replicated across all GPUs of all the workers.\nOneDeviceStrategytf.distribute.OneDeviceStrategy runs on a single device. This strategy will place any variables created in its scope on the specified device. Input distributed through this strategy will be prefetched to the specified device. Moreover, any functions called via strategy.run will also be placed on the specified device.\nYou can use this strategy to test your code before switching to other strategies which actually distributes to multiple devices&#x2F;machines.\n1strategy = tf.distribute.OneDeviceStrategy(device=&quot;/gpu:0&quot;)\n\n\nUsing tf.distribute.Strategy with KerasBy integrating into tf.keras backend, we’ve made it seamless for you to distribute your training written in the Keras training framework.\nHere’s what you need to change in your code:\n\nCreate an instance of the appropriate tf.distribute.Strategy\nMove the creation and compiling of Keras model inside strategy.scope.\n\nWe support all types of Keras models - sequential, functional and subclassed.\nHere is a snippet of code to do this for a very simple Keras model with one dense layer:\n1234mirrored_strategy = tf.distribute.MirroredStrategy()with mirrored_strategy.scope():  model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])  model.compile(loss=&#x27;mse&#x27;, optimizer=&#x27;sgd&#x27;)\n\nIn this example we used MirroredStrategy so we can run this on a machine with multiple GPUs. \nstrategy.scope() indicated which parts of the code to run distributed. Creating a model inside this scope allows us to create mirrored variables instead of regular variables. Compiling under the scope allows us to know that the user intends to train this model using this strategy. Once this is set up, you can fit your model like you would normally. \nMirroredStrategy takes care of replicating the model’s training on the available GPUs, aggregating gradients, and more.\n123456789# tf.data.Dataset as datasetdataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(100).batch(10)model.fit(dataset, epochs=2)model.evaluate(dataset)# np array as datasetimport numpy as npinputs, targets = np.ones((100, 1)), np.ones((100, 1))model.fit(inputs, targets, epochs=2, batch_size=10)\n\nIn both cases (dataset or numpy), each batch of the given input is divided equally among the multiple replicas. For instance, if using MirroredStrategy with 2 GPUs, each batch of size 10 will get divided among the 2 GPUs, with each receiving 5 input examples in each step. \nEach epoch will then train faster as you add more GPUs. Typically, you would want to increase your batch size as you add more accelerators so as to make effective use of the extra computing power. You will also need to re-tune your learning rate, depending on the model. \nHow to merge these replicated models?\n\n\nYou can use strategy.num_replicas_in_sync to get the number of replicas.\n123456789# Compute global batch size using number of replicas.BATCH_SIZE_PER_REPLICA = 5global_batch_size = (BATCH_SIZE_PER_REPLICA *                     mirrored_strategy.num_replicas_in_sync)dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(100)dataset = dataset.batch(global_batch_size)LEARNING_RATES_BY_BATCH_SIZE = &#123;5: 0.1, 10: 0.15&#125;learning_rate = LEARNING_RATES_BY_BATCH_SIZE[global_batch_size]\n\nUsing tf.distribute.Strategy with custom training loopsIf you need more flexibility and control over your training loops than is possible with Estimator or Keras, you can write custom training loops. \nFor instance, when using a GAN, you may want to take a different number of generator or discriminator steps each round. Similarly, the high level frameworks are not very suitable for Reinforcement Learning training.\n12345678910111213141516171819202122232425262728293031323334with mirrored_strategy.scope():  model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])  optimizer = tf.keras.optimizers.SGD()dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(1000).batch(    global_batch_size)dist_dataset = mirrored_strategy.experimental_distribute_dataset(dataset)@tf.functiondef train_step(dist_inputs):  def step_fn(inputs):    features, labels = inputs    with tf.GradientTape() as tape:      # training=True is only needed if there are layers with different      # behavior during training versus inference (e.g. Dropout).      logits = model(features, training=True)      cross_entropy = tf.nn.softmax_cross_entropy_with_logits(          logits=logits, labels=labels)      # scaled the total loss by the global batch size      loss = tf.reduce_sum(cross_entropy) * (1.0 / global_batch_size)    grads = tape.gradient(loss, model.trainable_variables)    optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))    return cross_entropy  per_example_losses = mirrored_strategy.run(step_fn, args=(dist_inputs,))  mean_loss = mirrored_strategy.reduce(      tf.distribute.ReduceOp.MEAN, per_example_losses, axis=0)  return mean_loss with mirrored_strategy.scope():  for inputs in dist_dataset:    print(train_step(inputs))\n\n\nUsing tf.distribute.Strategy with Estimator (Limited support)tf.estimator is a distributed training TensorFlow API that originally supported the async parameter server approach. \nWe pass the strategy object into the RunConfig for the Estimator.\n12345678910111213mirrored_strategy = tf.distribute.MirroredStrategy()config = tf.estimator.RunConfig(    train_distribute=mirrored_strategy, eval_distribute=mirrored_strategy)regressor = tf.estimator.LinearRegressor(    feature_columns=[tf.feature_column.numeric_column(&#x27;feats&#x27;)],    optimizer=&#x27;SGD&#x27;,    config=config)def input_fn():  dataset = tf.data.Dataset.from_tensors((&#123;&quot;feats&quot;:[1.]&#125;, [1.]))  return dataset.repeat(1000).batch(10)regressor.train(input_fn=input_fn, steps=10)regressor.evaluate(input_fn=input_fn, steps=10)\n\nAnother difference to highlight here between Estimator and Keras is the input handling. In Estimator, we do not do automatic splitting of batch, nor automatically shard the data across different workers. You have full control over how you want your data to be distributed across workers and devices, and you must provide an input_fn to specify how to distribute your data.\nYour input_fn is called once per worker, thus giving one dataset per worker. Then one batch from that dataset is fed to one replica on that worker, thereby consuming N batches for N replicas on 1 worker. In other words, the dataset returned by the input_fn should provide batches of size PER_REPLICA_BATCH_SIZE. And the global batch size for a step can be obtained as PER_REPLICA_BATCH_SIZE * strategy.num_replicas_in_sync.\nUse a GPUThis guide is for users who have tried above approaches and found that they need fine-grained control of how TensorFlow uses the GPU.\n12import tensorflow as tfprint(&quot;Num GPUs Available: &quot;, len(tf.config.experimental.list_physical_devices(&#x27;GPU&#x27;)))\n\n\nLogging device placementTo find out which devices your operations and tensors are assigned to, put tf.debugging.set_log_device_placement(True) as the first statement of your program. Enabling device placement logging causes any Tensor allocations or operations to be printed.\n12345678tf.debugging.set_log_device_placement(True)# Create some tensorsa = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])c = tf.matmul(a, b)print(c)\n\nManual device placementIf you would like a particular operation to run on a device of your choice instead of what’s automatically selected for you, you can use with tf.device to create a device context, and all the operations within that context will run on the same designated device.\n123456789tf.debugging.set_log_device_placement(True)# Place tensors on the CPUwith tf.device(&#x27;/CPU:0&#x27;):  a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])  b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])c = tf.matmul(a, b)print(c)\n\nLimiting GPU memory growthBy default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to CUDA_VISIBLE_DEVICES) visible to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices by reducing memory fragmentation. To limit TensorFlow to a specific set of GPUs we use the tf.config.experimental.set_visible_devices method.\n12345678910gpus = tf.config.experimental.list_physical_devices(&#x27;GPU&#x27;)if gpus:  # Restrict TensorFlow to only use the first GPU  try:    tf.config.experimental.set_visible_devices(gpus[0], &#x27;GPU&#x27;)    logical_gpus = tf.config.experimental.list_logical_devices(&#x27;GPU&#x27;)    print(len(gpus), &quot;Physical GPUs,&quot;, len(logical_gpus), &quot;Logical GPU&quot;)  except RuntimeError as e:    # Visible devices must be set before GPUs have been initialized    print(e)\n\n\n1234567891011gpus = tf.config.experimental.list_physical_devices(&#x27;GPU&#x27;)if gpus:  try:    # Currently, memory growth needs to be the same across GPUs    for gpu in gpus:      tf.config.experimental.set_memory_growth(gpu, True)    logical_gpus = tf.config.experimental.list_logical_devices(&#x27;GPU&#x27;)    print(len(gpus), &quot;Physical GPUs,&quot;, len(logical_gpus), &quot;Logical GPUs&quot;)  except RuntimeError as e:    # Memory growth must be set before GPUs have been initialized    print(e)\n\nThe second method is to configure a virtual GPU device with tf.config.experimental.set_virtual_device_configuration and set a hard limit on the total memory to allocate on the GPU.\n123456789101112gpus = tf.config.experimental.list_physical_devices(&#x27;GPU&#x27;)if gpus:  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU  try:    tf.config.experimental.set_virtual_device_configuration(        gpus[0],        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])    logical_gpus = tf.config.experimental.list_logical_devices(&#x27;GPU&#x27;)    print(len(gpus), &quot;Physical GPUs,&quot;, len(logical_gpus), &quot;Logical GPUs&quot;)  except RuntimeError as e:    # Virtual devices must be set before GPUs have been initialized    print(e)\n\nUsing a single GPU on a multi-GPU systemIf you have more than one GPU in your system, the GPU with the lowest ID will be selected by default. If you would like to run on a different GPU, you will need to specify the preference explicitly:\n12345678910tf.debugging.set_log_device_placement(True)try:  # Specify an invalid GPU device  with tf.device(&#x27;/device:GPU:2&#x27;):    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])    c = tf.matmul(a, b)except RuntimeError as e:  print(e)\n\nIf you would like TensorFlow to automatically choose an existing and supported device to run the operations in case the specified one doesn’t exist, you can call tf.config.set_soft_device_placement(True).\nUsing multiple GPUsDeveloping for multiple GPUs will allow a model to scale with the additional resources. If developing on a system with a single GPU, we can simulate multiple GPUs with virtual devices. This enables easy testing of multi-GPU setups without requiring additional resources.\n12345678910111213gpus = tf.config.experimental.list_physical_devices(&#x27;GPU&#x27;)if gpus:  # Create 2 virtual GPUs with 1GB memory each  try:    tf.config.experimental.set_virtual_device_configuration(        gpus[0],        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024),         tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])    logical_gpus = tf.config.experimental.list_logical_devices(&#x27;GPU&#x27;)    print(len(gpus), &quot;Physical GPU,&quot;, len(logical_gpus), &quot;Logical GPUs&quot;)  except RuntimeError as e:    # Virtual devices must be set before GPUs have been initialized    print(e)\n\nThe best practice for using multiple GPUs is to use tf.distribute.Strategy. Here is a simple example:\n123456789tf.debugging.set_log_device_placement(True)strategy = tf.distribute.MirroredStrategy()with strategy.scope():  inputs = tf.keras.layers.Input(shape=(1,))  predictions = tf.keras.layers.Dense(1)(inputs)  model = tf.keras.models.Model(inputs=inputs, outputs=predictions)  model.compile(loss=&#x27;mse&#x27;,                optimizer=tf.keras.optimizers.SGD(learning_rate=0.2))\n\nError info: NCCL cannot support logical gpus\n\n\n\n","dateCreated":"2020-05-09T17:22:15+08:00","dateModified":"2020-05-13T03:52:43+08:00","datePublished":"2020-05-09T17:22:15+08:00","description":"Distributed training strategies and how to use multi GPUs","headline":"Distributed training with TensorFlow","image":["https://1.bp.blogspot.com/-cLxpo2BA8oI/XZMQrDSXefI/AAAAAAAAnhs/7FS6r94KFNQFzVt_8Ihx1iyTltt7if_xgCLcBGAsYHQ/s1600/TensorFlow%2B2.0%2BLogo.png","https://www.analyticsindiamag.com/wp-content/uploads/2019/03/tfk1.png"],"mainEntityOfPage":{"@type":"WebPage","@id":"https://joddiy.github.io/2020/05/09/TF2-0-Accelerators/"},"publisher":{"@type":"Organization","name":"Joddiy Zhang","sameAs":["https://github.com/joddiy","https://scholar.google.com/citations?user=KH-xv38AAAAJ&hl=en&oi=sra","https://www.linkedin.com/in/joddiyzhang/"],"image":"14108933.jpeg","logo":{"@type":"ImageObject","url":"14108933.jpeg"}},"url":"https://joddiy.github.io/2020/05/09/TF2-0-Accelerators/","keywords":"Tools, Deep Learning, Tensorflow","thumbnailUrl":"https://1.bp.blogspot.com/-cLxpo2BA8oI/XZMQrDSXefI/AAAAAAAAnhs/7FS6r94KFNQFzVt_8Ihx1iyTltt7if_xgCLcBGAsYHQ/s1600/TensorFlow%2B2.0%2BLogo.png"}</script>
    <meta name="description" content="Distributed training strategies and how to use multi GPUs">
<meta property="og:type" content="blog">
<meta property="og:title" content="Distributed training with TensorFlow">
<meta property="og:url" content="https://joddiy.github.io/2020/05/09/TF2-0-Accelerators/index.html">
<meta property="og:site_name" content="Jz Blog">
<meta property="og:description" content="Distributed training strategies and how to use multi GPUs">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-05-09T09:22:15.000Z">
<meta property="article:modified_time" content="2020-05-12T19:52:43.000Z">
<meta property="article:author" content="Joddiy Zhang">
<meta property="article:tag" content="Tools">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="Tensorflow">
<meta name="twitter:card" content="summary">
    
    
        
    
    
        <meta property="og:image" content="https://joddiy.github.io/assets/images/14108933.jpeg"/>
    
    
        <meta property="og:image" content="https://1.bp.blogspot.com/-cLxpo2BA8oI/XZMQrDSXefI/AAAAAAAAnhs/7FS6r94KFNQFzVt_8Ihx1iyTltt7if_xgCLcBGAsYHQ/s1600/TensorFlow%2B2.0%2BLogo.png"/>
        <meta class="swiftype" name="image" data-type="enum" content="https://1.bp.blogspot.com/-cLxpo2BA8oI/XZMQrDSXefI/AAAAAAAAnhs/7FS6r94KFNQFzVt_8Ihx1iyTltt7if_xgCLcBGAsYHQ/s1600/TensorFlow%2B2.0%2BLogo.png"/>
    
    
        <meta property="og:image" content="https://www.analyticsindiamag.com/wp-content/uploads/2019/03/tfk1.png"/>
        <meta class="swiftype" name="image" data-type="enum" content="https://www.analyticsindiamag.com/wp-content/uploads/2019/03/tfk1.png"/>
    
    
    <!--STYLES-->
    
<link rel="stylesheet" href="/assets/css/style-x8blglznjjnb9pnnwui5zw4h43ysufmsh1b0omicawm4vhqcutzqavokgpne.min.css">

    <!--STYLES END-->
    

    

    
        
            
<link rel="stylesheet" href="/assets/css/gitalk.css">

        
    
</head>

    <body>
        <div id="blog">
            <!-- Define author's picture -->


    
        
            
        
    

<header id="header" data-behavior="4">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a
            class="header-title-link"
            href="/"
            aria-label=""
        >
            Jz Blog
        </a>
    </div>
    
        
            <a
                class="header-right-picture "
                href="#about"
                aria-label="Open the link: /#about"
            >
        
        
            <img class="header-picture" src="/assets/images/14108933.jpeg" alt="Author&#39;s picture"/>
        
        </a>
    
</header>

            <!-- Define author's picture -->



        
    

<nav id="sidebar" data-behavior="4">
    <div class="sidebar-container">
        
            <div class="sidebar-profile">
                <a
                    href="/#about"
                    aria-label="Read more about the author"
                >
                    <img class="sidebar-profile-picture" src="/assets/images/14108933.jpeg" alt="Author&#39;s picture"/>
                </a>
                <h4 class="sidebar-profile-name">Joddiy Zhang</h4>
                
                    <h5 class="sidebar-profile-bio"><p><a href="mailto:&#106;&#x6f;&#x64;&#100;&#x69;&#121;&#x7a;&#104;&#97;&#x6e;&#103;&#x40;&#103;&#109;&#x61;&#x69;&#x6c;&#x2e;&#99;&#x6f;&#109;">&#106;&#x6f;&#x64;&#100;&#x69;&#121;&#x7a;&#104;&#97;&#x6e;&#103;&#x40;&#103;&#109;&#x61;&#x69;&#x6c;&#x2e;&#99;&#x6f;&#109;</a></p>
</h5>
                
            </div>
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/"
                            
                            rel="noopener"
                            title="Home"
                        >
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-categories"
                            
                            rel="noopener"
                            title="Categories"
                        >
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-tags"
                            
                            rel="noopener"
                            title="Tags"
                        >
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Tags</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-archives"
                            
                            rel="noopener"
                            title="Archives"
                        >
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="#about"
                            
                            rel="noopener"
                            title="About"
                        >
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://github.com/joddiy"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="GitHub"
                        >
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://scholar.google.com/citations?user=KH-xv38AAAAJ&hl=en&oi=sra"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="Google Scholar"
                        >
                        <i class="sidebar-button-icon fab fa-google" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Google Scholar</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://www.linkedin.com/in/joddiyzhang/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="LinkedIn"
                        >
                        <i class="sidebar-button-icon fab fa-linkedin" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">LinkedIn</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
        <div class="post-header-cover
                    text-center
                    post-header-cover--partial"
             style="background-image:url('https://www.analyticsindiamag.com/wp-content/uploads/2019/03/tfk1.png');"
             data-behavior="4">
            
                <div class="post-header main-content-wrap text-center">
    
        <h1 class="post-title">
            Distributed training with TensorFlow
        </h1>
    
    
        <div class="post-meta">
    <time datetime="2020-05-09T17:22:15+08:00">
	
		    May 09, 2020
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Study-Notes/">Study Notes</a>


    
</div>

    
</div>

            
        </div>

            <div id="main" data-behavior="4"
                 class="hasCover
                        hasCoverMetaIn
                        ">
                
<article class="post">
    
    
    <div class="post-content markdown">
        <div class="main-content-wrap">
            <h1 id="Distributed-training-with-TensorFlow"><a href="#Distributed-training-with-TensorFlow" class="headerlink" title="Distributed training with TensorFlow"></a>Distributed training with TensorFlow</h1><p><a target="_blank" rel="noopener" href="https://www.tensorflow.org/guide/distributed_training">doc link</a></p>
<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><code>tf.distribute.Strategy</code> is a TensorFlow API to distribute training across multiple GPUs, multiple machines or TPUs. Using this API, you can distribute your existing models and training code with minimal code changes.</p>
<hr>
<h2 id="Types-of-strategies"><a href="#Types-of-strategies" class="headerlink" title="Types of strategies"></a>Types of strategies</h2><p>tf.distribute.Strategy intends to cover a number of use cases along different axes. Some of these combinations are currently supported and others will be added in the future. Some of these axes are:</p>
<ul>
<li><p>Synchronous vs asynchronous training: These are two common ways of distributing training with data parallelism. In sync training, all workers train over different slices of input data in sync, and aggregating gradients at each step. In async training, all workers are independently training over the input data and updating variables asynchronously. Typically sync training is supported via all-reduce and async through parameter server architecture.</p>
</li>
<li><p>Hardware platform: You may want to scale your training onto multiple GPUs on one machine, or multiple machines in a network (with 0 or more GPUs each), or on Cloud TPUs.</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>Training API</th>
<th>MirroredStrategy</th>
<th>TPUStrategy</th>
<th>MultiWorkerMirroredStrategy</th>
</tr>
</thead>
<tbody><tr>
<td>Keras API</td>
<td>Supported</td>
<td>Experimental support</td>
<td>Experimental support</td>
</tr>
<tr>
<td>Custom training loop</td>
<td>Experimental support</td>
<td>Experimental support</td>
<td>Support planned post 2.0</td>
</tr>
<tr>
<td>Estimator API</td>
<td>Limited Support</td>
<td>Not supported</td>
<td>Limited Support</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>CentralStorageStrategy</th>
<th>ParameterServerStrategy</th>
<th>OneDeviceStrategy</th>
</tr>
</thead>
<tbody><tr>
<td>Experimental support</td>
<td>Supported planned post 2.0</td>
<td>Supported</td>
</tr>
<tr>
<td>Support</td>
<td>planned post 2.0</td>
<td>No support yet</td>
</tr>
<tr>
<td>Limited Support</td>
<td>Limited Support</td>
<td>Limited Support</td>
</tr>
</tbody></table>
<h3 id="MirroredStrategy"><a href="#MirroredStrategy" class="headerlink" title="MirroredStrategy"></a>MirroredStrategy</h3><p>tf.distribute.MirroredStrategy supports synchronous distributed training on multiple GPUs on one machine. It creates one replica per GPU device. Each variable in the model is mirrored across all the replicas. Together, these variables form a single conceptual variable called MirroredVariable. These variables are kept in sync with each other by applying identical updates.</p>
<p>Efficient <strong>all-reduce</strong> algorithms are used to communicate the variable updates across the devices.</p>
<p>All-reduce aggregates tensors across all the devices by adding them up, and makes them available on each device. It’s a fused algorithm that is very efficient and can reduce the overhead of synchronization significantly. </p>
<p>There are many all-reduce algorithms and implementations available, depending on the type of communication available between devices. By default, it uses NVIDIA NCCL as the all-reduce implementation. You can choose from a few other options we provide, or write your own.</p>
<p>Here is the simplest way of creating MirroredStrategy:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mirrored_strategy = tf.distribute.MirroredStrategy()</span><br></pre></td></tr></table></figure>

<p>This will create a MirroredStrategy instance which will use all the GPUs that are visible to TensorFlow, and use NCCL as the cross device communication.</p>
<p>If you wish to use only some of the GPUs on your machine, you can do so like this:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mirrored_strategy = tf.distribute.MirroredStrategy(devices=[<span class="string">&quot;/gpu:0&quot;</span>, <span class="string">&quot;/gpu:1&quot;</span>])</span><br></pre></td></tr></table></figure>

<p>Currently, tf.distribute.HierarchicalCopyAllReduce and tf.distribute.ReductionToOneDevice are two options other than tf.distribute.NcclAllReduce which is the default.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mirrored_strategy = tf.distribute.MirroredStrategy(</span><br><span class="line">    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())</span><br></pre></td></tr></table></figure>

<h3 id="CentralStorageStrategy"><a href="#CentralStorageStrategy" class="headerlink" title="CentralStorageStrategy"></a>CentralStorageStrategy</h3><p>tf.distribute.experimental.CentralStorageStrategy does synchronous training as well. Variables are not mirrored, instead they are placed on the CPU and operations are replicated across all local GPUs. If there is only one GPU, all variables and operations will be placed on that GPU.</p>
<p>Create an instance of CentralStorageStrategy by:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">central_storage_strategy = tf.distribute.experimental.CentralStorageStrategy()</span><br></pre></td></tr></table></figure>

<h3 id="MultiWorkerMirroredStrategy"><a href="#MultiWorkerMirroredStrategy" class="headerlink" title="MultiWorkerMirroredStrategy"></a>MultiWorkerMirroredStrategy</h3><p>tf.distribute.experimental.MultiWorkerMirroredStrategy is very similar to MirroredStrategy. It implements synchronous distributed training across multiple workers, each with potentially multiple GPUs. Similar to MirroredStrategy, it creates copies of all variables in the model on each device across all workers.</p>
<p>It uses CollectiveOps as the multi-worker all-reduce communication method used to keep variables in sync. </p>
<p>A collective op is a single op in the TensorFlow graph which can automatically choose an all-reduce algorithm in the TensorFlow runtime according to hardware, network topology and tensor sizes.</p>
<p>It also implements additional performance optimizations. For example, it includes a static optimization that converts multiple all-reductions on small tensors into fewer all-reductions on larger tensors.</p>
<div class="alert warning"><p>How?</p>
</div>

<p>MultiWorkerMirroredStrategy currently allows you to choose between two different implementations of collective ops. CollectiveCommunication.RING implements ring-based collectives using gRPC as the communication layer. CollectiveCommunication.NCCL uses Nvidia’s NCCL to implement collectives.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">multiworker_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(</span><br><span class="line">    tf.distribute.experimental.CollectiveCommunication.NCCL)</span><br></pre></td></tr></table></figure>


<h3 id="TPUStrategy"><a href="#TPUStrategy" class="headerlink" title="TPUStrategy"></a>TPUStrategy</h3><p>tf.distribute.experimental.TPUStrategy lets you run your TensorFlow training on Tensor Processing Units (TPUs). </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(</span><br><span class="line">    tpu=tpu_address)</span><br><span class="line">tf.config.experimental_connect_to_cluster(cluster_resolver)</span><br><span class="line">tf.tpu.experimental.initialize_tpu_system(cluster_resolver)</span><br><span class="line">tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)</span><br></pre></td></tr></table></figure>

<h3 id="ParameterServerStrategy"><a href="#ParameterServerStrategy" class="headerlink" title="ParameterServerStrategy"></a>ParameterServerStrategy</h3><p>tf.distribute.experimental.ParameterServerStrategy supports parameter servers training on multiple machines. In this setup, some machines are designated as workers and some as parameter servers. Each variable of the model is placed on one parameter server. Computation is replicated across all GPUs of all the workers.</p>
<h3 id="OneDeviceStrategy"><a href="#OneDeviceStrategy" class="headerlink" title="OneDeviceStrategy"></a>OneDeviceStrategy</h3><p>tf.distribute.OneDeviceStrategy runs on a single device. This strategy will place any variables created in its scope on the specified device. Input distributed through this strategy will be prefetched to the specified device. Moreover, any functions called via strategy.run will also be placed on the specified device.</p>
<p>You can use this strategy to test your code before switching to other strategies which actually distributes to multiple devices&#x2F;machines.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">strategy = tf.distribute.OneDeviceStrategy(device=<span class="string">&quot;/gpu:0&quot;</span>)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Using-tf-distribute-Strategy-with-Keras"><a href="#Using-tf-distribute-Strategy-with-Keras" class="headerlink" title="Using tf.distribute.Strategy with Keras"></a>Using tf.distribute.Strategy with Keras</h2><p>By integrating into tf.keras backend, we’ve made it seamless for you to distribute your training written in the Keras training framework.</p>
<p>Here’s what you need to change in your code:</p>
<ul>
<li>Create an instance of the appropriate tf.distribute.Strategy</li>
<li>Move the creation and compiling of Keras model inside strategy.scope.</li>
</ul>
<p>We support all types of Keras models - sequential, functional and subclassed.</p>
<p>Here is a snippet of code to do this for a very simple Keras model with one dense layer:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mirrored_strategy = tf.distribute.MirroredStrategy()</span><br><span class="line"><span class="keyword">with</span> mirrored_strategy.scope():</span><br><span class="line">  model = tf.keras.Sequential([tf.keras.layers.Dense(<span class="number">1</span>, input_shape=(<span class="number">1</span>,))])</span><br><span class="line">  model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;mse&#x27;</span>, optimizer=<span class="string">&#x27;sgd&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>In this example we used MirroredStrategy so we can run this on a machine with multiple GPUs. </p>
<p>strategy.scope() indicated which parts of the code to run distributed. Creating a model inside this scope allows us to create mirrored variables instead of regular variables. Compiling under the scope allows us to know that the user intends to train this model using this strategy. Once this is set up, you can fit your model like you would normally. </p>
<p>MirroredStrategy takes care of replicating the model’s training on the available GPUs, aggregating gradients, and more.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf.data.Dataset as dataset</span></span><br><span class="line">dataset = tf.data.Dataset.from_tensors(([<span class="number">1.</span>], [<span class="number">1.</span>])).repeat(<span class="number">100</span>).batch(<span class="number">10</span>)</span><br><span class="line">model.fit(dataset, epochs=<span class="number">2</span>)</span><br><span class="line">model.evaluate(dataset)</span><br><span class="line"></span><br><span class="line"><span class="comment"># np array as dataset</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">inputs, targets = np.ones((<span class="number">100</span>, <span class="number">1</span>)), np.ones((<span class="number">100</span>, <span class="number">1</span>))</span><br><span class="line">model.fit(inputs, targets, epochs=<span class="number">2</span>, batch_size=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<p>In both cases (dataset or numpy), each batch of the given input is divided equally among the multiple replicas. For instance, if using MirroredStrategy with 2 GPUs, each batch of size 10 will get divided among the 2 GPUs, with each receiving 5 input examples in each step. </p>
<p>Each epoch will then train faster as you add more GPUs. Typically, you would want to increase your batch size as you add more accelerators so as to make effective use of the extra computing power. You will also need to re-tune your learning rate, depending on the model. </p>
<div class="alert warning"><p>How to merge these replicated models?</p>
</div>

<p>You can use strategy.num_replicas_in_sync to get the number of replicas.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compute global batch size using number of replicas.</span></span><br><span class="line">BATCH_SIZE_PER_REPLICA = <span class="number">5</span></span><br><span class="line">global_batch_size = (BATCH_SIZE_PER_REPLICA *</span><br><span class="line">                     mirrored_strategy.num_replicas_in_sync)</span><br><span class="line">dataset = tf.data.Dataset.from_tensors(([<span class="number">1.</span>], [<span class="number">1.</span>])).repeat(<span class="number">100</span>)</span><br><span class="line">dataset = dataset.batch(global_batch_size)</span><br><span class="line"></span><br><span class="line">LEARNING_RATES_BY_BATCH_SIZE = &#123;<span class="number">5</span>: <span class="number">0.1</span>, <span class="number">10</span>: <span class="number">0.15</span>&#125;</span><br><span class="line">learning_rate = LEARNING_RATES_BY_BATCH_SIZE[global_batch_size]</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="Using-tf-distribute-Strategy-with-custom-training-loops"><a href="#Using-tf-distribute-Strategy-with-custom-training-loops" class="headerlink" title="Using tf.distribute.Strategy with custom training loops"></a>Using tf.distribute.Strategy with custom training loops</h2><p>If you need more flexibility and control over your training loops than is possible with Estimator or Keras, you can write custom training loops. </p>
<p>For instance, when using a GAN, you may want to take a different number of generator or discriminator steps each round. Similarly, the high level frameworks are not very suitable for Reinforcement Learning training.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> mirrored_strategy.scope():</span><br><span class="line">  model = tf.keras.Sequential([tf.keras.layers.Dense(<span class="number">1</span>, input_shape=(<span class="number">1</span>,))])</span><br><span class="line">  optimizer = tf.keras.optimizers.SGD()</span><br><span class="line"></span><br><span class="line">dataset = tf.data.Dataset.from_tensors(([<span class="number">1.</span>], [<span class="number">1.</span>])).repeat(<span class="number">1000</span>).batch(</span><br><span class="line">    global_batch_size)</span><br><span class="line">dist_dataset = mirrored_strategy.experimental_distribute_dataset(dataset)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_step</span>(<span class="params">dist_inputs</span>):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">step_fn</span>(<span class="params">inputs</span>):</span><br><span class="line">    features, labels = inputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">      <span class="comment"># training=True is only needed if there are layers with different</span></span><br><span class="line">      <span class="comment"># behavior during training versus inference (e.g. Dropout).</span></span><br><span class="line">      logits = model(features, training=<span class="literal">True</span>)</span><br><span class="line">      cross_entropy = tf.nn.softmax_cross_entropy_with_logits(</span><br><span class="line">          logits=logits, labels=labels)</span><br><span class="line">      <span class="comment"># scaled the total loss by the global batch size</span></span><br><span class="line">      loss = tf.reduce_sum(cross_entropy) * (<span class="number">1.0</span> / global_batch_size)</span><br><span class="line"></span><br><span class="line">    grads = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(<span class="built_in">list</span>(<span class="built_in">zip</span>(grads, model.trainable_variables)))</span><br><span class="line">    <span class="keyword">return</span> cross_entropy</span><br><span class="line"></span><br><span class="line">  per_example_losses = mirrored_strategy.run(step_fn, args=(dist_inputs,))</span><br><span class="line">  mean_loss = mirrored_strategy.reduce(</span><br><span class="line">      tf.distribute.ReduceOp.MEAN, per_example_losses, axis=<span class="number">0</span>)</span><br><span class="line">  <span class="keyword">return</span> mean_loss</span><br><span class="line"></span><br><span class="line"> <span class="keyword">with</span> mirrored_strategy.scope():</span><br><span class="line">  <span class="keyword">for</span> inputs <span class="keyword">in</span> dist_dataset:</span><br><span class="line">    <span class="built_in">print</span>(train_step(inputs))</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Using-tf-distribute-Strategy-with-Estimator-Limited-support"><a href="#Using-tf-distribute-Strategy-with-Estimator-Limited-support" class="headerlink" title="Using tf.distribute.Strategy with Estimator (Limited support)"></a>Using tf.distribute.Strategy with Estimator (Limited support)</h2><p>tf.estimator is a distributed training TensorFlow API that originally supported the async parameter server approach. </p>
<p>We pass the strategy object into the RunConfig for the Estimator.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">mirrored_strategy = tf.distribute.MirroredStrategy()</span><br><span class="line">config = tf.estimator.RunConfig(</span><br><span class="line">    train_distribute=mirrored_strategy, eval_distribute=mirrored_strategy)</span><br><span class="line">regressor = tf.estimator.LinearRegressor(</span><br><span class="line">    feature_columns=[tf.feature_column.numeric_column(<span class="string">&#x27;feats&#x27;</span>)],</span><br><span class="line">    optimizer=<span class="string">&#x27;SGD&#x27;</span>,</span><br><span class="line">    config=config)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">input_fn</span>():</span><br><span class="line">  dataset = tf.data.Dataset.from_tensors((&#123;<span class="string">&quot;feats&quot;</span>:[<span class="number">1.</span>]&#125;, [<span class="number">1.</span>]))</span><br><span class="line">  <span class="keyword">return</span> dataset.repeat(<span class="number">1000</span>).batch(<span class="number">10</span>)</span><br><span class="line">regressor.train(input_fn=input_fn, steps=<span class="number">10</span>)</span><br><span class="line">regressor.evaluate(input_fn=input_fn, steps=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<hr>
<p>Another difference to highlight here between Estimator and Keras is the input handling. In Estimator, we do not do automatic splitting of batch, nor automatically shard the data across different workers. You have full control over how you want your data to be distributed across workers and devices, and you must provide an input_fn to specify how to distribute your data.</p>
<p>Your input_fn is called once per worker, thus giving one dataset per worker. Then one batch from that dataset is fed to one replica on that worker, thereby consuming N batches for N replicas on 1 worker. In other words, the dataset returned by the input_fn should provide batches of size PER_REPLICA_BATCH_SIZE. And the global batch size for a step can be obtained as PER_REPLICA_BATCH_SIZE * strategy.num_replicas_in_sync.</p>
<h2 id="Use-a-GPU"><a href="#Use-a-GPU" class="headerlink" title="Use a GPU"></a>Use a GPU</h2><p>This guide is for users who have tried above approaches and found that they need fine-grained control of how TensorFlow uses the GPU.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Num GPUs Available: &quot;</span>, <span class="built_in">len</span>(tf.config.experimental.list_physical_devices(<span class="string">&#x27;GPU&#x27;</span>)))</span><br></pre></td></tr></table></figure>


<h3 id="Logging-device-placement"><a href="#Logging-device-placement" class="headerlink" title="Logging device placement"></a>Logging device placement</h3><p>To find out which devices your operations and tensors are assigned to, put tf.debugging.set_log_device_placement(True) as the first statement of your program. Enabling device placement logging causes any Tensor allocations or operations to be printed.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.debugging.set_log_device_placement(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create some tensors</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>], [<span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line">c = tf.matmul(a, b)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(c)</span><br></pre></td></tr></table></figure>

<h3 id="Manual-device-placement"><a href="#Manual-device-placement" class="headerlink" title="Manual device placement"></a>Manual device placement</h3><p>If you would like a particular operation to run on a device of your choice instead of what’s automatically selected for you, you can use with tf.device to create a device context, and all the operations within that context will run on the same designated device.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.debugging.set_log_device_placement(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Place tensors on the CPU</span></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">&#x27;/CPU:0&#x27;</span>):</span><br><span class="line">  a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line">  b = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>], [<span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line"></span><br><span class="line">c = tf.matmul(a, b)</span><br><span class="line"><span class="built_in">print</span>(c)</span><br></pre></td></tr></table></figure>

<h3 id="Limiting-GPU-memory-growth"><a href="#Limiting-GPU-memory-growth" class="headerlink" title="Limiting GPU memory growth"></a>Limiting GPU memory growth</h3><p>By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to CUDA_VISIBLE_DEVICES) visible to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices by reducing memory fragmentation. To limit TensorFlow to a specific set of GPUs we use the tf.config.experimental.set_visible_devices method.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">gpus = tf.config.experimental.list_physical_devices(<span class="string">&#x27;GPU&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> gpus:</span><br><span class="line">  <span class="comment"># Restrict TensorFlow to only use the first GPU</span></span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    tf.config.experimental.set_visible_devices(gpus[<span class="number">0</span>], <span class="string">&#x27;GPU&#x27;</span>)</span><br><span class="line">    logical_gpus = tf.config.experimental.list_logical_devices(<span class="string">&#x27;GPU&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(gpus), <span class="string">&quot;Physical GPUs,&quot;</span>, <span class="built_in">len</span>(logical_gpus), <span class="string">&quot;Logical GPU&quot;</span>)</span><br><span class="line">  <span class="keyword">except</span> RuntimeError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="comment"># Visible devices must be set before GPUs have been initialized</span></span><br><span class="line">    <span class="built_in">print</span>(e)</span><br></pre></td></tr></table></figure>


<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">gpus = tf.config.experimental.list_physical_devices(<span class="string">&#x27;GPU&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> gpus:</span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    <span class="comment"># Currently, memory growth needs to be the same across GPUs</span></span><br><span class="line">    <span class="keyword">for</span> gpu <span class="keyword">in</span> gpus:</span><br><span class="line">      tf.config.experimental.set_memory_growth(gpu, <span class="literal">True</span>)</span><br><span class="line">    logical_gpus = tf.config.experimental.list_logical_devices(<span class="string">&#x27;GPU&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(gpus), <span class="string">&quot;Physical GPUs,&quot;</span>, <span class="built_in">len</span>(logical_gpus), <span class="string">&quot;Logical GPUs&quot;</span>)</span><br><span class="line">  <span class="keyword">except</span> RuntimeError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="comment"># Memory growth must be set before GPUs have been initialized</span></span><br><span class="line">    <span class="built_in">print</span>(e)</span><br></pre></td></tr></table></figure>

<p>The second method is to configure a virtual GPU device with tf.config.experimental.set_virtual_device_configuration and set a hard limit on the total memory to allocate on the GPU.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">gpus = tf.config.experimental.list_physical_devices(<span class="string">&#x27;GPU&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> gpus:</span><br><span class="line">  <span class="comment"># Restrict TensorFlow to only allocate 1GB of memory on the first GPU</span></span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    tf.config.experimental.set_virtual_device_configuration(</span><br><span class="line">        gpus[<span class="number">0</span>],</span><br><span class="line">        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=<span class="number">1024</span>)])</span><br><span class="line">    logical_gpus = tf.config.experimental.list_logical_devices(<span class="string">&#x27;GPU&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(gpus), <span class="string">&quot;Physical GPUs,&quot;</span>, <span class="built_in">len</span>(logical_gpus), <span class="string">&quot;Logical GPUs&quot;</span>)</span><br><span class="line">  <span class="keyword">except</span> RuntimeError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="comment"># Virtual devices must be set before GPUs have been initialized</span></span><br><span class="line">    <span class="built_in">print</span>(e)</span><br></pre></td></tr></table></figure>

<h3 id="Using-a-single-GPU-on-a-multi-GPU-system"><a href="#Using-a-single-GPU-on-a-multi-GPU-system" class="headerlink" title="Using a single GPU on a multi-GPU system"></a>Using a single GPU on a multi-GPU system</h3><p>If you have more than one GPU in your system, the GPU with the lowest ID will be selected by default. If you would like to run on a different GPU, you will need to specify the preference explicitly:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tf.debugging.set_log_device_placement(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">  <span class="comment"># Specify an invalid GPU device</span></span><br><span class="line">  <span class="keyword">with</span> tf.device(<span class="string">&#x27;/device:GPU:2&#x27;</span>):</span><br><span class="line">    a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line">    b = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>], [<span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line">    c = tf.matmul(a, b)</span><br><span class="line"><span class="keyword">except</span> RuntimeError <span class="keyword">as</span> e:</span><br><span class="line">  <span class="built_in">print</span>(e)</span><br></pre></td></tr></table></figure>

<p>If you would like TensorFlow to automatically choose an existing and supported device to run the operations in case the specified one doesn’t exist, you can call tf.config.set_soft_device_placement(True).</p>
<h3 id="Using-multiple-GPUs"><a href="#Using-multiple-GPUs" class="headerlink" title="Using multiple GPUs"></a>Using multiple GPUs</h3><p>Developing for multiple GPUs will allow a model to scale with the additional resources. If developing on a system with a single GPU, we can simulate multiple GPUs with virtual devices. This enables easy testing of multi-GPU setups without requiring additional resources.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">gpus = tf.config.experimental.list_physical_devices(<span class="string">&#x27;GPU&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> gpus:</span><br><span class="line">  <span class="comment"># Create 2 virtual GPUs with 1GB memory each</span></span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    tf.config.experimental.set_virtual_device_configuration(</span><br><span class="line">        gpus[<span class="number">0</span>],</span><br><span class="line">        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=<span class="number">1024</span>),</span><br><span class="line">         tf.config.experimental.VirtualDeviceConfiguration(memory_limit=<span class="number">1024</span>)])</span><br><span class="line">    logical_gpus = tf.config.experimental.list_logical_devices(<span class="string">&#x27;GPU&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(gpus), <span class="string">&quot;Physical GPU,&quot;</span>, <span class="built_in">len</span>(logical_gpus), <span class="string">&quot;Logical GPUs&quot;</span>)</span><br><span class="line">  <span class="keyword">except</span> RuntimeError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="comment"># Virtual devices must be set before GPUs have been initialized</span></span><br><span class="line">    <span class="built_in">print</span>(e)</span><br></pre></td></tr></table></figure>

<p>The best practice for using multiple GPUs is to use tf.distribute.Strategy. Here is a simple example:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.debugging.set_log_device_placement(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">strategy = tf.distribute.MirroredStrategy()</span><br><span class="line"><span class="keyword">with</span> strategy.scope():</span><br><span class="line">  inputs = tf.keras.layers.Input(shape=(<span class="number">1</span>,))</span><br><span class="line">  predictions = tf.keras.layers.Dense(<span class="number">1</span>)(inputs)</span><br><span class="line">  model = tf.keras.models.Model(inputs=inputs, outputs=predictions)</span><br><span class="line">  model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;mse&#x27;</span>,</span><br><span class="line">                optimizer=tf.keras.optimizers.SGD(learning_rate=<span class="number">0.2</span>))</span><br></pre></td></tr></table></figure>

<div class="alert danger"><p>Error info: NCCL cannot support logical gpus</p>
</div>



            


        </div>
    </div>
    <div id="post-footer" class="post-footer main-content-wrap">
        
            <div class="post-footer-tags">
                <span class="text-color-light text-small">TAGGED IN</span><br/>
                
    <a class="tag tag--primary tag--small t-none-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a> <a class="tag tag--primary tag--small t-none-link" href="/tags/Tensorflow/" rel="tag">Tensorflow</a> <a class="tag tag--primary tag--small t-none-link" href="/tags/Tools/" rel="tag">Tools</a>

            </div>
        
        
            <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2020/05/11/C-Plus-Plus/"
                    data-tooltip="C++ Notes"
                    aria-label="PREVIOUS: C++ Notes"
                >
                    
                        <i class="fa fa-angle-left" aria-hidden="true"></i>
                        <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                    </a>
            </li>
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2020/05/07/TF2-0-Function/"
                    data-tooltip="TensorFlow 2.0 Functions, not Sessions."
                    aria-label="NEXT: TensorFlow 2.0 Functions, not Sessions."
                >
                    
                        <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                        <i class="fa fa-angle-right" aria-hidden="true"></i>
                    </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a
                class="post-action-btn btn btn--default btn-open-shareoptions"
                href="#btn-open-shareoptions"
                aria-label="Share this post"
            >
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://joddiy.github.io/2020/05/09/TF2-0-Accelerators/"
                    title="Share on Facebook"
                    aria-label="Share on Facebook"
                >
                    <i class="fab fa-facebook" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://twitter.com/intent/tweet?text=https://joddiy.github.io/2020/05/09/TF2-0-Accelerators/"
                    title="Share on Twitter"
                    aria-label="Share on Twitter"
                >
                    <i class="fab fa-twitter" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://plus.google.com/share?url=https://joddiy.github.io/2020/05/09/TF2-0-Accelerators/"
                    title="Share on Google+"
                    aria-label="Share on Google+"
                >
                    <i class="fab fa-google-plus" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
                <li class="post-action">
                    <a
                        class="post-action-btn btn btn--default"
                        href="#gitalk"
                        aria-label="Leave a comment"
                    >
                        <i class="fa fa-comment"></i>
                    </a>
                </li>
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


        
        
            
                <div id="gitalk"></div>

            
        
    </div>
</article>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2024 Joddiy Zhang. All Rights Reserved.
    </span>
</footer>

            </div>
            
                <div id="bottom-bar" class="post-bottom-bar" data-behavior="4">
                    <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2020/05/11/C-Plus-Plus/"
                    data-tooltip="C++ Notes"
                    aria-label="PREVIOUS: C++ Notes"
                >
                    
                        <i class="fa fa-angle-left" aria-hidden="true"></i>
                        <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                    </a>
            </li>
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2020/05/07/TF2-0-Function/"
                    data-tooltip="TensorFlow 2.0 Functions, not Sessions."
                    aria-label="NEXT: TensorFlow 2.0 Functions, not Sessions."
                >
                    
                        <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                        <i class="fa fa-angle-right" aria-hidden="true"></i>
                    </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a
                class="post-action-btn btn btn--default btn-open-shareoptions"
                href="#btn-open-shareoptions"
                aria-label="Share this post"
            >
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://joddiy.github.io/2020/05/09/TF2-0-Accelerators/"
                    title="Share on Facebook"
                    aria-label="Share on Facebook"
                >
                    <i class="fab fa-facebook" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://twitter.com/intent/tweet?text=https://joddiy.github.io/2020/05/09/TF2-0-Accelerators/"
                    title="Share on Twitter"
                    aria-label="Share on Twitter"
                >
                    <i class="fab fa-twitter" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://plus.google.com/share?url=https://joddiy.github.io/2020/05/09/TF2-0-Accelerators/"
                    title="Share on Google+"
                    aria-label="Share on Google+"
                >
                    <i class="fab fa-google-plus" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
                <li class="post-action">
                    <a
                        class="post-action-btn btn btn--default"
                        href="#gitalk"
                        aria-label="Leave a comment"
                    >
                        <i class="fa fa-comment"></i>
                    </a>
                </li>
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


                </div>
                
    <div id="share-options-bar" class="share-options-bar" data-behavior="4">
        <i id="btn-close-shareoptions" class="fa fa-times"></i>
        <ul class="share-options">
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://www.facebook.com/sharer/sharer.php?u=https://joddiy.github.io/2020/05/09/TF2-0-Accelerators/"
                        aria-label="Share on Facebook"
                    >
                        <i class="fab fa-facebook" aria-hidden="true"></i><span>Share on Facebook</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://twitter.com/intent/tweet?text=https://joddiy.github.io/2020/05/09/TF2-0-Accelerators/"
                        aria-label="Share on Twitter"
                    >
                        <i class="fab fa-twitter" aria-hidden="true"></i><span>Share on Twitter</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://plus.google.com/share?url=https://joddiy.github.io/2020/05/09/TF2-0-Accelerators/"
                        aria-label="Share on Google+"
                    >
                        <i class="fab fa-google-plus" aria-hidden="true"></i><span>Share on Google+</span>
                    </a>
                </li>
            
        </ul>
    </div>


            
        </div>
        


    
        
    

<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-times"></i>
        </div>
        
            <img id="about-card-picture" src="/assets/images/14108933.jpeg" alt="Author&#39;s picture"/>
        
            <h4 id="about-card-name">Joddiy Zhang</h4>
        
            <div id="about-card-bio"><p><a href="mailto:&#106;&#111;&#100;&#x64;&#x69;&#x79;&#x7a;&#104;&#97;&#110;&#x67;&#64;&#x67;&#109;&#97;&#105;&#108;&#x2e;&#x63;&#111;&#109;">&#106;&#111;&#100;&#x64;&#x69;&#x79;&#x7a;&#104;&#97;&#110;&#x67;&#64;&#x67;&#109;&#97;&#105;&#108;&#x2e;&#x63;&#111;&#109;</a></p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>Machine Learning Engineer</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker-alt"></i>
                <br/>
                Singapore
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->

<script src="/assets/js/script-sqrh47zm5nkjgifq4rx38uvns4r2rarrrvwuhjxiztyrddruca5ukl7nw6br.min.js"></script>

<!--SCRIPTS END-->


    
        
<script src="/assets/js/gitalk.js"></script>

        <script type="text/javascript">
          (function() {
            new Gitalk({
              clientID: 'bee9685b2dc9739b6bd5',
              clientSecret: 'a0c683f383a94fae6d021ab932f37f7e56899410',
              repo: 'joddiy.github.io',
              owner: 'joddiy',
              admin: ['joddiy'],
              id: '2020/05/09/TF2-0-Accelerators/',
              ...{"language":"en","perPage":10,"distractionFreeMode":false,"enableHotKey":true,"pagerDirection":"first"}
            }).render('gitalk')
          })()
        </script>
    




    </body>
</html>
