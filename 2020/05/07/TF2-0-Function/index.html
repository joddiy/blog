
<!DOCTYPE html>
<html lang="en">
    
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Jz Blog">
    <title>TensorFlow 2.0 Functions, not Sessions. - Jz Blog</title>
    <meta name="author" content="Joddiy Zhang">
    
    
        <link rel="icon" href="https://joddiy.github.io/assets/images/favicon.ico">
    
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Joddiy Zhang","sameAs":["https://github.com/joddiy","https://scholar.google.com/citations?user=KH-xv38AAAAJ&hl=en&oi=sra","https://www.linkedin.com/in/joddiyzhang/"],"image":"14108933.jpeg"},"articleBody":"TensorFlow 2.0: Functions, not Sessions.doc link\nDesign ProposalBasic idea: Python functions as GraphsWhere tf.function is a decorator that “defines a TensorFlow function”. A “TensorFlow function” defines a computation as a graph of TensorFlow operations, with named arguments and explicit return values. \n123456789101112import tensorflow as tf@tf.functiondef compute_z1(x, y):  return tf.add(x, y)@tf.functiondef compute_z0(x):  return compute_z1(x, tf.square(x))z0 = compute_z0(2.)z1 = compute_z1(2., 2.)\n\nHaving the Python function correspond to what the runtime will execute reduces conceptual complexity in translating between the two domains. \nReferencing state: Variables, tables etc.A function decorated Python function encapsulates a graph and its execution. The Python function may reference stateful objects (i.e., state backed by DT_RESOURCE tensors in the runtime, e.g., tf.Variable) by referencing the corresponding Python object, and these will be captured as implicit inputs to the function.\nComparing TensorFlow code today with how we propose it looks in 2.x:\n12345678910111213141516171819202122232425262728293031323334# TF 1.xW = tf.Variable(  tf.glorot_uniform_initializer()(    (10, 10)))b = tf.Variable(tf.zeros(10))c = tf.Variable(0)x = tf.placeholder(tf.float32)ctr = c.assign_add(1)with tf.control_dependencies([ctr]):  y = tf.matmul(x, W) + binit =   tf.global_variables_initializer()with tf.Session() as sess:  sess.run(init)  print(sess.run(y,  feed_dict=&#123;x: make_input_value()&#125;))  assert int(sess.run(c)) == 1# TF 2.0W = tf.Variable(  tf.glorot_uniform_initializer()(    (10, 10)))b = tf.Variable(tf.zeros(10))c = tf.Variable(0)@tf.functiondef f(x):  c.assign_add(1)  return tf.matmul(x, W) + bprint(f(make_input_value())assert int(c) == 1\n\nWorthy of note here - in TensorFlow 1.x, the memory underlying the variables W and b in the runtime lives for the lifetime of the Session - unrelated to the lifetime of the Python objects. In 2.x, the lifetime of the Python objects and the runtime state are tied together.\nControl dependenciesIn TensorFlow graphs today, control dependencies are sometimes needed to ensure correct evaluation order. \n123456789101112131415161718192021# TF 1.xv = tf.Variable(1.0)init_op = tf.global_variables_initializer()assign_op = v.assign(2.0)read = v.read_value()with tf.Session() as sess:  sess.run(init_op)  val = sess.run(read)  print(val) # Will print 1.0, the assign is ignored  val = sess.run([read, assign_op])[0]  print(val)   # Non-deterministically prints 1.0 or 2.0, # TF2.0 v = tf.Variable(1.0)@tf.functiondef f():  v.assign(2.0)  return v.read_value()print(f()) # Always prints 2.0.\n\n\nNote that the intention here is to avoid observable differences from program order. For example:\n12345678a = tf.Variable(1.0)b = tf.Variable(1.0)@tf.functiondef f():  a.assign(2.0)  b.assign(3.0)  return a + bprint(f())\n\nWill always print 5.0 since the assignments will occur before the read. However, there is no guaranteed ordering between the assignment of a and b (as any difference in that is not observable).\nFunctions that create state1234567891011v = None@tf.functiondef f(x):  global v  if v is None:    v = tf.Variable(1.0)  return tf.cast(x, tf.float32) + vf(tf.constant(1, dtype=tf.float32)) # Creates the variable, returns 2.0f(tf.constant(2, dtype=tf.int32))   # Reuses the variable, returns 3.0\n\n\nState (like tf.Variable objects) are only created the first time the function f is called. If any variables are created in the first execution of f, then @tf.function will trace f again the second time it is invoked in order to record the behavior that will be used from then on. \nThe caller must make sure that any variable referenced by the function still exists whenever the function is evaluated. @tf.function itself will keep only weak references to these created variables.\n\nTrace CachesSince new graphs are traced when new input signatures are encountered, a function can encapsulate multiple graphs. For example, considering the following, there are two graphs created here:\n123456@tf.functiondef f(x):  return tf.square(x)f(tf.constant(1, dtype=tf.int32))f(tf.constant(1.0, dtype=tf.float32))\n\nNote the use of tf.constant to ensure that the argument is a Tensor. If the argument were a Python value, then additional graphs will be traced for each such value. For example, the following two calls will result in two additional graphs being traced:\n12f(1.0)f(2.0)\n\nWhere arguments are not Tensors, the “value” of the argument is used to compute the trace_cache_key. For example:\n123456@tf.functiondef f(x, use_multiply):  return tf.multiply(x, x) if use_multiply else tf.square(x)f(tf.constant(2.0), True)f(tf.constant(2.0), False)\nwill result in 2 graphs being created, since the two calls result in two different cache keys because the value of the Python object (the second argument) changes between the two.\nNote that the “type” of Tensor inputs to the function also incorporates the shape. For example:\n123456789@tf.functiondef f(x): \treturn tf.add(x, 1.)f(tf.constant([2.0]))f(tf.constant([2.0, 3.0]))f(tf.constant([[2.0]]))f(tf.constant([3.0]))f(tf.constant([4.0, 5.0]))\n\nwill result in 3 graphs being created.\nThe trace_cache_key also incorporates the “context” in which the call was made. For example:\n1234567@tf.functiondef f(x): return tf.add(x, 1.)with tf.device(&quot;/device:CPU:0&quot;):  f(tf.constant(2.0))with tf.device(&quot;/device:GPU:0&quot;):  f(tf.constant(2.0))\n\nWill create 2 graphs.\nCAUTION: Too many traces\nCAUTION: Mutable non-Tensor arguments\nThe trace_cache_key includes the Python object for non-Tensor arguments. Mutations of these arguments might not be detected. For example:\n123456789101112# non-Tensor objectclass Params(object):  multiply = Truep = Params()@tf.functiondef f(x, y):  return tf.multiply(x, 2.) if y.multiply else tf.add(x, 2.)f(3., p) # Returns 6.0p.multiply = Falsef(3., p)  # Mutations to `p` may not trigger a retrace, so might still return 6.0\n\nInput SignaturesAn “input signature” can be explicitly specified to control the trace_cache_key computation based on the type and shape of Tensor (and list of Tensor) arguments to f.\n1234567891011121314@tf.function(input_signature=((tf.float32, [None]))def f(x): \treturn tf.add(x, 1.)f(tf.constant([2.0]))      # Returns [3.0]f(tf.constant([2.0, 3.0])) # Matches the input signature as [None]                           # matches the actual shape [2]f(tf.constant([[2.0]]))    # Raises an error as the arguments don&#x27;t match the                           # input signature.f(tf.constant([2], dtype=tf.int32)) # Raises an error as the dtype of the argument                                    # does not match the input signature# f is backed by a single Graph since the input signature specification allowed# for the same graph to be used when the input shape is (1,) or (2,).\n\n\nFor a Tensor argument, it specifies a (dtype, shape pattern).\n(tf.float32, [None]) means the argument must be a float32 vector (with any number of elements).\n(tf.int32, []) means that the argument must be an int32 scalar.\n\n\nFor a list of Tensor objects, it specifies an optional list length and the signature for elements in the list (i.e., the dtype and shape pattern for all elements in the list).\nFor non-Tensor arguments: tf.PYTHON_VALUE\n\nYou can use the tf.TRACE_ON_NEW_VALUE to release the restriction of dtype:\n123456@tf.function(input_signature=((tf.TRACE_ON_NEW_VALUE, [None]))def f(x): \treturn tf.square(x)f(tf.constant([2.0])) # Returns 4.0f(tf.constant([2, 2], dtype=tf.int32) # Returns [4, 4] after tracing a new graph\n\nClassesIf a member function of a class does not create variables, it may be decorated with @tf.function and it will work:\n123456789101112131415161718class AnyShapeModel(object):  def __init__(self):    self.v = None  @tf.function  def increment(self, amount):    if self.v is None:      self.v = tf.Variable(tf.zeros_like(amount))    self.v.assign_add(amount)model1 = AnyShapeModel()model1.increment(tf.constant(3))assert int(model1.v) == 3model1.increment(tf.constant(4))assert int(model1.v) == 7model2 = AnyShapeModel()model2.increment(tf.constant([4, 5]))assert model2.v.numpy() == [4, 5]\n\nThe semantics here are that each new instance is allowed to create variables in each @tf.function once.\nfunction-ing Python control flowIf the function has data-dependent control flow then though the function will execute fine with eager execution enabled, function decorating it will fail. For example:\n123456789101112def f(x, y):  if tf.equal(y, 0.0):    return y  return x / yx = tf.constant(2.0)y = tf.constant(2.0)f(x, y) # Will be 1.0df = tf.function(f)df(x, y)  # Will raise an error complaining about the data-dependent control flow\n\nTo fix this, one would have to use the graph construction APIs for control flow (tf.cond, tf.while_loop):\n12345678910def f(x, y):  return tf.cond(tf.equal(y, 0.0), lambda: y, lambda: x/y)x = tf.constant(2.0)y = tf.constant(2.0)f(x, y) # Will be 1.0df = tf.function(f)df(x, y)  # Will be 1.0\n\nThis situation can be improved with the help of autograph to allow expression of control flow in Python. \n12df = tf.function(autograph=True)(f)f(x, y) # Will be 1.0\n\n\n","dateCreated":"2020-05-07T21:03:33+08:00","dateModified":"2020-05-10T05:32:56+08:00","datePublished":"2020-05-07T21:03:33+08:00","description":"Design Proposal of function in Tensorflow 2.0","headline":"TensorFlow 2.0 Functions, not Sessions.","image":["https://1.bp.blogspot.com/-cLxpo2BA8oI/XZMQrDSXefI/AAAAAAAAnhs/7FS6r94KFNQFzVt_8Ihx1iyTltt7if_xgCLcBGAsYHQ/s1600/TensorFlow%2B2.0%2BLogo.png","https://www.analyticsindiamag.com/wp-content/uploads/2019/03/tfk1.png"],"mainEntityOfPage":{"@type":"WebPage","@id":"https://joddiy.github.io/2020/05/07/TF2-0-Function/"},"publisher":{"@type":"Organization","name":"Joddiy Zhang","sameAs":["https://github.com/joddiy","https://scholar.google.com/citations?user=KH-xv38AAAAJ&hl=en&oi=sra","https://www.linkedin.com/in/joddiyzhang/"],"image":"14108933.jpeg","logo":{"@type":"ImageObject","url":"14108933.jpeg"}},"url":"https://joddiy.github.io/2020/05/07/TF2-0-Function/","keywords":"Tools, Deep Learning, Tensorflow","thumbnailUrl":"https://1.bp.blogspot.com/-cLxpo2BA8oI/XZMQrDSXefI/AAAAAAAAnhs/7FS6r94KFNQFzVt_8Ihx1iyTltt7if_xgCLcBGAsYHQ/s1600/TensorFlow%2B2.0%2BLogo.png"}</script>
    <meta name="description" content="Design Proposal of function in Tensorflow 2.0">
<meta property="og:type" content="blog">
<meta property="og:title" content="TensorFlow 2.0 Functions, not Sessions.">
<meta property="og:url" content="https://joddiy.github.io/2020/05/07/TF2-0-Function/index.html">
<meta property="og:site_name" content="Jz Blog">
<meta property="og:description" content="Design Proposal of function in Tensorflow 2.0">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-05-07T13:03:33.000Z">
<meta property="article:modified_time" content="2020-05-09T21:32:56.000Z">
<meta property="article:author" content="Joddiy Zhang">
<meta property="article:tag" content="Tools">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="Tensorflow">
<meta name="twitter:card" content="summary">
    
    
        
    
    
        <meta property="og:image" content="https://joddiy.github.io/assets/images/14108933.jpeg"/>
    
    
        <meta property="og:image" content="https://1.bp.blogspot.com/-cLxpo2BA8oI/XZMQrDSXefI/AAAAAAAAnhs/7FS6r94KFNQFzVt_8Ihx1iyTltt7if_xgCLcBGAsYHQ/s1600/TensorFlow%2B2.0%2BLogo.png"/>
        <meta class="swiftype" name="image" data-type="enum" content="https://1.bp.blogspot.com/-cLxpo2BA8oI/XZMQrDSXefI/AAAAAAAAnhs/7FS6r94KFNQFzVt_8Ihx1iyTltt7if_xgCLcBGAsYHQ/s1600/TensorFlow%2B2.0%2BLogo.png"/>
    
    
        <meta property="og:image" content="https://www.analyticsindiamag.com/wp-content/uploads/2019/03/tfk1.png"/>
        <meta class="swiftype" name="image" data-type="enum" content="https://www.analyticsindiamag.com/wp-content/uploads/2019/03/tfk1.png"/>
    
    
    <!--STYLES-->
    
<link rel="stylesheet" href="/assets/css/style-x8blglznjjnb9pnnwui5zw4h43ysufmsh1b0omicawm4vhqcutzqavokgpne.min.css">

    <!--STYLES END-->
    

    

    
        
            
<link rel="stylesheet" href="/assets/css/gitalk.css">

        
    
</head>

    <body>
        <div id="blog">
            <!-- Define author's picture -->


    
        
            
        
    

<header id="header" data-behavior="4">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a
            class="header-title-link"
            href="/"
            aria-label=""
        >
            Jz Blog
        </a>
    </div>
    
        
            <a
                class="header-right-picture "
                href="#about"
                aria-label="Open the link: /#about"
            >
        
        
            <img class="header-picture" src="/assets/images/14108933.jpeg" alt="Author&#39;s picture"/>
        
        </a>
    
</header>

            <!-- Define author's picture -->



        
    

<nav id="sidebar" data-behavior="4">
    <div class="sidebar-container">
        
            <div class="sidebar-profile">
                <a
                    href="/#about"
                    aria-label="Read more about the author"
                >
                    <img class="sidebar-profile-picture" src="/assets/images/14108933.jpeg" alt="Author&#39;s picture"/>
                </a>
                <h4 class="sidebar-profile-name">Joddiy Zhang</h4>
                
                    <h5 class="sidebar-profile-bio"><p>author.bio</p>
</h5>
                
            </div>
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/"
                            
                            rel="noopener"
                            title="Home"
                        >
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-categories"
                            
                            rel="noopener"
                            title="Categories"
                        >
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-tags"
                            
                            rel="noopener"
                            title="Tags"
                        >
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Tags</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-archives"
                            
                            rel="noopener"
                            title="Archives"
                        >
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="#about"
                            
                            rel="noopener"
                            title="About"
                        >
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://github.com/joddiy"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="GitHub"
                        >
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://scholar.google.com/citations?user=KH-xv38AAAAJ&hl=en&oi=sra"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="Google Scholar"
                        >
                        <i class="sidebar-button-icon fab fa-google" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Google Scholar</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://www.linkedin.com/in/joddiyzhang/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="LinkedIn"
                        >
                        <i class="sidebar-button-icon fab fa-linkedin" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">LinkedIn</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
        <div class="post-header-cover
                    text-center
                    post-header-cover--partial"
             style="background-image:url('https://www.analyticsindiamag.com/wp-content/uploads/2019/03/tfk1.png');"
             data-behavior="4">
            
                <div class="post-header main-content-wrap text-center">
    
        <h1 class="post-title">
            TensorFlow 2.0 Functions, not Sessions.
        </h1>
    
    
        <div class="post-meta">
    <time datetime="2020-05-07T21:03:33+08:00">
	
		    May 07, 2020
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Study-Notes/">Study Notes</a>


    
</div>

    
</div>

            
        </div>

            <div id="main" data-behavior="4"
                 class="hasCover
                        hasCoverMetaIn
                        ">
                
<article class="post">
    
    
    <div class="post-content markdown">
        <div class="main-content-wrap">
            <h1 id="TensorFlow-2-0-Functions-not-Sessions"><a href="#TensorFlow-2-0-Functions-not-Sessions" class="headerlink" title="TensorFlow 2.0: Functions, not Sessions."></a>TensorFlow 2.0: Functions, not Sessions.</h1><p><a target="_blank" rel="noopener" href="https://github.com/tensorflow/community/blob/49c9f13500ddedddf60144690b9be744a3654b11/rfcs/20180918-functions-not-sessions-20.md">doc link</a></p>
<h2 id="Design-Proposal"><a href="#Design-Proposal" class="headerlink" title="Design Proposal"></a>Design Proposal</h2><h3 id="Basic-idea-Python-functions-as-Graphs"><a href="#Basic-idea-Python-functions-as-Graphs" class="headerlink" title="Basic idea: Python functions as Graphs"></a>Basic idea: Python functions as Graphs</h3><p>Where tf.function is a decorator that “defines a TensorFlow function”. A “TensorFlow function” defines a computation as a graph of TensorFlow operations, with named arguments and explicit return values. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_z1</span>(<span class="params">x, y</span>):</span><br><span class="line">  <span class="keyword">return</span> tf.add(x, y)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_z0</span>(<span class="params">x</span>):</span><br><span class="line">  <span class="keyword">return</span> compute_z1(x, tf.square(x))</span><br><span class="line"></span><br><span class="line">z0 = compute_z0(<span class="number">2.</span>)</span><br><span class="line">z1 = compute_z1(<span class="number">2.</span>, <span class="number">2.</span>)</span><br></pre></td></tr></table></figure>

<p>Having the Python function correspond to what the runtime will execute reduces conceptual complexity in translating between the two domains. </p>
<h3 id="Referencing-state-Variables-tables-etc"><a href="#Referencing-state-Variables-tables-etc" class="headerlink" title="Referencing state: Variables, tables etc."></a>Referencing state: Variables, tables etc.</h3><p>A function decorated Python function encapsulates a graph and its execution. The Python function may reference stateful objects (i.e., state backed by DT_RESOURCE tensors in the runtime, e.g., tf.Variable) by referencing the corresponding Python object, and these will be captured as implicit inputs to the function.</p>
<p>Comparing TensorFlow code today with how we propose it looks in 2.x:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TF 1.x</span></span><br><span class="line">W = tf.Variable(</span><br><span class="line">  tf.glorot_uniform_initializer()(</span><br><span class="line">    (<span class="number">10</span>, <span class="number">10</span>)))</span><br><span class="line">b = tf.Variable(tf.zeros(<span class="number">10</span>))</span><br><span class="line">c = tf.Variable(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32)</span><br><span class="line">ctr = c.assign_add(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies([ctr]):</span><br><span class="line">  y = tf.matmul(x, W) + b</span><br><span class="line">init = </span><br><span class="line">  tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  sess.run(init)</span><br><span class="line">  <span class="built_in">print</span>(sess.run(y,</span><br><span class="line">  feed_dict=&#123;x: make_input_value()&#125;))</span><br><span class="line">  <span class="keyword">assert</span> <span class="built_in">int</span>(sess.run(c)) == <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># TF 2.0</span></span><br><span class="line">W = tf.Variable(</span><br><span class="line">  tf.glorot_uniform_initializer()(</span><br><span class="line">    (<span class="number">10</span>, <span class="number">10</span>)))</span><br><span class="line">b = tf.Variable(tf.zeros(<span class="number">10</span>))</span><br><span class="line">c = tf.Variable(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">  c.assign_add(<span class="number">1</span>)</span><br><span class="line">  <span class="keyword">return</span> tf.matmul(x, W) + b</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(f(make_input_value())</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">int</span>(c) == <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>Worthy of note here - in TensorFlow 1.x, the memory underlying the variables W and b in the runtime lives for the lifetime of the Session - unrelated to the lifetime of the Python objects. In 2.x, the lifetime of the Python objects and the runtime state are tied together.</p>
<h3 id="Control-dependencies"><a href="#Control-dependencies" class="headerlink" title="Control dependencies"></a>Control dependencies</h3><p>In TensorFlow graphs today, control dependencies are sometimes needed to ensure correct evaluation order. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TF 1.x</span></span><br><span class="line">v = tf.Variable(<span class="number">1.0</span>)</span><br><span class="line">init_op = tf.global_variables_initializer()</span><br><span class="line">assign_op = v.assign(<span class="number">2.0</span>)</span><br><span class="line">read = v.read_value()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  val = sess.run(read)</span><br><span class="line">  <span class="built_in">print</span>(val) <span class="comment"># Will print 1.0, the assign is ignored</span></span><br><span class="line">  val = sess.run([read, assign_op])[<span class="number">0</span>]</span><br><span class="line">  <span class="built_in">print</span>(val)   <span class="comment"># Non-deterministically prints 1.0 or 2.0,</span></span><br><span class="line"></span><br><span class="line"> <span class="comment"># TF2.0</span></span><br><span class="line"> v = tf.Variable(<span class="number">1.0</span>)</span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>():</span><br><span class="line">  v.assign(<span class="number">2.0</span>)</span><br><span class="line">  <span class="keyword">return</span> v.read_value()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(f()) <span class="comment"># Always prints 2.0.</span></span><br></pre></td></tr></table></figure>


<p>Note that the intention here is to avoid observable differences from program order. For example:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = tf.Variable(<span class="number">1.0</span>)</span><br><span class="line">b = tf.Variable(<span class="number">1.0</span>)</span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>():</span><br><span class="line">  a.assign(<span class="number">2.0</span>)</span><br><span class="line">  b.assign(<span class="number">3.0</span>)</span><br><span class="line">  <span class="keyword">return</span> a + b</span><br><span class="line"><span class="built_in">print</span>(f())</span><br></pre></td></tr></table></figure>

<p>Will always print 5.0 since the assignments will occur before the read. However, there is no guaranteed ordering between the assignment of a and b (as any difference in that is not observable).</p>
<h3 id="Functions-that-create-state"><a href="#Functions-that-create-state" class="headerlink" title="Functions that create state"></a>Functions that create state</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">v = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">  <span class="keyword">global</span> v</span><br><span class="line">  <span class="keyword">if</span> v <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    v = tf.Variable(<span class="number">1.0</span>)</span><br><span class="line">  <span class="keyword">return</span> tf.cast(x, tf.float32) + v</span><br><span class="line"></span><br><span class="line">f(tf.constant(<span class="number">1</span>, dtype=tf.float32)) <span class="comment"># Creates the variable, returns 2.0</span></span><br><span class="line">f(tf.constant(<span class="number">2</span>, dtype=tf.int32))   <span class="comment"># Reuses the variable, returns 3.0</span></span><br></pre></td></tr></table></figure>

<ol>
<li>State (like tf.Variable objects) are only created the first time the function f is called. If any variables are created in the first execution of f, then @tf.function will trace f again the second time it is invoked in order to record the behavior that will be used from then on. </li>
<li>The caller must make sure that any variable referenced by the function still exists whenever the function is evaluated. @tf.function itself will keep only weak references to these created variables.</li>
</ol>
<h3 id="Trace-Caches"><a href="#Trace-Caches" class="headerlink" title="Trace Caches"></a>Trace Caches</h3><p>Since new graphs are traced when new input signatures are encountered, a function can encapsulate multiple graphs. For example, considering the following, there are two graphs created here:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">  <span class="keyword">return</span> tf.square(x)</span><br><span class="line"></span><br><span class="line">f(tf.constant(<span class="number">1</span>, dtype=tf.int32))</span><br><span class="line">f(tf.constant(<span class="number">1.0</span>, dtype=tf.float32))</span><br></pre></td></tr></table></figure>

<p>Note the use of tf.constant to ensure that the argument is a Tensor. If the argument were a Python value, then additional graphs will be traced for each such value. For example, the following two calls will result in two additional graphs being traced:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">f(<span class="number">1.0</span>)</span><br><span class="line">f(<span class="number">2.0</span>)</span><br></pre></td></tr></table></figure>

<p>Where arguments are not Tensors, the “value” of the argument is used to compute the trace_cache_key. For example:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x, use_multiply</span>):</span><br><span class="line">  <span class="keyword">return</span> tf.multiply(x, x) <span class="keyword">if</span> use_multiply <span class="keyword">else</span> tf.square(x)</span><br><span class="line"></span><br><span class="line">f(tf.constant(<span class="number">2.0</span>), <span class="literal">True</span>)</span><br><span class="line">f(tf.constant(<span class="number">2.0</span>), <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>will result in 2 graphs being created, since the two calls result in two different cache keys because the value of the Python object (the second argument) changes between the two.</p>
<p>Note that the “type” of Tensor inputs to the function also incorporates the shape. For example:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>): </span><br><span class="line">	<span class="keyword">return</span> tf.add(x, <span class="number">1.</span>)</span><br><span class="line"></span><br><span class="line">f(tf.constant([<span class="number">2.0</span>]))</span><br><span class="line">f(tf.constant([<span class="number">2.0</span>, <span class="number">3.0</span>]))</span><br><span class="line">f(tf.constant([[<span class="number">2.0</span>]]))</span><br><span class="line">f(tf.constant([<span class="number">3.0</span>]))</span><br><span class="line">f(tf.constant([<span class="number">4.0</span>, <span class="number">5.0</span>]))</span><br></pre></td></tr></table></figure>

<p>will result in 3 graphs being created.</p>
<p>The trace_cache_key also incorporates the “context” in which the call was made. For example:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>): <span class="keyword">return</span> tf.add(x, <span class="number">1.</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">&quot;/device:CPU:0&quot;</span>):</span><br><span class="line">  f(tf.constant(<span class="number">2.0</span>))</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">&quot;/device:GPU:0&quot;</span>):</span><br><span class="line">  f(tf.constant(<span class="number">2.0</span>))</span><br></pre></td></tr></table></figure>

<p>Will create 2 graphs.</p>
<p><strong>CAUTION: Too many traces</strong></p>
<p><strong>CAUTION: Mutable non-Tensor arguments</strong></p>
<p>The trace_cache_key includes the Python object for non-Tensor arguments. Mutations of these arguments might not be detected. For example:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># non-Tensor object</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Params</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">  multiply = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">p = Params()</span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x, y</span>):</span><br><span class="line">  <span class="keyword">return</span> tf.multiply(x, <span class="number">2.</span>) <span class="keyword">if</span> y.multiply <span class="keyword">else</span> tf.add(x, <span class="number">2.</span>)</span><br><span class="line"></span><br><span class="line">f(<span class="number">3.</span>, p) <span class="comment"># Returns 6.0</span></span><br><span class="line">p.multiply = <span class="literal">False</span></span><br><span class="line">f(<span class="number">3.</span>, p)  <span class="comment"># Mutations to `p` may not trigger a retrace, so might still return 6.0</span></span><br></pre></td></tr></table></figure>

<h3 id="Input-Signatures"><a href="#Input-Signatures" class="headerlink" title="Input Signatures"></a>Input Signatures</h3><p>An “input signature” can be explicitly specified to control the trace_cache_key computation based on the type and shape of Tensor (and list of Tensor) arguments to f.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function(<span class="params">input_signature=(<span class="params">(<span class="params">tf.float32, [<span class="literal">None</span>]</span>)</span>)</span></span></span><br><span class="line"><span class="params"><span class="meta"><span class="keyword">def</span> f(<span class="params">x</span>): </span></span></span><br><span class="line"><span class="params"><span class="meta">	<span class="keyword">return</span> tf.add(<span class="params">x, <span class="number">1.</span></span>)</span></span></span><br><span class="line"><span class="params"><span class="meta"></span></span></span><br><span class="line"><span class="params"><span class="meta">f(<span class="params">tf.constant(<span class="params">[<span class="number">2.0</span>]</span>)</span>)      <span class="comment"># Returns [3.0]</span></span></span></span><br><span class="line"><span class="params"><span class="meta">f(<span class="params">tf.constant(<span class="params">[<span class="number">2.0</span>, <span class="number">3.0</span>]</span>)</span>) <span class="comment"># Matches the input signature as [None]</span></span></span></span><br><span class="line"><span class="params"><span class="meta">                           <span class="comment"># matches the actual shape [2]</span></span></span></span><br><span class="line"><span class="params"><span class="meta">f(<span class="params">tf.constant(<span class="params">[[<span class="number">2.0</span>]]</span>)</span>)    <span class="comment"># Raises an error as the arguments don&#x27;t match the</span></span></span></span><br><span class="line"><span class="params"><span class="meta">                           <span class="comment"># input signature.</span></span></span></span><br><span class="line"><span class="params"><span class="meta">f(<span class="params">tf.constant(<span class="params">[<span class="number">2</span>], dtype=tf.int32</span>)</span>) <span class="comment"># Raises an error as the dtype of the argument</span></span></span></span><br><span class="line"><span class="params"><span class="meta">                                    <span class="comment"># does not match the input signature</span></span></span></span><br><span class="line"><span class="params"><span class="meta"></span></span></span><br><span class="line"><span class="params"><span class="meta"><span class="comment"># f is backed by a single Graph since the input signature specification allowed</span></span></span></span><br><span class="line"><span class="params"><span class="meta"><span class="comment"># for the same graph to be used when the input shape is (1,) or (2,).</span></span></span></span><br></pre></td></tr></table></figure>

<ol>
<li>For a Tensor argument, it specifies a (dtype, shape pattern).<ul>
<li>(tf.float32, [None]) means the argument must be a float32 vector (with any number of elements).</li>
<li>(tf.int32, []) means that the argument must be an int32 scalar.</li>
</ul>
</li>
<li>For a list of Tensor objects, it specifies an optional list length and the signature for elements in the list (i.e., the dtype and shape pattern for all elements in the list).</li>
<li>For non-Tensor arguments: tf.PYTHON_VALUE</li>
</ol>
<p>You can use the tf.TRACE_ON_NEW_VALUE to release the restriction of dtype:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function(<span class="params">input_signature=(<span class="params">(<span class="params">tf.TRACE_ON_NEW_VALUE, [<span class="literal">None</span>]</span>)</span>)</span></span></span><br><span class="line"><span class="params"><span class="meta"><span class="keyword">def</span> f(<span class="params">x</span>): </span></span></span><br><span class="line"><span class="params"><span class="meta">	<span class="keyword">return</span> tf.square(<span class="params">x</span>)</span></span></span><br><span class="line"><span class="params"><span class="meta"></span></span></span><br><span class="line"><span class="params"><span class="meta">f(<span class="params">tf.constant(<span class="params">[<span class="number">2.0</span>]</span>)</span>) <span class="comment"># Returns 4.0</span></span></span></span><br><span class="line"><span class="params"><span class="meta">f(<span class="params">tf.constant(<span class="params">[<span class="number">2</span>, <span class="number">2</span>], dtype=tf.int32</span>) <span class="comment"># Returns [4, 4] after tracing a new graph</span></span></span></span></span><br></pre></td></tr></table></figure>

<h3 id="Classes"><a href="#Classes" class="headerlink" title="Classes"></a>Classes</h3><p>If a member function of a class does not create variables, it may be decorated with @tf.function and it will work:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AnyShapeModel</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">    self.v = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="meta">  @tf.function</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">increment</span>(<span class="params">self, amount</span>):</span><br><span class="line">    <span class="keyword">if</span> self.v <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      self.v = tf.Variable(tf.zeros_like(amount))</span><br><span class="line">    self.v.assign_add(amount)</span><br><span class="line"></span><br><span class="line">model1 = AnyShapeModel()</span><br><span class="line">model1.increment(tf.constant(<span class="number">3</span>))</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">int</span>(model1.v) == <span class="number">3</span></span><br><span class="line">model1.increment(tf.constant(<span class="number">4</span>))</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">int</span>(model1.v) == <span class="number">7</span></span><br><span class="line">model2 = AnyShapeModel()</span><br><span class="line">model2.increment(tf.constant([<span class="number">4</span>, <span class="number">5</span>]))</span><br><span class="line"><span class="keyword">assert</span> model2.v.numpy() == [<span class="number">4</span>, <span class="number">5</span>]</span><br></pre></td></tr></table></figure>

<p>The semantics here are that each new instance is allowed to create variables in each @tf.function once.</p>
<h3 id="function-ing-Python-control-flow"><a href="#function-ing-Python-control-flow" class="headerlink" title="function-ing Python control flow"></a>function-ing Python control flow</h3><p>If the function has <strong>data-dependent control flow</strong> then though the function will execute fine with eager execution enabled, function decorating it will fail. For example:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x, y</span>):</span><br><span class="line">  <span class="keyword">if</span> tf.equal(y, <span class="number">0.0</span>):</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line">  <span class="keyword">return</span> x / y</span><br><span class="line"></span><br><span class="line">x = tf.constant(<span class="number">2.0</span>)</span><br><span class="line">y = tf.constant(<span class="number">2.0</span>)</span><br><span class="line"></span><br><span class="line">f(x, y) <span class="comment"># Will be 1.0</span></span><br><span class="line"></span><br><span class="line">df = tf.function(f)</span><br><span class="line">df(x, y)  <span class="comment"># Will raise an error complaining about the data-dependent control flow</span></span><br></pre></td></tr></table></figure>

<p>To fix this, one would have to use the graph construction APIs for control flow (tf.cond, tf.while_loop):</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x, y</span>):</span><br><span class="line">  <span class="keyword">return</span> tf.cond(tf.equal(y, <span class="number">0.0</span>), <span class="keyword">lambda</span>: y, <span class="keyword">lambda</span>: x/y)</span><br><span class="line"></span><br><span class="line">x = tf.constant(<span class="number">2.0</span>)</span><br><span class="line">y = tf.constant(<span class="number">2.0</span>)</span><br><span class="line"></span><br><span class="line">f(x, y) <span class="comment"># Will be 1.0</span></span><br><span class="line"></span><br><span class="line">df = tf.function(f)</span><br><span class="line">df(x, y)  <span class="comment"># Will be 1.0</span></span><br></pre></td></tr></table></figure>

<p>This situation can be improved with the help of autograph to allow expression of control flow in Python. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = tf.function(autograph=<span class="literal">True</span>)(f)</span><br><span class="line">f(x, y) <span class="comment"># Will be 1.0</span></span><br></pre></td></tr></table></figure>



            


        </div>
    </div>
    <div id="post-footer" class="post-footer main-content-wrap">
        
            <div class="post-footer-tags">
                <span class="text-color-light text-small">TAGGED IN</span><br/>
                
    <a class="tag tag--primary tag--small t-none-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a> <a class="tag tag--primary tag--small t-none-link" href="/tags/Tensorflow/" rel="tag">Tensorflow</a> <a class="tag tag--primary tag--small t-none-link" href="/tags/Tools/" rel="tag">Tools</a>

            </div>
        
        
            <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2020/05/09/TF2-0-Accelerators/"
                    data-tooltip="Distributed training with TensorFlow"
                    aria-label="PREVIOUS: Distributed training with TensorFlow"
                >
                    
                        <i class="fa fa-angle-left" aria-hidden="true"></i>
                        <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                    </a>
            </li>
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2020/05/01/Packed-Neural-Network-Training/"
                    data-tooltip="Understanding and Optimizing Packed Neural Network Training for Hyper-Parameter Tuning"
                    aria-label="NEXT: Understanding and Optimizing Packed Neural Network Training for Hyper-Parameter Tuning"
                >
                    
                        <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                        <i class="fa fa-angle-right" aria-hidden="true"></i>
                    </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a
                class="post-action-btn btn btn--default btn-open-shareoptions"
                href="#btn-open-shareoptions"
                aria-label="Share this post"
            >
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://joddiy.github.io/2020/05/07/TF2-0-Function/"
                    title="Share on Facebook"
                    aria-label="Share on Facebook"
                >
                    <i class="fab fa-facebook" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://twitter.com/intent/tweet?text=https://joddiy.github.io/2020/05/07/TF2-0-Function/"
                    title="Share on Twitter"
                    aria-label="Share on Twitter"
                >
                    <i class="fab fa-twitter" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://plus.google.com/share?url=https://joddiy.github.io/2020/05/07/TF2-0-Function/"
                    title="Share on Google+"
                    aria-label="Share on Google+"
                >
                    <i class="fab fa-google-plus" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
                <li class="post-action">
                    <a
                        class="post-action-btn btn btn--default"
                        href="#gitalk"
                        aria-label="Leave a comment"
                    >
                        <i class="fa fa-comment"></i>
                    </a>
                </li>
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


        
        
            
                <div id="gitalk"></div>

            
        
    </div>
</article>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2024 Joddiy Zhang. All Rights Reserved.
    </span>
</footer>

            </div>
            
                <div id="bottom-bar" class="post-bottom-bar" data-behavior="4">
                    <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2020/05/09/TF2-0-Accelerators/"
                    data-tooltip="Distributed training with TensorFlow"
                    aria-label="PREVIOUS: Distributed training with TensorFlow"
                >
                    
                        <i class="fa fa-angle-left" aria-hidden="true"></i>
                        <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                    </a>
            </li>
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2020/05/01/Packed-Neural-Network-Training/"
                    data-tooltip="Understanding and Optimizing Packed Neural Network Training for Hyper-Parameter Tuning"
                    aria-label="NEXT: Understanding and Optimizing Packed Neural Network Training for Hyper-Parameter Tuning"
                >
                    
                        <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                        <i class="fa fa-angle-right" aria-hidden="true"></i>
                    </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a
                class="post-action-btn btn btn--default btn-open-shareoptions"
                href="#btn-open-shareoptions"
                aria-label="Share this post"
            >
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://joddiy.github.io/2020/05/07/TF2-0-Function/"
                    title="Share on Facebook"
                    aria-label="Share on Facebook"
                >
                    <i class="fab fa-facebook" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://twitter.com/intent/tweet?text=https://joddiy.github.io/2020/05/07/TF2-0-Function/"
                    title="Share on Twitter"
                    aria-label="Share on Twitter"
                >
                    <i class="fab fa-twitter" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://plus.google.com/share?url=https://joddiy.github.io/2020/05/07/TF2-0-Function/"
                    title="Share on Google+"
                    aria-label="Share on Google+"
                >
                    <i class="fab fa-google-plus" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
                <li class="post-action">
                    <a
                        class="post-action-btn btn btn--default"
                        href="#gitalk"
                        aria-label="Leave a comment"
                    >
                        <i class="fa fa-comment"></i>
                    </a>
                </li>
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


                </div>
                
    <div id="share-options-bar" class="share-options-bar" data-behavior="4">
        <i id="btn-close-shareoptions" class="fa fa-times"></i>
        <ul class="share-options">
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://www.facebook.com/sharer/sharer.php?u=https://joddiy.github.io/2020/05/07/TF2-0-Function/"
                        aria-label="Share on Facebook"
                    >
                        <i class="fab fa-facebook" aria-hidden="true"></i><span>Share on Facebook</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://twitter.com/intent/tweet?text=https://joddiy.github.io/2020/05/07/TF2-0-Function/"
                        aria-label="Share on Twitter"
                    >
                        <i class="fab fa-twitter" aria-hidden="true"></i><span>Share on Twitter</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://plus.google.com/share?url=https://joddiy.github.io/2020/05/07/TF2-0-Function/"
                        aria-label="Share on Google+"
                    >
                        <i class="fab fa-google-plus" aria-hidden="true"></i><span>Share on Google+</span>
                    </a>
                </li>
            
        </ul>
    </div>


            
        </div>
        


    
        
    

<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-times"></i>
        </div>
        
            <img id="about-card-picture" src="/assets/images/14108933.jpeg" alt="Author&#39;s picture"/>
        
            <h4 id="about-card-name">Joddiy Zhang</h4>
        
            <div id="about-card-bio"><p>author.bio</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>author.job</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker-alt"></i>
                <br/>
                Singapore
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->

<script src="/assets/js/script-sqrh47zm5nkjgifq4rx38uvns4r2rarrrvwuhjxiztyrddruca5ukl7nw6br.min.js"></script>

<!--SCRIPTS END-->


    
        
<script src="/assets/js/gitalk.js"></script>

        <script type="text/javascript">
          (function() {
            new Gitalk({
              clientID: 'bee9685b2dc9739b6bd5',
              clientSecret: 'a0c683f383a94fae6d021ab932f37f7e56899410',
              repo: 'joddiy.github.io',
              owner: 'joddiy',
              admin: ['joddiy'],
              id: '2020/05/07/TF2-0-Function/',
              ...{"language":"en","perPage":10,"distractionFreeMode":false,"enableHotKey":true,"pagerDirection":"first"}
            }).render('gitalk')
          })()
        </script>
    




    </body>
</html>
