
<!DOCTYPE html>
<html lang="en">
    
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Jz Blog">
    <title>NLP Summary - Jz Blog</title>
    <meta name="author" content="Joddiy Zhang">
    
    
        <link rel="icon" href="https://joddiy.github.io/assets/images/favicon.ico">
    
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Joddiy Zhang","sameAs":["https://github.com/joddiy","https://scholar.google.com/citations?user=KH-xv38AAAAJ&hl=en&oi=sra","https://www.linkedin.com/in/joddiyzhang/"],"image":"14108933.jpeg"},"articleBody":"ELMoIntroductionOur word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. \nlearning high quality representations can be challenging. They should ideally model both: \n\ncomplex characteristics of word use (e.g., syntax and semantics), and\nhow these uses vary across linguistic contexts (i.e., to model polysemy).\n\nELMo (Embeddings from Language Models). ELMo representations are deep, in the sense that they are a function of all of the internal layers of the biLM.\nUsing intrinsic evaluations, we show that the higher-level LSTM states capture context-dependent aspects of word meaning (e.g., they can be used without modification to perform well on supervised word sense disambiguation tasks) while lowerlevel states model aspects of syntax (e.g., they can be used to do part-of-speech tagging). \nRelated workDue to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors are a standard component of most state-of-the-art NLP architectures. However, these approaches for learning word vectors only allow a single contextindependent representation for each word.\nPreviously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information or learning separate vectors for each word sense. Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes.\nOther recent work has also focused on learning context-dependent representations. context2vec uses a bidirectional LSTM to encode the context around a pivot word. Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation (MT) system or an unsupervised language model.\n\nwhat?\n\nPrevious work has also shown that different layers of deep biRNNs encode different types of information. \nELMo: Embeddings from Language ModelsELMo word representations are functions of the entire input sentence. They are computed on top of two-layer biLMs with character convolutions, as a linear function of the internal network states. This setup allows us to do semi-supervised learning, where the biLM is pretrained at a large scale and easily incorporated into a wide range of existing neural NLP architectures.\nBidirectional language modelsAt each position k, each LSTM layer outputs a context-dependent representation h(LM, k, j) where j &#x3D; 1,…,L. The top layer LSTM output, h(LM, k, L) , is used to predict the next token tk+1 with a Softmax layer.\nA backward LM can be implemented in an analogous way to a forward LM, with each backward LSTM layer j in a L layer deep model producing representations h(LM, k, j) of tk given (tk+1,…,tN).\nA biLM combines both a forward and backward LM. Our formulation jointly maximizes the log likelihood of the forward and backward directions:\n\nWe tie the parameters for both the token representation (Θx) and Softmax layer (Θs) in the forward and backward direction while maintaining separate parameters for the LSTMs in each direction.\nELMoELMo is a task specific combination of the intermediate layer representations in the biLM. For each token tk, a L-layer biLM computes a set of 2L + 1 representations.\n\nh(LM, k, j) is the token layer.\nELMo collapses all layers in R into a single vector, ELMok &#x3D; E (Rk; Θe). More generally, we compute a task specific weighting of all biLM layers:\nstask are softmax-normalized weights and the scalar parameter γtask allows the task model to scale the entire ELMo vector. γ is of practical importance to aid the optimization process.\nConsidering that the activations of each biLM layer have a different distribution, in some cases it also helped to apply layer normalization (Ba et al., 2016) to each biLM layer before weighting.\nUsing biLMs for supervised NLP tasksTo add ELMo to the supervised model, we first freeze the weights of the biLM and then concatenate the ELMo vector (ELMo, task, k) with xk and pass the ELMo enhanced representation [x; (ELMo, task, k)] into the task RNN.\nFinally, we found it beneficial to add a moderate amount of dropout to ELMo and in some cases to regularize the ELMo weights by adding λ∥w∥2 to the loss. This imposes an inductive bias on the ELMo weights to stay close to an average of all biLM layers.\nPre-trained bidirectional language model architectureFor a purely character-based input representation, we halved all embedding and hidden dimensions from the single best model CNN-BIG-LSTM in Jozefowicz et al. (2016). The final model uses L &#x3D; 2 biLSTM layers with 4096 units and 512 dimension projections and a residual connection from the first to second layer. The context insensitive type representation uses 2048 character n-gram convolutional filters followed by two highway layers (Srivastava et al., 2015) and a linear projection down to a 512 representation.\nThe context insensitive type representation uses 2048 character n-gram convolutional filters followed by two highway layers and a linear projection down to a 512 representation. \nAs a result, the biLM provides three layers of representations for each input token, including those outside the training set due to the purely character input.\nOnce pretrained, the biLM can compute representations for any task. \nULMFiTIntroductionWe propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model.\ntransductive transfer vs. inductive transfer\n\nwhat?\n\nLanguage models (LM) overfit to small datasets and suffered catastrophic forgetting when fine-tuned with a classifier. Compared to CV, NLP models are typically more shallow and thus require different fine-tuning methods.\nWe propose a new method, Universal Language Model Fine-tuning (ULMFiT) that addresses these issues and enables robust inductive transfer learning for any NLP task.\nThe same 3-layer LSTM architecture— with the same hyperparameters and no additions other than tuned dropout hyperparameters— outperforms highly engineered models and trans-fer learning approaches on six widely studied text classification tasks.\nOur contributions are the following: \n\nWe propose Universal Language Model Fine-tuning (ULMFiT), a method that can be used to achieve CV-like transfer learning for any task for NLP. \nWe propose discriminative fine-tuning, slanted triangular learning rates, and gradual unfreezing, novel techniques to retain previous knowledge and avoid catastrophic forgetting during fine-tuning. \nWe significantly outperform the state-of-the-art on six representative text classification datasets, with an error reduction of 18-24% on the majority of datasets. \nWe show that our method enables extremely sample-efficient transfer learning and perform an extensive ablation analysis.\n\nRelated workHypercolumnsThe prevailing approach is to pretrain embeddings that capture additional context via other tasks. Embeddings at different levels are then used as features, concatenated either with the word embeddings or with the inputs at intermediate layers. This method is known as hypercolumns.\nUniversal Language Model Fine-tuningWe propose Universal Language Model Fine-tuning (ULMFiT), which pretrains a language model (LM) on a large general-domain corpus and fine-tunes it on the target task using novel techniques. The method is universal in the sense that it meets these practical criteria: \n\nIt works across tasks varying in document size, number, and label type; \nit uses a single architecture and training process; \nit requires no custom feature engineering or preprocessing; and \nit does not require additional in-domain documents or labels.\n\nULMFiT consists of the following steps, which we show in Figure 1: a) General-domain LM pretraining; b) target task LM fine-tuning; and c) target task classifier fine-tuning.\n\nGeneral-domain LM pretrainingWe pretrain the language model on Wikitext-103 (Merity et al., 2017b) consisting of 28,595 preprocessed Wikipedia articles and 103 million words.\nTarget task LM fine-tuningWe thus fine-tune the LM on data of the target task. Given a pretrained general-domain LM, this stage converges faster as it only needs to adapt to the idiosyncrasies of the target data, and it allows us to train a robust LM even for small datasets. We propose discriminative fine-tuning and slanted triangular learning rates.\nDiscriminative fine-tuningFor discriminative fine-tuning, we split the parameters θ into {θ1, . . . , θL} where θl contains the parameters of the model at the l-th layer and L is the number of layers of the model. Similarly, we obtain {η1,…,ηL} where ηl is the learning rate of the l-th layer.\nThe SGD update with discriminative fine-tuning is then the following:\n\nWe empirically found it to work well to first choose the learning rate ηL of the last layer by fine-tuning only the last layer and using ηl−1 &#x3D; ηl&#x2F;2.6 as the learning rate for lower layers.\nSlanted triangular learning rateswe propose slanted triangular learning rates (STLR), which first linearly increases the learning rate and then linearly decays it according to the following update schedule, which can be seen in:\n\n\nwhere T is the number of training iterations, cut_frac is the fraction of iterations we increase the LR, cut is the iteration when we switch from increasing to decreasing the LR, p is the fraction of the number of iterations we have increased or will decrease the LR respectively, ratio specifies how much smaller the lowest LR is from the maximum LR ηmax, and ηt is the learning rate at iteration t.\nWe generally use cut_frac &#x3D; 0.1, ratio &#x3D; 32 and ηmax &#x3D; 0.01.\nTarget task classifier fine-tuningFinally, for fine-tuning the classifier, we augment the pretrained language model with two additional linear blocks.\nConcat poolingAs input documents can consist of hundreds of words, information may get lost if we only consider the last hidden state of the model. For this reason, we concatenate the hidden state at the last time step hT of the document with both the max-pooled and the mean-pooled representation of the hidden states over as many time steps as fit in GPU memory H &#x3D; {h1,…,hT}:\n\nGradual unfreezingWe first unfreeze the last layer and fine-tune all unfrozen layers for one epoch. We then unfreeze the next lower frozen layer and repeat, until we finetune all layers until convergence at the last iteration.\nBPTT for Text Classification (BPT3C)In order to make fine-tuning a classifier for large documents feasible, we propose BPTT for Text Classification (BPT3C): We divide the document into fixedlength batches of size b. At the beginning of each batch, the model is initialized with the final state of the previous batch. \nBidirectional language modelFor all our experiments, we pretrain both a forward and a backward LM. \nTransformerIntroductionRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t.\nThis inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.\nBackgroundThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.\nIn these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions.\nIn the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.\nTo the best of our knowledge, however, the Transformer is the first transduction model relyingentirely on self-attention to compute representations of its input and output without using sequencealignedRNNs or convolution.\nModel ArchitectureThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure:\n\nEncoder and Decoder StacksEncoder:\nThe encoder is composed of a stack of N &#x3D; 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. \nWe employ a residual connection around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. \nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel &#x3D; 512.\nDecoder:\nThe decoder is also composed of a stack of N &#x3D; 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.\nSimilar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. \nWe also modify the self-attention sub-layer(Masked Multi-Head Attention) in the decoder stack to ensure that the predictions for position i can depend only on the known outputs at positions less than i.\nAttentionAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. \n\nAttention(Q, K, V) &#x3D; softmax(Q * K.T &#x2F; √dk) * V\n\n\nScaled Dot-Product Attention:\nThe input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by pdk, and apply a softmax function to obtain the weights on the values.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:\n\n\nX ∈ (len, dmodel), WQ ∈ (dmodel, dk), WK ∈ (dmodel, dk), WV ∈ (dmodel, dv)Q &#x3D; X * WQ ∈ (len, dk)K &#x3D; X * WK ∈ (len, dk)V &#x3D; X * WV ∈ (len, dv)Q * K.T ∈ (len, len)softmax(Q * K.T &#x2F; √dk) ∈ (len, len)softmax(Q * K.T &#x2F; √dk) * V ∈ (len, dv)\n\n\ntwo types of attention: additive attention and dot-product attention?\n\nWhy divide √dk?\n\nAnd the author suspect for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients\nq · k &#x3D; sum(qi * ki) for i from 1 to dk, has mean 0 and variance dk.\n\nMulti-Head Attention:\nWe found it beneficial to linearly project the queries, keys and values h times with different, learnedlinear projections to dk, dk and dv dimensions, respectively.\nOn each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values(len*dv). These are concatenated and once again projected, resulting in the final values, as depicted in Figure:\n\n\nThe Q, K, V should be the output of previous layer, the shape is (len, dmodel)headi ∈ (len, dv), WO ∈ (h * dv, dmodel)MultiHead(Q, K, V) ∈ (len, dmodel)the author uses h &#x3D; 8, dk &#x3D; dv &#x3D; dmodel&#x2F;h &#x3D; 64, dmodel &#x3D; 512\n\n\nIn implemention, we combine the h head in one linear layer as:X ∈ (len, dmodel), WQ ∈ (dmodel, h * dk), WK ∈ (dmodel, h * dk), WV ∈ (dmodel, h * dv)h * dk &#x3D; dmodelQ &#x3D; X * WQ ∈ (len, h * dk)K &#x3D; X * WK ∈ (len, h * dk)V &#x3D; X * WV ∈ (len, h * dv)Q * K.T ∈ (len, len)softmax(Q * K.T &#x2F; √dk) ∈ (len, len)softmax(Q * K.T &#x2F; √dk) * V ∈ (len, h * dv)\n\nApplications of Attention in our Model:\n\nIn a self-attention in encoder, the Q, K, V comes from previous layer’s output\nIn the attention in decoder, the Q comes from the previous decoder, and K, V comes from encoder.\nIn decoder, when we predict word i, we mask out (setting to −∞) all work i+1 to len in K, V from encoder.\n\nPosition-wise Feed-Forward Networks\n\nX ∈ (len, dmodel), W1 ∈ (dmodel, dff), W1 ∈ (dff, dmodel), b ∈ (len, dmodel)X * W1 ∈ (len, dff)FFN(x) ∈ (len, dmodel)dmodel &#x3D; 512, dff &#x3D; 2048\n\nEmbeddings and SoftmaxIn our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation(convert the decoder output to predicted next-token probabilities). In the embedding layers, we multiply those weights by √dmodel.\n\nWHY?\n\nPositional EncodingIn order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.\nWe add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks. In this work, we use sine and cosine functions of different frequencies:\n\nwhere pos is the position and i is the dimension, and any fixed offset k, PE(pos+k) can be represented as a linear function of PE(pos).\nWhy Self-AttentionWe compare various aspects of self-attention layers to the recurrent and convolutional layers.\n\nOne is the total computational complexity per layer.\nIn terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d.\nTo improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n&#x2F;r).\nA single convolutional layer with kernel width k &lt; n does not connect all pairs of input and output positions. Doing so requires a stack of O(n&#x2F;k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions. Convolutional layers are generally more expensive than recurrent layers, by a factor of k.\nAnother is the amount of computation that can be parallelized.\nThe third is the path length between long-range dependencies in the network.\nLearning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies.\nGPTLabeled data for learning these specific tasks is scarce, so it is challenging for discriminatively trained models to perform adequately.\nWe demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task.\nIntroductionModels that can leverage linguistic information from unlabeled data provide a valuable alternative to gathering more annotation, which can be time-consuming and expensive. Further, even in cases where considerable supervision is available, learning good representations in an unsupervised fashion can provide a significant performance boost. \nLeveraging more than word-level information from unlabeled text, however, is challenging for two main reasons. \n\nIt is unclear what type of optimization objectives are most effective at learning text representations that are useful for transfer. \nThere is no consensus on the most effective way to transfer these learned representations to the target task.\n\nIn this paper, we explore a semi-supervised approach for language understanding tasks using a combination of unsupervised pre-training and supervised fine-tuning. \nOur goal is to learn a universal representation that transfers with little adaptation to a wide range of tasks.\nWe employ a two-stage training procedure:\n\nFirst, we use a language modeling objective on the unlabeled data to learn the initial parameters of a neural network model. \nSubsequently, we adapt these parameters to a target task using the corresponding supervised objective.\n\nFor our model architecture, we use the Transformer.\nDuring transfer, we utilize task-specific input adaptations derived from traversal-style approaches, which process structured text input as a single contiguous sequence of tokens.\nRelated WorkSemi-supervised learning for NLP\nThe earliest approaches used unlabeled data to compute word-level or phrase-level statistics. Over the last few years, researchers have demonstrated the benefits of using word embeddings.\nThese approaches, however, mainly transfer word-level information, whereas we aim to capture higher-level semantics.\nUnsupervised pre-training\nResearch demonstrated that pre-training acts as a regularization scheme, enabling better generalization in deep neural networks.\nThe usage of LSTM models in other approaches restricts their prediction ability to a short range. Our choice of transformer networks allows us to capture longerrange linguistic structure.\nOther approaches use hidden representations from a pre-trained model involving a substantial amount of new parameters for each separate target task, whereas we require minimal changes to our model architecture during transfer.\nAuxiliary training objectives\nPOS tagging, chunking, named entity recognition, and language modeling.\nFrameworkOur training procedure consists of two stages. The first stage is learning a high-capacity languagemodel on a large corpus of text. This is followed by a fine-tuning stage, where we adapt the model toa discriminative task with labeled data.\nUnsupervised pre-trainingWe use a standard language modeling objective to maximize the following likelihood:\n\nwhere k is the size of the context window, and the conditional probability P is modeled using a neural network with parameters Θ. These parameters are trained using stochastic gradient descent.\nIn our experiments, we use a multi-layer Transformer decoder or the language model, which is a variant of the transformer. This model applies a multi-headed self-attention operation over the input context tokens followed by position-wise feedforward layers to produce an output distribution over target tokens:\n\nwhere U &#x3D; (u−k, … , u−1) is the context vector of tokens, n is the number of layers, We is the tokenembedding matrix, and Wp is the position embedding matrix.\nSupervised fine-tuningWe assume a labeled dataset C, where each instance consists of a sequence of input tokens, x1, … , xm, along with a label y.\nThe inputs are passed through our pre-trained model to obtain the final transformer block’s activation h(m,l) , which is then fed into an added linear output layer with parameters Wy to predict y:\n\nThis gives us the following objective to maximize:\n\nSpecifically, we optimize the following objective (with weight λ):\n\nOverall, the only extra parameters we require during fine-tuning are Wy , and embeddings for delimiter tokens.\nTask-specific input transformationsCertain other tasks, like question answering or textual entailment, have structured inputs such as ordered sentence pairs, or triplets of document, question, and answers.\nSince our pre-trained model was trained on contiguous sequences of text, we require some modifications to apply it to these tasks.\nwe use a traversal-style approach [52], where we convert structured inputs into an ordered sequence that our pre-trained model can process. These input transformations allow us to avoid making extensive changes to the architecture across tasks. \n\nAll transformations include adding randomly initialized start and end tokens (⟨s⟩, ⟨e⟩).\nTextual entailment\nSimilarity\nQuestion Answering and Commonsense Reasoning\nBertBERT, which stands for Bidirectional Encoder Representations from Transformers.\nBERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.\nIntroductionLanguage model pre-training has been shown to be effective for improving many natural language processing tasks.\nThere are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.\n\nThe feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.\nThe fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.\nWe argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.\nThe major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-to-right architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.\nIn this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a “masked language model” (MLM) pre-training objective.\nThe masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.\nUnlike left-to-right language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer.\nIn addition to the masked language model, we also use a “next sentence prediction” task that jointly pretrains text-pair representations. \nRelated WorkUnsupervised Feature-based ApproachesPre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch. \nELMo and its predecessor generalize traditional word embedding research along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model.\nUnsupervised Fine-tuning ApproachesMore recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task.\nThe advantage of these approaches is that few parameters need to be learned from scratch.\nAt least partly due to this advantage, OpenAI GPT (Radford et al., 2018) achieved previously state-of-the-art results on many sentence-level tasks from the GLUE benchmark.\nTransfer Learning from Supervised DataBERTThere are two steps in our framework: pre-training and fine-tuning.\nDuring pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks.\nA distinctive feature of BERT is its unified architecture across different tasks. There is minimal difference between the pre-trained architecture and the final downstream architecture.\nModel Architecture\n\nBERT’s model architecture is a multi-layer bidirectional Transformer encoder.\nIn this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.\nWe primarily report results on two model sizes: BERT(BASE) (L&#x3D;12, H&#x3D;768, A&#x3D;12, Total Parameters&#x3D;110M) and BERT(LARGE) (L&#x3D;24, H&#x3D;1024, A&#x3D;16, Total Parameters&#x3D;340M).\nIn all cases we set the feed-forward&#x2F;filter size to be 4H, i.e., 3072 for the H &#x3D; 768 and 4096 for the H &#x3D; 1024.\nBERT(BASE) was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left.\nInput&#x2F;Output Representations\nTo make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., ⟨ Question, Answer ⟩) in one token sequence.\nA “sequence” refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together.\nWe use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary.\nThe first token of every sequence is always a special classification token ([CLS]). \nSentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. \n\n\nC ∈ (H,), Ti ∈ (H,)\n\nPre-training BERTWe pre-train BERT using two unsupervised tasks, described in this section.\nTask #1: Masked LM\nIntuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to-right and a right-to-left model.\nUnfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly “see itself”, and the model could trivially predict the target word in a multi-layered context.\nIn order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens.\nIn this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. We refer to this procedure as a “masked LM” (MLM).\nIn all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders, we only predict the masked words rather than reconstructing the entire input.\nA downside is that we are creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not appear during fine-tuning. \nTo mitigate this, we chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time.\nThen, Ti will be used to predict the original token with cross entropy loss. \nTask #2: Next Sentence Prediction (NSP)\nIn order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus.\nSpecifically, when choosing the sentences A and B for each pre- training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext).\nAs we show in Figure 1, C is used for next sentence prediction (NSP).\nPre-training data\nFor the pre-training corpus we use the BooksCorpus (800M words) and English Wikipedia (2,500M words). It is criti- cal to use a document-level corpus rather than a shuffled sentence-level corpus in order to extract long contiguous sequences.\nFine-tuning BERTFor each task, we simply plug in the task-specific inputs and outputs into BERT and fine-tune all the parameters end-to-end.\nAt the output, the token representations are fed into an output layer for token-level tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as entailment or sentiment analysis.\nCompared to pre-training, fine-tuning is relatively inexpensive.\nGPT2","dateCreated":"2020-06-10T15:42:33+08:00","dateModified":"2024-02-17T02:02:43+08:00","datePublished":"2020-06-10T15:42:33+08:00","description":"Paper summary for elmo, transformer, bet, and gpt.","headline":"NLP Summary","image":["https://ria.gallerycdn.vsassets.io/extensions/ria/nlp/0.2.11/1487192519683/Microsoft.VisualStudio.Services.Icons.Default","https://thumbor.forbes.com/thumbor/960x0/https%3A%2F%2Fblogs-images.forbes.com%2Fbernardmarr%2Ffiles%2F2019%2F06%2F5-Amazing-Examples-Of-Natural-Language-Processing-NLP-In-Practice-1200x639.jpg"],"mainEntityOfPage":{"@type":"WebPage","@id":"https://joddiy.github.io/2020/06/10/NLP-Summary/"},"publisher":{"@type":"Organization","name":"Joddiy Zhang","sameAs":["https://github.com/joddiy","https://scholar.google.com/citations?user=KH-xv38AAAAJ&hl=en&oi=sra","https://www.linkedin.com/in/joddiyzhang/"],"image":"14108933.jpeg","logo":{"@type":"ImageObject","url":"14108933.jpeg"}},"url":"https://joddiy.github.io/2020/06/10/NLP-Summary/","keywords":"NLP","thumbnailUrl":"https://ria.gallerycdn.vsassets.io/extensions/ria/nlp/0.2.11/1487192519683/Microsoft.VisualStudio.Services.Icons.Default"}</script>
    <meta name="description" content="Paper summary for elmo, transformer, bet, and gpt.">
<meta property="og:type" content="blog">
<meta property="og:title" content="NLP Summary">
<meta property="og:url" content="https://joddiy.github.io/2020/06/10/NLP-Summary/index.html">
<meta property="og:site_name" content="Jz Blog">
<meta property="og:description" content="Paper summary for elmo, transformer, bet, and gpt.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://joddiy.github.io/1.png">
<meta property="og:image" content="https://joddiy.github.io/2.png">
<meta property="og:image" content="https://joddiy.github.io/3.png">
<meta property="og:image" content="https://joddiy.github.io/4.png">
<meta property="og:image" content="https://joddiy.github.io/5.png">
<meta property="og:image" content="https://joddiy.github.io/6.png">
<meta property="og:image" content="https://joddiy.github.io/7.png">
<meta property="og:image" content="https://joddiy.github.io/8.png">
<meta property="og:image" content="https://joddiy.github.io/9.png">
<meta property="og:image" content="https://joddiy.github.io/10.png">
<meta property="og:image" content="https://joddiy.github.io/11.png">
<meta property="og:image" content="https://joddiy.github.io/12.png">
<meta property="og:image" content="https://joddiy.github.io/13.png">
<meta property="og:image" content="https://joddiy.github.io/14.png">
<meta property="og:image" content="https://joddiy.github.io/19.png">
<meta property="og:image" content="https://joddiy.github.io/20.png">
<meta property="og:image" content="https://joddiy.github.io/21.png">
<meta property="og:image" content="https://joddiy.github.io/22.png">
<meta property="og:image" content="https://joddiy.github.io/23.png">
<meta property="og:image" content="https://joddiy.github.io/18.png">
<meta property="og:image" content="https://joddiy.github.io/17.png">
<meta property="og:image" content="https://joddiy.github.io/15.png">
<meta property="og:image" content="https://joddiy.github.io/16.png">
<meta property="article:published_time" content="2020-06-10T07:42:33.000Z">
<meta property="article:modified_time" content="2024-02-16T18:02:43.636Z">
<meta property="article:author" content="Joddiy Zhang">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://joddiy.github.io/1.png">
    
    
        
    
    
        <meta property="og:image" content="https://joddiy.github.io/assets/images/14108933.jpeg"/>
    
    
        <meta property="og:image" content="https://ria.gallerycdn.vsassets.io/extensions/ria/nlp/0.2.11/1487192519683/Microsoft.VisualStudio.Services.Icons.Default"/>
        <meta class="swiftype" name="image" data-type="enum" content="https://ria.gallerycdn.vsassets.io/extensions/ria/nlp/0.2.11/1487192519683/Microsoft.VisualStudio.Services.Icons.Default"/>
    
    
        <meta property="og:image" content="https://thumbor.forbes.com/thumbor/960x0/https%3A%2F%2Fblogs-images.forbes.com%2Fbernardmarr%2Ffiles%2F2019%2F06%2F5-Amazing-Examples-Of-Natural-Language-Processing-NLP-In-Practice-1200x639.jpg"/>
        <meta class="swiftype" name="image" data-type="enum" content="https://thumbor.forbes.com/thumbor/960x0/https%3A%2F%2Fblogs-images.forbes.com%2Fbernardmarr%2Ffiles%2F2019%2F06%2F5-Amazing-Examples-Of-Natural-Language-Processing-NLP-In-Practice-1200x639.jpg"/>
    
    
    <!--STYLES-->
    
<link rel="stylesheet" href="/assets/css/style-x8blglznjjnb9pnnwui5zw4h43ysufmsh1b0omicawm4vhqcutzqavokgpne.min.css">

    <!--STYLES END-->
    

    

    
        
            
<link rel="stylesheet" href="/assets/css/gitalk.css">

        
    
</head>

    <body>
        <div id="blog">
            <!-- Define author's picture -->


    
        
            
        
    

<header id="header" data-behavior="4">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a
            class="header-title-link"
            href="/"
            aria-label=""
        >
            Jz Blog
        </a>
    </div>
    
        
            <a
                class="header-right-picture "
                href="#about"
                aria-label="Open the link: /#about"
            >
        
        
            <img class="header-picture" src="/assets/images/14108933.jpeg" alt="Author&#39;s picture"/>
        
        </a>
    
</header>

            <!-- Define author's picture -->



        
    

<nav id="sidebar" data-behavior="4">
    <div class="sidebar-container">
        
            <div class="sidebar-profile">
                <a
                    href="/#about"
                    aria-label="Read more about the author"
                >
                    <img class="sidebar-profile-picture" src="/assets/images/14108933.jpeg" alt="Author&#39;s picture"/>
                </a>
                <h4 class="sidebar-profile-name">Joddiy Zhang</h4>
                
                    <h5 class="sidebar-profile-bio"><p><a href="mailto:&#x6a;&#111;&#x64;&#x64;&#105;&#121;&#x7a;&#104;&#97;&#x6e;&#103;&#x40;&#103;&#x6d;&#x61;&#105;&#108;&#46;&#99;&#x6f;&#109;">&#x6a;&#111;&#x64;&#x64;&#105;&#121;&#x7a;&#104;&#97;&#x6e;&#103;&#x40;&#103;&#x6d;&#x61;&#105;&#108;&#46;&#99;&#x6f;&#109;</a></p>
</h5>
                
            </div>
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/"
                            
                            rel="noopener"
                            title="Home"
                        >
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-categories"
                            
                            rel="noopener"
                            title="Categories"
                        >
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-tags"
                            
                            rel="noopener"
                            title="Tags"
                        >
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Tags</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-archives"
                            
                            rel="noopener"
                            title="Archives"
                        >
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="#about"
                            
                            rel="noopener"
                            title="About"
                        >
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://github.com/joddiy"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="GitHub"
                        >
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://scholar.google.com/citations?user=KH-xv38AAAAJ&hl=en&oi=sra"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="Google Scholar"
                        >
                        <i class="sidebar-button-icon fab fa-google" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Google Scholar</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://www.linkedin.com/in/joddiyzhang/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="LinkedIn"
                        >
                        <i class="sidebar-button-icon fab fa-linkedin" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">LinkedIn</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
        <div class="post-header-cover
                    text-center
                    post-header-cover--partial"
             style="background-image:url('https://thumbor.forbes.com/thumbor/960x0/https%3A%2F%2Fblogs-images.forbes.com%2Fbernardmarr%2Ffiles%2F2019%2F06%2F5-Amazing-Examples-Of-Natural-Language-Processing-NLP-In-Practice-1200x639.jpg');"
             data-behavior="4">
            
        </div>

            <div id="main" data-behavior="4"
                 class="hasCover
                        hasCoverMetaOut
                        ">
                
<article class="post">
    
    
        <div class="post-header main-content-wrap text-center">
    
        <h1 class="post-title">
            NLP Summary
        </h1>
    
    
        <div class="post-meta">
    <time datetime="2020-06-10T15:42:33+08:00">
	
		    Jun 10, 2020
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Research/">Research</a>


    
</div>

    
</div>

    
    <div class="post-content markdown">
        <div class="main-content-wrap">
            <h1 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. </p>
<p>learning high quality representations can be challenging. They should ideally model both: </p>
<ol>
<li>complex characteristics of word use (e.g., syntax and semantics), and</li>
<li>how these uses vary across linguistic contexts (i.e., to model polysemy).</li>
</ol>
<p>ELMo (Embeddings from Language Models). ELMo representations are deep, in the sense that they are a function of all of the internal layers of the biLM.</p>
<p>Using intrinsic evaluations, we show that the higher-level LSTM states capture context-dependent aspects of word meaning (e.g., they can be used without modification to perform well on supervised word sense disambiguation tasks) while lowerlevel states model aspects of syntax (e.g., they can be used to do part-of-speech tagging). </p>
<h2 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h2><p>Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors are a standard component of most state-of-the-art NLP architectures. However, these approaches for learning word vectors only allow <strong>a single contextindependent representation for each word</strong>.</p>
<p>Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information or learning separate vectors for each word sense. Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes.</p>
<p>Other recent work has also focused on learning <strong>context-dependent representations</strong>. context2vec uses a bidirectional LSTM to encode the <strong>context around a pivot word</strong>. Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either <strong>a supervised neural machine translation (MT) system</strong> or <strong>an unsupervised language model</strong>.</p>
<blockquote>
<p>what?</p>
</blockquote>
<p>Previous work has also shown that different layers of deep biRNNs encode different types of information. </p>
<h2 id="ELMo-Embeddings-from-Language-Models"><a href="#ELMo-Embeddings-from-Language-Models" class="headerlink" title="ELMo: Embeddings from Language Models"></a>ELMo: Embeddings from Language Models</h2><p>ELMo word representations are functions of the entire input sentence. They are computed on top of two-layer biLMs with character convolutions, as a linear function of the internal network states. This setup allows us to do semi-supervised learning, where the biLM is pretrained at a large scale and easily incorporated into a wide range of existing neural NLP architectures.</p>
<h3 id="Bidirectional-language-models"><a href="#Bidirectional-language-models" class="headerlink" title="Bidirectional language models"></a>Bidirectional language models</h3><p>At each position k, each LSTM layer outputs a context-dependent representation <code>h(LM, k, j)</code> where j &#x3D; 1,…,L. The top layer LSTM output, <code>h(LM, k, L)</code> , is used to predict the next token tk+1 with a Softmax layer.</p>
<p>A backward LM can be implemented in an analogous way to a forward LM, with each backward LSTM layer j in a L layer deep model producing representations <code>h(LM, k, j)</code> of tk given (tk+1,…,tN).</p>
<p>A biLM combines both a forward and backward LM. Our formulation jointly maximizes the log likelihood of the forward and backward directions:</p>
<p><img src="/1.png"></p>
<p>We tie the parameters for both the token representation (Θx) and Softmax layer (Θs) in the forward and backward direction while maintaining separate parameters for the LSTMs in each direction.</p>
<h3 id="ELMo-1"><a href="#ELMo-1" class="headerlink" title="ELMo"></a>ELMo</h3><p>ELMo is a task specific combination of the intermediate layer representations in the biLM. For each token tk, a L-layer biLM computes a set of 2L + 1 representations.</p>
<p><img src="/2.png"></p>
<p><code>h(LM, k, j)</code> is the token layer.</p>
<p>ELMo collapses all layers in R into a single vector, ELMok &#x3D; E (Rk; Θe). More generally, we compute a task specific weighting of all biLM layers:</p>
<p>stask are softmax-normalized weights and the scalar parameter γtask allows the task model to scale the entire ELMo vector. γ is of practical importance to aid the optimization process.</p>
<p>Considering that the activations of each biLM layer have a different distribution, in some cases it also helped to apply layer normalization (Ba et al., 2016) to each biLM layer before weighting.</p>
<h3 id="Using-biLMs-for-supervised-NLP-tasks"><a href="#Using-biLMs-for-supervised-NLP-tasks" class="headerlink" title="Using biLMs for supervised NLP tasks"></a>Using biLMs for supervised NLP tasks</h3><p>To add ELMo to the supervised model, we first freeze the weights of the biLM and then concatenate the ELMo vector <code>(ELMo, task, k)</code> with xk and pass the ELMo enhanced representation [x; <code>(ELMo, task, k)</code>] into the task RNN.</p>
<p>Finally, we found it beneficial to add a moderate amount of dropout to ELMo and in some cases to regularize the ELMo weights by adding λ∥w∥2 to the loss. This imposes an inductive bias on the ELMo weights to stay close to an average of all biLM layers.</p>
<h3 id="Pre-trained-bidirectional-language-model-architecture"><a href="#Pre-trained-bidirectional-language-model-architecture" class="headerlink" title="Pre-trained bidirectional language model architecture"></a>Pre-trained bidirectional language model architecture</h3><p>For a purely character-based input representation, we halved all embedding and hidden dimensions from the single best model CNN-BIG-LSTM in Jozefowicz et al. (2016). The final model uses L &#x3D; 2 biLSTM layers with 4096 units and 512 dimension projections and a residual connection from the first to second layer. The context insensitive type representation uses 2048 character n-gram convolutional filters followed by two highway layers (Srivastava et al., 2015) and a linear projection down to a 512 representation.</p>
<p>The context insensitive type representation uses 2048 character n-gram convolutional filters followed by two highway layers and a linear projection down to a 512 representation. </p>
<p>As a result, the biLM provides three layers of representations for each input token, including those outside the training set due to the purely character input.</p>
<p>Once pretrained, the biLM can compute representations for any task. </p>
<h1 id="ULMFiT"><a href="#ULMFiT" class="headerlink" title="ULMFiT"></a>ULMFiT</h1><h2 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h2><p>We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model.</p>
<p>transductive transfer vs. inductive transfer</p>
<blockquote>
<p>what?</p>
</blockquote>
<p>Language models (LM) overfit to small datasets and suffered catastrophic forgetting when fine-tuned with a classifier. Compared to CV, NLP models are typically more shallow and thus require different fine-tuning methods.</p>
<p>We propose a new method, Universal Language Model Fine-tuning (ULMFiT) that addresses these issues and enables robust inductive transfer learning for any NLP task.</p>
<p>The same 3-layer LSTM architecture— with the same hyperparameters and no additions other than tuned dropout hyperparameters— outperforms highly engineered models and trans-fer learning approaches on six widely studied text classification tasks.</p>
<p>Our contributions are the following: </p>
<ol>
<li>We propose Universal Language Model Fine-tuning (ULMFiT), a method that can be used to achieve CV-like transfer learning for any task for NLP. </li>
<li>We propose <em>discriminative fine-tuning, slanted triangular learning rates, and gradual unfreezing</em>, novel techniques to retain previous knowledge and avoid catastrophic forgetting during fine-tuning. </li>
<li>We significantly outperform the state-of-the-art on six representative text classification datasets, with an error reduction of 18-24% on the majority of datasets. </li>
<li>We show that our method enables extremely sample-efficient transfer learning and perform an extensive ablation analysis.</li>
</ol>
<h2 id="Related-work-1"><a href="#Related-work-1" class="headerlink" title="Related work"></a>Related work</h2><h3 id="Hypercolumns"><a href="#Hypercolumns" class="headerlink" title="Hypercolumns"></a>Hypercolumns</h3><p>The prevailing approach is to pretrain embeddings that capture additional context via other tasks. Embeddings at different levels are then used as features, concatenated either with the word embeddings or with the inputs at intermediate layers. This method is known as hypercolumns.</p>
<h2 id="Universal-Language-Model-Fine-tuning"><a href="#Universal-Language-Model-Fine-tuning" class="headerlink" title="Universal Language Model Fine-tuning"></a>Universal Language Model Fine-tuning</h2><p>We propose Universal Language Model Fine-tuning (ULMFiT), which pretrains a language model (LM) on a large general-domain corpus and fine-tunes it on the target task using novel techniques. The method is universal in the sense that it meets these practical criteria: </p>
<ol>
<li>It works across tasks varying in document size, number, and label type; </li>
<li>it uses a single architecture and training process; </li>
<li>it requires no custom feature engineering or preprocessing; and </li>
<li>it does not require additional in-domain documents or labels.</li>
</ol>
<p>ULMFiT consists of the following steps, which we show in Figure 1: a) General-domain LM pretraining; b) target task LM fine-tuning; and c) target task classifier fine-tuning.</p>
<p><img src="/3.png"></p>
<h3 id="General-domain-LM-pretraining"><a href="#General-domain-LM-pretraining" class="headerlink" title="General-domain LM pretraining"></a>General-domain LM pretraining</h3><p>We pretrain the language model on Wikitext-103 (Merity et al., 2017b) consisting of 28,595 preprocessed Wikipedia articles and 103 million words.</p>
<h3 id="Target-task-LM-fine-tuning"><a href="#Target-task-LM-fine-tuning" class="headerlink" title="Target task LM fine-tuning"></a>Target task LM fine-tuning</h3><p>We thus fine-tune the LM on data of the target task. Given a pretrained general-domain LM, this stage converges faster as it only needs to adapt to the idiosyncrasies of the target data, and it allows us to train a robust LM even for small datasets. We propose discriminative fine-tuning and slanted triangular learning rates.</p>
<h4 id="Discriminative-fine-tuning"><a href="#Discriminative-fine-tuning" class="headerlink" title="Discriminative fine-tuning"></a>Discriminative fine-tuning</h4><p>For discriminative fine-tuning, we split the parameters θ into {θ1, . . . , θL} where θl contains the parameters of the model at the l-th layer and L is the number of layers of the model. Similarly, we obtain {η1,…,ηL} where ηl is the learning rate of the l-th layer.</p>
<p>The SGD update with discriminative fine-tuning is then the following:</p>
<p><img src="/4.png"></p>
<p>We empirically found it to work well to first choose the learning rate ηL of the last layer by fine-tuning only the last layer and using ηl−1 &#x3D; ηl&#x2F;2.6 as the learning rate for lower layers.</p>
<h4 id="Slanted-triangular-learning-rates"><a href="#Slanted-triangular-learning-rates" class="headerlink" title="Slanted triangular learning rates"></a>Slanted triangular learning rates</h4><p>we propose slanted triangular learning rates (STLR), which first linearly increases the learning rate and then linearly decays it according to the following update schedule, which can be seen in:</p>
<p><img src="/5.png"></p>
<p><img src="/6.png"></p>
<p>where T is the number of training iterations, cut_frac is the fraction of iterations we increase the LR, cut is the iteration when we switch from increasing to decreasing the LR, p is the fraction of the number of iterations we have increased or will decrease the LR respectively, ratio specifies how much smaller the lowest LR is from the maximum LR ηmax, and ηt is the learning rate at iteration t.</p>
<p>We generally use cut_frac &#x3D; 0.1, ratio &#x3D; 32 and ηmax &#x3D; 0.01.</p>
<h3 id="Target-task-classifier-fine-tuning"><a href="#Target-task-classifier-fine-tuning" class="headerlink" title="Target task classifier fine-tuning"></a>Target task classifier fine-tuning</h3><p>Finally, for fine-tuning the classifier, we augment the pretrained language model with two additional linear blocks.</p>
<h4 id="Concat-pooling"><a href="#Concat-pooling" class="headerlink" title="Concat pooling"></a>Concat pooling</h4><p>As input documents can consist of hundreds of words, information may get lost if we only consider the last hidden state of the model. For this reason, we concatenate the hidden state at the last time step hT of the document with both the max-pooled and the mean-pooled representation of the hidden states over as many time steps as fit in GPU memory H &#x3D; {h1,…,hT}:</p>
<p><img src="/7.png"></p>
<h4 id="Gradual-unfreezing"><a href="#Gradual-unfreezing" class="headerlink" title="Gradual unfreezing"></a>Gradual unfreezing</h4><p>We first unfreeze the last layer and fine-tune all unfrozen layers for one epoch. We then unfreeze the next lower frozen layer and repeat, until we finetune all layers until convergence at the last iteration.</p>
<h4 id="BPTT-for-Text-Classification-BPT3C"><a href="#BPTT-for-Text-Classification-BPT3C" class="headerlink" title="BPTT for Text Classification (BPT3C)"></a>BPTT for Text Classification (BPT3C)</h4><p>In order to make fine-tuning a classifier for large documents feasible, we propose BPTT for Text Classification (BPT3C): We divide the document into fixedlength batches of size b. At the beginning of each batch, the model is initialized with the final state of the previous batch. </p>
<h4 id="Bidirectional-language-model"><a href="#Bidirectional-language-model" class="headerlink" title="Bidirectional language model"></a>Bidirectional language model</h4><p>For all our experiments, we pretrain both a forward and a backward LM. </p>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><h2 id="Introduction-2"><a href="#Introduction-2" class="headerlink" title="Introduction"></a>Introduction</h2><p>Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t.</p>
<p>This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.</p>
<p>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences</p>
<p>In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.</p>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.</p>
<p>In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions.</p>
<p>In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with <strong>Multi-Head Attention</strong>.</p>
<p>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.</p>
<p>To the best of our knowledge, however, the Transformer is the first transduction model relying<br>entirely on self-attention to compute representations of its input and output without using sequencealigned<br>RNNs or convolution.</p>
<h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure:</p>
<p><img src="/8.png"></p>
<h3 id="Encoder-and-Decoder-Stacks"><a href="#Encoder-and-Decoder-Stacks" class="headerlink" title="Encoder and Decoder Stacks"></a>Encoder and Decoder Stacks</h3><p><strong>Encoder:</strong></p>
<p>The encoder is composed of a stack of N &#x3D; 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. </p>
<p>We employ a residual connection around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. </p>
<p>To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel &#x3D; 512.</p>
<p><strong>Decoder:</strong></p>
<p>The decoder is also composed of a stack of N &#x3D; 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.</p>
<p>Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. </p>
<p>We also modify the self-attention sub-layer(Masked Multi-Head Attention) in the decoder stack to ensure that the predictions for position i can depend only on the known outputs at positions less than i.</p>
<h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. </p>
<blockquote>
<p>Attention(Q, K, V) &#x3D; softmax(Q * K.T &#x2F; √dk) * V</p>
</blockquote>
<p><img src="/9.png"></p>
<p><strong>Scaled Dot-Product Attention:</strong></p>
<p>The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by pdk, and apply a softmax function to obtain the weights on the values.</p>
<p>In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:</p>
<p><img src="/10.png"></p>
<blockquote>
<p>X ∈ (len, dmodel), WQ ∈ (dmodel, dk), WK ∈ (dmodel, dk), WV ∈ (dmodel, dv)<br>Q &#x3D; X * WQ ∈ (len, dk)<br>K &#x3D; X * WK ∈ (len, dk)<br>V &#x3D; X * WV ∈ (len, dv)<br>Q * K.T ∈ (len, len)<br>softmax(Q * K.T &#x2F; √dk) ∈ (len, len)<br>softmax(Q * K.T &#x2F; √dk) * V ∈ (len, dv)</p>
</blockquote>
<blockquote>
<p>two types of attention: additive attention and dot-product attention?</p>
</blockquote>
<p>Why divide √dk?</p>
<ul>
<li>And the author suspect for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients</li>
<li>q · k &#x3D; sum(qi * ki) for i from 1 to dk, has mean 0 and variance dk.</li>
</ul>
<p><strong>Multi-Head Attention:</strong></p>
<p>We found it beneficial to linearly project the queries, keys and values h times with different, learned<br>linear projections to dk, dk and dv dimensions, respectively.</p>
<p>On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values(len*dv). These are concatenated and once again projected, resulting in the final values, as depicted in Figure:</p>
<p><img src="/11.png"></p>
<blockquote>
<p>The Q, K, V should be the output of previous layer, the shape is (len, dmodel)<br>headi ∈ (len, dv), WO ∈ (h * dv, dmodel)<br>MultiHead(Q, K, V) ∈ (len, dmodel)<br>the author uses h &#x3D; 8, dk &#x3D; dv &#x3D; dmodel&#x2F;h &#x3D; 64, dmodel &#x3D; 512</p>
</blockquote>
<blockquote>
<p>In implemention, we combine the h head in one linear layer as:<br>X ∈ (len, dmodel), WQ ∈ (dmodel, h * dk), WK ∈ (dmodel, h * dk), WV ∈ (dmodel, h * dv)<br>h * dk &#x3D; dmodel<br>Q &#x3D; X * WQ ∈ (len, h * dk)<br>K &#x3D; X * WK ∈ (len, h * dk)<br>V &#x3D; X * WV ∈ (len, h * dv)<br>Q * K.T ∈ (len, len)<br>softmax(Q * K.T &#x2F; √dk) ∈ (len, len)<br>softmax(Q * K.T &#x2F; √dk) * V ∈ (len, h * dv)</p>
</blockquote>
<p><strong>Applications of Attention in our Model:</strong></p>
<ul>
<li>In a self-attention in encoder, the Q, K, V comes from previous layer’s output</li>
<li>In the attention in decoder, the Q comes from the previous decoder, and K, V comes from encoder.</li>
<li>In decoder, when we predict word i, we mask out (setting to −∞) all work i+1 to len in K, V from encoder.</li>
</ul>
<h3 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h3><p><img src="/12.png"></p>
<blockquote>
<p>X ∈ (len, dmodel), W1 ∈ (dmodel, dff), W1 ∈ (dff, dmodel), b ∈ (len, dmodel)<br>X * W1 ∈ (len, dff)<br>FFN(x) ∈ (len, dmodel)<br>dmodel &#x3D; 512, dff &#x3D; 2048</p>
</blockquote>
<h3 id="Embeddings-and-Softmax"><a href="#Embeddings-and-Softmax" class="headerlink" title="Embeddings and Softmax"></a>Embeddings and Softmax</h3><p>In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation(convert the decoder output to predicted next-token probabilities). In the embedding layers, we multiply those weights by √dmodel.</p>
<blockquote>
<p>WHY?</p>
</blockquote>
<h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>In order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.</p>
<p>We add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks. In this work, we use sine and cosine functions of different frequencies:</p>
<p><img src="/13.png"></p>
<p>where pos is the position and i is the dimension, and any fixed offset k, PE(pos+k) can be represented as a linear function of PE(pos).</p>
<h2 id="Why-Self-Attention"><a href="#Why-Self-Attention" class="headerlink" title="Why Self-Attention"></a>Why Self-Attention</h2><p>We compare various aspects of self-attention layers to the recurrent and convolutional layers.</p>
<p><img src="/14.png"></p>
<p><strong>One is the total computational complexity per layer.</strong></p>
<p>In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d.</p>
<p>To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n&#x2F;r).</p>
<p>A single convolutional layer with kernel width k &lt; n does not connect all pairs of input and output positions. Doing so requires a stack of O(n&#x2F;k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions. Convolutional layers are generally more expensive than recurrent layers, by a factor of k.</p>
<p><strong>Another is the amount of computation that can be parallelized.</strong></p>
<p><strong>The third is the path length between long-range dependencies in the network.</strong></p>
<p>Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies.</p>
<h1 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h1><p>Labeled data for learning these specific tasks is scarce, so it is challenging for discriminatively trained models to perform adequately.</p>
<p>We demonstrate that large gains on these tasks can be realized by <span style="color:blue">generative pre-training</span> of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task.</p>
<h2 id="Introduction-3"><a href="#Introduction-3" class="headerlink" title="Introduction"></a>Introduction</h2><p>Models that can leverage linguistic information from unlabeled data provide a valuable alternative to gathering more annotation, which can be time-consuming and expensive. Further, even in cases where considerable supervision is available, learning good representations in an unsupervised fashion can provide a significant performance boost. </p>
<p>Leveraging more than word-level information from unlabeled text, however, is challenging for two main reasons. </p>
<ol>
<li>It is unclear what type of optimization objectives are most effective at learning text representations that are useful for transfer. </li>
<li>There is no consensus on the most effective way to transfer these learned representations to the target task.</li>
</ol>
<p>In this paper, we explore a semi-supervised approach for language understanding tasks using <span style="color:blue">a combination of unsupervised pre-training and supervised fine-tuning. </span></p>
<p>Our goal is to learn a universal representation that transfers with little adaptation to a wide range of tasks.</p>
<p>We employ a two-stage training procedure:</p>
<ol>
<li>First, we use a language modeling objective on the unlabeled data to learn the initial parameters of a neural network model. </li>
<li>Subsequently, we adapt these parameters to a target task using the corresponding supervised objective.</li>
</ol>
<p>For our model architecture, we use the Transformer.</p>
<p>During transfer, we utilize task-specific input adaptations derived from traversal-style approaches, which process structured text input as a single contiguous sequence of tokens.</p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p><strong>Semi-supervised learning for NLP</strong></p>
<p>The earliest approaches used unlabeled data to compute word-level or phrase-level statistics. Over the last few years, researchers have demonstrated the benefits of using word embeddings.</p>
<p>These approaches, however, mainly transfer word-level information, whereas we aim to capture higher-level semantics.</p>
<p><strong>Unsupervised pre-training</strong></p>
<p>Research demonstrated that pre-training acts as a regularization scheme, enabling better generalization in deep neural networks.</p>
<p>The usage of LSTM models in other approaches restricts their prediction ability to a short range. Our choice of transformer networks allows us to capture longerrange linguistic structure.</p>
<p>Other approaches use hidden representations from a pre-trained model involving a substantial amount of new parameters for each separate target task, whereas we require minimal changes to our model architecture during transfer.</p>
<p><strong>Auxiliary training objectives</strong></p>
<p>POS tagging, chunking, named entity recognition, and language modeling.</p>
<h2 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h2><p>Our training procedure consists of two stages. The first stage is learning a high-capacity language<br>model on a large corpus of text. This is followed by a fine-tuning stage, where we adapt the model to<br>a discriminative task with labeled data.</p>
<h3 id="Unsupervised-pre-training"><a href="#Unsupervised-pre-training" class="headerlink" title="Unsupervised pre-training"></a>Unsupervised pre-training</h3><p>We use a standard language modeling objective to maximize the following likelihood:</p>
<p><img src="/19.png"></p>
<p>where k is the size of the context window, and the conditional probability P is modeled using a neural network with parameters Θ. These parameters are trained using stochastic gradient descent.</p>
<p>In our experiments, we use a multi-layer Transformer decoder or the language model, which is a variant of the transformer. This model applies a multi-headed self-attention operation over the input context tokens followed by position-wise feedforward layers to produce an output distribution over target tokens:</p>
<p><img src="/20.png"></p>
<p>where U &#x3D; (u−k, … , u−1) is the context vector of tokens, n is the number of layers, We is the token<br>embedding matrix, and Wp is the position embedding matrix.</p>
<h3 id="Supervised-fine-tuning"><a href="#Supervised-fine-tuning" class="headerlink" title="Supervised fine-tuning"></a>Supervised fine-tuning</h3><p>We assume a labeled dataset C, where each instance consists of a sequence of input tokens, x1, … , xm, along with a label y.</p>
<p>The inputs are passed through our pre-trained model to obtain the final transformer block’s activation h(m,l) , which is then fed into an added linear output layer with parameters Wy to predict y:</p>
<p><img src="/21.png"></p>
<p>This gives us the following objective to maximize:</p>
<p><img src="/22.png"></p>
<p>Specifically, we optimize the following objective (with weight λ):</p>
<p><img src="/23.png"></p>
<p>Overall, the only extra parameters we require during fine-tuning are Wy , and embeddings for delimiter tokens.</p>
<h3 id="Task-specific-input-transformations"><a href="#Task-specific-input-transformations" class="headerlink" title="Task-specific input transformations"></a>Task-specific input transformations</h3><p>Certain other tasks, like question answering or textual entailment, have structured inputs such as ordered sentence pairs, or triplets of document, question, and answers.</p>
<p>Since our pre-trained model was trained on contiguous sequences of text, we require some modifications to apply it to these tasks.</p>
<p>we use a traversal-style approach [52], where we convert structured inputs into an ordered sequence that our pre-trained model can process. These input transformations allow us to avoid making extensive changes to the architecture across tasks. </p>
<p><img src="/18.png"></p>
<p>All transformations include adding randomly initialized start and end tokens (⟨s⟩, ⟨e⟩).</p>
<p><strong>Textual entailment</strong></p>
<p><strong>Similarity</strong></p>
<p><strong>Question Answering and Commonsense Reasoning</strong></p>
<h1 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h1><p>BERT, which stands for Bidirectional Encoder Representations from Transformers.</p>
<p>BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.</p>
<h2 id="Introduction-4"><a href="#Introduction-4" class="headerlink" title="Introduction"></a>Introduction</h2><p>Language model pre-training has been shown to be effective for improving many natural language processing tasks.</p>
<p>There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.</p>
<p><img src="/17.png"></p>
<p>The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.</p>
<p>The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.</p>
<p>We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.</p>
<p>The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-to-right architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer.</p>
<p>In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a <span style="color:blue">“masked language model” (MLM)</span> pre-training objective.</p>
<p>The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.</p>
<p>Unlike left-to-right language model pre-training, <span style="color:blue">the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer.</span></p>
<p>In addition to the masked language model, we also use a <span style="color:blue">“next sentence prediction”</span> task that jointly pretrains text-pair representations. </p>
<h2 id="Related-Work-1"><a href="#Related-Work-1" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="Unsupervised-Feature-based-Approaches"><a href="#Unsupervised-Feature-based-Approaches" class="headerlink" title="Unsupervised Feature-based Approaches"></a>Unsupervised Feature-based Approaches</h3><p>Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch. </p>
<p>ELMo and its predecessor generalize traditional word embedding research along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model.</p>
<h3 id="Unsupervised-Fine-tuning-Approaches"><a href="#Unsupervised-Fine-tuning-Approaches" class="headerlink" title="Unsupervised Fine-tuning Approaches"></a>Unsupervised Fine-tuning Approaches</h3><p>More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task.</p>
<p>The advantage of these approaches is that few parameters need to be learned from scratch.</p>
<p>At least partly due to this advantage, OpenAI GPT (Radford et al., 2018) achieved previously state-of-the-art results on many sentence-level tasks from the GLUE benchmark.</p>
<h3 id="Transfer-Learning-from-Supervised-Data"><a href="#Transfer-Learning-from-Supervised-Data" class="headerlink" title="Transfer Learning from Supervised Data"></a>Transfer Learning from Supervised Data</h3><h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>There are two steps in our framework: pre-training and fine-tuning.</p>
<p>During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks.</p>
<p>A distinctive feature of BERT is its unified architecture across different tasks. There is minimal difference between the pre-trained architecture and the final downstream architecture.</p>
<p><strong>Model Architecture</strong></p>
<p><img src="/15.png"></p>
<p>BERT’s model architecture is a multi-layer bidirectional Transformer encoder.</p>
<p>In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.</p>
<p>We primarily report results on two model sizes: BERT(BASE) (L&#x3D;12, H&#x3D;768, A&#x3D;12, Total Parameters&#x3D;110M) and BERT(LARGE) (L&#x3D;24, H&#x3D;1024, A&#x3D;16, Total Parameters&#x3D;340M).</p>
<p>In all cases we set the feed-forward&#x2F;filter size to be 4H, i.e., 3072 for the H &#x3D; 768 and 4096 for the H &#x3D; 1024.</p>
<p>BERT(BASE) was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses <span style="color:blue">bidirectional self-attention</span>, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left.</p>
<p><strong>Input&#x2F;Output Representations</strong></p>
<p>To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., ⟨ Question, Answer ⟩) in one token sequence.</p>
<p>A “sequence” refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together.</p>
<p>We use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary.</p>
<p>The first token of every sequence is always a special classification token ([CLS]). </p>
<p>Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. </p>
<p><img src="/16.png"></p>
<blockquote>
<p>C ∈ (H,), Ti ∈ (H,)</p>
</blockquote>
<h3 id="Pre-training-BERT"><a href="#Pre-training-BERT" class="headerlink" title="Pre-training BERT"></a>Pre-training BERT</h3><p>We pre-train BERT using two unsupervised tasks, described in this section.</p>
<p><strong>Task #1: Masked LM</strong></p>
<p>Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to-right and a right-to-left model.</p>
<p>Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly “see itself”, and the model could trivially predict the target word in a multi-layered context.</p>
<p>In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens.</p>
<p>In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. We refer to this procedure as a <span style="color:blue">“masked LM” (MLM).</span></p>
<p>In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders, we only predict the masked words rather than reconstructing the entire input.</p>
<p><span style="color:blue">A downside</span> is that we are creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not appear during fine-tuning. </p>
<p>To mitigate this, we chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time.</p>
<p>Then, Ti will be used to predict the original token with cross entropy loss. </p>
<p><strong>Task #2: Next Sentence Prediction (NSP)</strong></p>
<p>In order to train a model that understands sentence relationships, we pre-train for a <strong>binarized next sentence prediction task</strong> that can be trivially generated from any monolingual corpus.</p>
<p>Specifically, when choosing the sentences A and B for each pre- training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext).</p>
<p>As we show in Figure 1, C is used for next sentence prediction (NSP).</p>
<p><strong>Pre-training data</strong></p>
<p>For the pre-training corpus we use the BooksCorpus (800M words) and English Wikipedia (2,500M words). It is criti- cal to use a document-level corpus rather than a shuffled sentence-level corpus in order to extract long contiguous sequences.</p>
<h3 id="Fine-tuning-BERT"><a href="#Fine-tuning-BERT" class="headerlink" title="Fine-tuning BERT"></a>Fine-tuning BERT</h3><p>For each task, we simply plug in the task-specific inputs and outputs into BERT and fine-tune all the parameters end-to-end.</p>
<p>At the output, the token representations are fed into an output layer for token-level tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as entailment or sentiment analysis.</p>
<p>Compared to pre-training, fine-tuning is relatively inexpensive.</p>
<h1 id="GPT2"><a href="#GPT2" class="headerlink" title="GPT2"></a>GPT2</h1>
            


        </div>
    </div>
    <div id="post-footer" class="post-footer main-content-wrap">
        
            <div class="post-footer-tags">
                <span class="text-color-light text-small">TAGGED IN</span><br/>
                
    <a class="tag tag--primary tag--small t-none-link" href="/tags/NLP/" rel="tag">NLP</a>

            </div>
        
        
            <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2020/07/21/Big-Data-Interview/"
                    data-tooltip="Big Data Interview"
                    aria-label="PREVIOUS: Big Data Interview"
                >
                    
                        <i class="fa fa-angle-left" aria-hidden="true"></i>
                        <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                    </a>
            </li>
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2020/06/04/Leetcode/"
                    data-tooltip="Leetcode Top 100"
                    aria-label="NEXT: Leetcode Top 100"
                >
                    
                        <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                        <i class="fa fa-angle-right" aria-hidden="true"></i>
                    </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a
                class="post-action-btn btn btn--default btn-open-shareoptions"
                href="#btn-open-shareoptions"
                aria-label="Share this post"
            >
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://joddiy.github.io/2020/06/10/NLP-Summary/"
                    title="Share on Facebook"
                    aria-label="Share on Facebook"
                >
                    <i class="fab fa-facebook" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://twitter.com/intent/tweet?text=https://joddiy.github.io/2020/06/10/NLP-Summary/"
                    title="Share on Twitter"
                    aria-label="Share on Twitter"
                >
                    <i class="fab fa-twitter" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://plus.google.com/share?url=https://joddiy.github.io/2020/06/10/NLP-Summary/"
                    title="Share on Google+"
                    aria-label="Share on Google+"
                >
                    <i class="fab fa-google-plus" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
                <li class="post-action">
                    <a
                        class="post-action-btn btn btn--default"
                        href="#gitalk"
                        aria-label="Leave a comment"
                    >
                        <i class="fa fa-comment"></i>
                    </a>
                </li>
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


        
        
            
                <div id="gitalk"></div>

            
        
    </div>
</article>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2024 Joddiy Zhang. All Rights Reserved.
    </span>
</footer>

            </div>
            
                <div id="bottom-bar" class="post-bottom-bar" data-behavior="4">
                    <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2020/07/21/Big-Data-Interview/"
                    data-tooltip="Big Data Interview"
                    aria-label="PREVIOUS: Big Data Interview"
                >
                    
                        <i class="fa fa-angle-left" aria-hidden="true"></i>
                        <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                    </a>
            </li>
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2020/06/04/Leetcode/"
                    data-tooltip="Leetcode Top 100"
                    aria-label="NEXT: Leetcode Top 100"
                >
                    
                        <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                        <i class="fa fa-angle-right" aria-hidden="true"></i>
                    </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a
                class="post-action-btn btn btn--default btn-open-shareoptions"
                href="#btn-open-shareoptions"
                aria-label="Share this post"
            >
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://joddiy.github.io/2020/06/10/NLP-Summary/"
                    title="Share on Facebook"
                    aria-label="Share on Facebook"
                >
                    <i class="fab fa-facebook" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://twitter.com/intent/tweet?text=https://joddiy.github.io/2020/06/10/NLP-Summary/"
                    title="Share on Twitter"
                    aria-label="Share on Twitter"
                >
                    <i class="fab fa-twitter" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://plus.google.com/share?url=https://joddiy.github.io/2020/06/10/NLP-Summary/"
                    title="Share on Google+"
                    aria-label="Share on Google+"
                >
                    <i class="fab fa-google-plus" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
                <li class="post-action">
                    <a
                        class="post-action-btn btn btn--default"
                        href="#gitalk"
                        aria-label="Leave a comment"
                    >
                        <i class="fa fa-comment"></i>
                    </a>
                </li>
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


                </div>
                
    <div id="share-options-bar" class="share-options-bar" data-behavior="4">
        <i id="btn-close-shareoptions" class="fa fa-times"></i>
        <ul class="share-options">
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://www.facebook.com/sharer/sharer.php?u=https://joddiy.github.io/2020/06/10/NLP-Summary/"
                        aria-label="Share on Facebook"
                    >
                        <i class="fab fa-facebook" aria-hidden="true"></i><span>Share on Facebook</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://twitter.com/intent/tweet?text=https://joddiy.github.io/2020/06/10/NLP-Summary/"
                        aria-label="Share on Twitter"
                    >
                        <i class="fab fa-twitter" aria-hidden="true"></i><span>Share on Twitter</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://plus.google.com/share?url=https://joddiy.github.io/2020/06/10/NLP-Summary/"
                        aria-label="Share on Google+"
                    >
                        <i class="fab fa-google-plus" aria-hidden="true"></i><span>Share on Google+</span>
                    </a>
                </li>
            
        </ul>
    </div>


            
        </div>
        


    
        
    

<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-times"></i>
        </div>
        
            <img id="about-card-picture" src="/assets/images/14108933.jpeg" alt="Author&#39;s picture"/>
        
            <h4 id="about-card-name">Joddiy Zhang</h4>
        
            <div id="about-card-bio"><p><a href="mailto:&#106;&#111;&#100;&#x64;&#x69;&#x79;&#x7a;&#104;&#97;&#110;&#x67;&#64;&#x67;&#109;&#97;&#105;&#108;&#x2e;&#x63;&#111;&#109;">&#106;&#111;&#100;&#x64;&#x69;&#x79;&#x7a;&#104;&#97;&#110;&#x67;&#64;&#x67;&#109;&#97;&#105;&#108;&#x2e;&#x63;&#111;&#109;</a></p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>Machine Learning Engineer</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker-alt"></i>
                <br/>
                Singapore
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->

<script src="/assets/js/script-sqrh47zm5nkjgifq4rx38uvns4r2rarrrvwuhjxiztyrddruca5ukl7nw6br.min.js"></script>

<!--SCRIPTS END-->


    
        
<script src="/assets/js/gitalk.js"></script>

        <script type="text/javascript">
          (function() {
            new Gitalk({
              clientID: 'bee9685b2dc9739b6bd5',
              clientSecret: 'a0c683f383a94fae6d021ab932f37f7e56899410',
              repo: 'joddiy.github.io',
              owner: 'joddiy',
              admin: ['joddiy'],
              id: '2020/06/10/NLP-Summary/',
              ...{"language":"en","perPage":10,"distractionFreeMode":false,"enableHotKey":true,"pagerDirection":"first"}
            }).render('gitalk')
          })()
        </script>
    




    </body>
</html>
