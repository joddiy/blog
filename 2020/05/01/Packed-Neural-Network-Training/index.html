
<!DOCTYPE html>
<html lang="en">
    
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Jz Blog">
    <title>Understanding and Optimizing Packed Neural Network Training for Hyper-Parameter Tuning - Jz Blog</title>
    <meta name="author" content="Joddiy Zhang">
    
    
        <link rel="icon" href="https://joddiy.github.io/assets/images/favicon.ico">
    
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Joddiy Zhang","sameAs":["https://github.com/joddiy","https://scholar.google.com/citations?user=KH-xv38AAAAJ&hl=en&oi=sra","https://www.linkedin.com/in/joddiyzhang/"],"image":"14108933.jpeg"},"articleBody":"Understanding and Optimizing Packed Neural Network Training for Hyper-Parameter TuningSubjectThis paper studies jointly training multiple neural network models on a single GPU. Our research prototype is in TensorFlow, and we evaluate performance across different models (ResNet, MobileNet, DenseNet, and MLP) and training scenarios. \nContribution\npacking two models can bring up to 40% performance improvement over unpacked setups for a single training step and the improvement increases when packing more models; \nthe benefit of a pack primitive largely depends on a number of factors including memory capacity, chip architecture, neural network structure, and batch size; \nthere exists a trade-off between packing and unpacking when training multiple neural network models on limited resources; (4) a pack-based Hyperband is up to 2.7× faster than the original Hyperband training method in our experiment setting, with this improvement growing as memory size increases and subsequently the density of models packed.\n\nIntroductionBackground\nIt is increasingly the case that a diverse set of model training tasks share limited training resources. The long-running nature of these tasks and the large variation in their size and complexity makes efficient resource sharing a crucial concern. \nThese concerns are compounded by an extensive trial-and-error development process where parameters are tuned and architectures are tweaked that result in a large number of trial models to train.\n\nMulti-tenant traininggranularity of full devicesSuch a policy can result in poor resource utilization due to its coarse granularity. If a training workload consists of a large number of small neural networks, allocating entire devices to these training tasks is wasteful and significantly delays any large model training.\nfine-grained resource sharingUnlike CPUs, the full virtualization of GPU resources is very nascent. In this setting, those parallel kernels would transfer and store multiple copies of the same training data on the device. \nPack modelsA multiple static neural network architectures (e.g., ones that are typically used in Computer Vision) can be rewritten as a single concatenated network that preserves the input, output, and backpropagation semantics.\n\nfacilitate partitioning of a single device\nsynchronize data processing threads on GPUs\ncollapse common variables in the computation graph.\n\nIt is often the case during hyperparameter or neural architecture search that multiple similar networks are trained, and packed configurations can feed a single I&#x2F;O stream of training examples to each variant of the network. In contrast, an isolated sharing configuration may lead to duplicated work and wasted resources.\nConclusion\npacking models together is not strictly beneficial, that packing models that are too different can result in excessive blocking and wasted work.\nthis paper studies the range of possible improvements (and&#x2F;or overheads) for using packed configurations. \ndesign an algorithm that heuristically finds performant packing configurations\n\nBackgroundMotivation\nSuppose, we are training two models on the same dataset to test if a small tweak in network architecture will improve performance. If the system could pack together models when compatible in size, then these redundant data streams can be fused together to improve performance.\nBasic Framework APIWe desire a framework that can pack models together when possible and jointly optimize their respective computation graphs to reduce redundant work. \nEach training task is identified by four inputs.\n\nModel.\nOptimizer.\nBatch Size.\nSteps.\n\nSuch a system requires three basic primitives load, pack, and free. \n\nload1load(model, device)\nfree1checkpt = free(model, device)\npack1234output1 = nn1(input1)output2 = nn2(input2)==&gt;[output1 output2] = nn12([input1, input2])\n\nThis pack operation is fully differentiable and preserves the correctness semantics of the two original networks. The models can be jointly trained.\nIf the models are too different the device may waste effort stalling on one model.\nImplementationBasic Packing123resnet_out = ... #reference to resnet output mobilenet_out = ..#reference to mobilenet output packed_output = stack([resnet_out, mobilenet_out])\n\nIf one of the models is significantly more complex than the others, it will block progress.\n\nMisaligned Batch Sizes\n\nOur method is to rewrite the packed model to include a dummy operation that pads models with the smaller batch size to match the larger ones in dimension. \n\n\nMisaligned Step Counts\n\nwhen the earliest model finishes, the packed model has to be check-pointed and reconfigured to only include the remaining models. This introduces a switching overhead of loading and freeing the device memory.\nEliminating RedundancyDimensional differences between the models or training differences between the models can lead to wasted work. \nIn this case, input1 and input2 refer to the same dataset. We can avoid transferring the batch multiple times by symbolically rewriting the network description to refer to the same input:\n1234output1 = nn_relu(input1)output2 = nn_sigmoid(input2)==&gt;[output1 output2] = nn12([input, input])\n\nWhen packing models that use the same preprocessing tasks, the pack primitive fuse the steam processing accordingly.\nProfiling Model PackingAs it stands, model packing leads to the following tradeoffs. Potential performance improvements include: \n\nperforming computations (inference and backpropagation) in parallel if and when possible, \neliminating redundant data transfers when models are trained on the same dataset, \ncombining redudant computations including preprocessing.\n\nOn the other hand, the potential overheads include: \n\nmodels that dominate the device resources and block the progress of the others, \npadding misaligned batch sizes,\nloading and unloading models with a differing number of training steps.\n\nIn particular, we find that packing is most beneficial when the models jointly trained are similar and train on the same dataset.\nMetricswe define the improvement metric as follows:\n[IMPV]\n1234Ts(Seq) = Ts(Model 1)+···+Ts(Model n)Ts(Pack) = Ts(Pack(Model 1,··· ,n))IMPV = (Ts(Seq)−Ts(Pack))/Ts(Seq)\nThe switching overhead of training n models is defined as:\n[SwOH]\n1SwOH = Te(Seq)−Te(Model 1)···−Te(Model n)\n\nMicro-BenchmarksImprovement of packing models when increasing number of models and batch size on GPU.\n\nAblation StudyWe test more cases based on the five factors:\n\nThe best scenarios are where the same training data and same batch size are used. Over all the configurations, the pack primitive always brings benefits when we training models with same data or same batch size since it will reduces the data transfer. \n\nMemory UsageMLP-3 model maintains the same as batch size goes up. For convolutional neural networks like ResNet, MobileNet and DenseNet, the GPU memory usage is proportional to the batch size.\n\nSwitching OverheadsThe switching overhead is minor compared to the overall training time.\n\nPacking vs CUDA ParallelismAlthough CUDA supports it, our results show that it is an inefficient technique. We find that in all but the simplest cases lead to an out-of-memory error: “failed to allocateXXXfromdevice:CUDA_ERROR_OUT_OF_MEMORY: out of memory”. \nPacking-Aware Hyperparameter SearchHyperband for Hyperparameter SearchHyperband works by repeatedly sampling random parameter configurations, partially training models with those configurations and discarding those configurations that do not seem promising. \nHyperband is very effective for parallel hyperparameter search in comparison to sequential algorithms such as Bayesian Optimization.\nThe algorithm is long-running since it consists of a large number of trial models&#x2F;configurations to run and each of them will occupy the entire GPU resource when running.\nPacked HyperbandOur pack primitive allows Hyperband to jointly train configurations when possible thereby reducing the overall training time. \nThe optimization problem is to search over all packable partitions to find the best possible configuratio. We call this primitive pack_opt(), which solves the search problem to produce feasible packing configuration. \nHeuristic SolutionWe design a nearest-neighbor based heuristic. The method randomly selects a single configuration as the centroid and packs all other configurations similar to it until the device runs out of memory. \nFor calculating the similarity, we map hyperparameter configurations to multidimensional feature space and measure the pairwise Euclid distance among all the configurations. \nWe take standard distance unit as 1, and compute the distance between any two configurations. For categorical hyperparameters like optimizer and activation, the distance is 0 if same and 1 if different, for numeric hyperparameters, we use the index to compute distance. \n\nEvaluationA Random Pack Hyperband can save more time than Batch-size Pack Hyperband since it achieves a better GPU resource utilization. Our kNN strategy gets the best of both worlds: it finds the most beneficial packing opportunities while completely utilizing the available resources, and benefits are scalable when deployed in an environment with larger GPU resource.\n\nRelated WorkWe focus on related work that studies placement in machine learning and hyperparameter tuning systems.\nSystems for Hyperparameter Tuning\nGoogle Vizier [3] exposes hyperparamter search as a service to its organization’s data scientists. Aggressive “scale-out” has been the main design principle of Vizier and similar systems.\nA more controlled resource usage. CHOPT proposes a novel stop-and-go scheme that can take advantage of transient cloud based resources. \nHyperSched, proposes a scheduling framework for hyperparameter tuning tasks when there are contended specialized resources. And, some work has been done on resource management [33] and pipeline re-use [18] in the non-deep learning setting.\n\nCoarse-grained Multi-tenancydividing resources at the granularity of full devices\n\nOne of the earliest works in this space was the TetriSched project, which considered how to specify and optimize locality constraints during process scheduling.\nProject Philly analyzed a trace of machine learning workloads run on a large cluster of GPUs in Microsoft and indicated the trade-off between locality and GPU utilization.\n\nFine-grained Multi-tenancyThere are a number of projects which attempt to achieve the fine granularity. This idea called packing.\n\nGandiva is a cluster scheduling framework for deep learning jobs that provides primitives such as time-slicing and migration to schedule different jobs, it supports packing as well, implementing packing arrangements using a greedy algorithm without decent optimizations.\nNarayanan et al. [26] discuss a packing idea that tries to combine various models and trains them together.\n\nHowever, this proposed packing method of layer fusion has limited application scenarios as it only focuses on models with the same batch size.\nMulti-tenancy Systems\nCROSSBOW\nPipeDream\nEase.ml\n\nDiscussion and LimitationsAny distributed training and the regarding optimization is out of the scope of the paper. \n","dateCreated":"2020-05-01T00:00:00+08:00","dateModified":"2020-05-11T16:34:49+08:00","datePublished":"2020-05-01T00:00:00+08:00","description":"Study notes of paper https://arxiv.org/abs/2002.02885","headline":"Understanding and Optimizing Packed Neural Network Training for Hyper-Parameter Tuning","image":["https://res.cloudinary.com/dzarsr3se/image/upload/v1588964038/blog/Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning/1_mfq4g7.png","https://www.simplilearn.com/ice9/free_resources_article_thumb/Convolutional_Neural_Network_Tutorial.jpg"],"mainEntityOfPage":{"@type":"WebPage","@id":"https://joddiy.github.io/2020/05/01/Packed-Neural-Network-Training/"},"publisher":{"@type":"Organization","name":"Joddiy Zhang","sameAs":["https://github.com/joddiy","https://scholar.google.com/citations?user=KH-xv38AAAAJ&hl=en&oi=sra","https://www.linkedin.com/in/joddiyzhang/"],"image":"14108933.jpeg","logo":{"@type":"ImageObject","url":"14108933.jpeg"}},"url":"https://joddiy.github.io/2020/05/01/Packed-Neural-Network-Training/","keywords":"Deep Learning, Hyper-Parameter Tuning, Graph Optimization","thumbnailUrl":"https://res.cloudinary.com/dzarsr3se/image/upload/v1588964038/blog/Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning/1_mfq4g7.png"}</script>
    <meta name="description" content="Study notes of paper https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2002.02885">
<meta property="og:type" content="blog">
<meta property="og:title" content="Understanding and Optimizing Packed Neural Network Training for Hyper-Parameter Tuning">
<meta property="og:url" content="https://joddiy.github.io/2020/05/01/Packed-Neural-Network-Training/index.html">
<meta property="og:site_name" content="Jz Blog">
<meta property="og:description" content="Study notes of paper https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2002.02885">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://res.cloudinary.com/dzarsr3se/image/upload/v1588964038/blog/Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning/1_mfq4g7.png">
<meta property="og:image" content="https://res.cloudinary.com/dzarsr3se/image/upload/v1588964039/blog/Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning/2_yjjdme.png">
<meta property="og:image" content="https://res.cloudinary.com/dzarsr3se/image/upload/v1588964038/blog/Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning/3_mozcet.png">
<meta property="og:image" content="https://res.cloudinary.com/dzarsr3se/image/upload/v1588964038/blog/Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning/4_aeswjs.png">
<meta property="og:image" content="https://res.cloudinary.com/dzarsr3se/image/upload/v1588964038/blog/Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning/T1_u9m0tn.png">
<meta property="og:image" content="https://res.cloudinary.com/dzarsr3se/image/upload/v1588964038/blog/Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning/5_a4nf4v.png">
<meta property="og:image" content="https://res.cloudinary.com/dzarsr3se/image/upload/v1588964038/blog/Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning/6_otfjyk.png">
<meta property="og:image" content="https://res.cloudinary.com/dzarsr3se/image/upload/v1588964037/blog/Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning/T2_yivti7.png">
<meta property="og:image" content="https://res.cloudinary.com/dzarsr3se/image/upload/v1588964037/blog/Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning/T3_efcra4.png">
<meta property="og:image" content="https://res.cloudinary.com/dzarsr3se/image/upload/v1588964037/blog/Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning/T4_gzjwyr.png">
<meta property="article:published_time" content="2020-04-30T16:00:00.000Z">
<meta property="article:modified_time" content="2020-05-11T08:34:49.000Z">
<meta property="article:author" content="Joddiy Zhang">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="Hyper-Parameter Tuning">
<meta property="article:tag" content="Graph Optimization">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://res.cloudinary.com/dzarsr3se/image/upload/v1588964038/blog/Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning/1_mfq4g7.png">
    
    
        
    
    
        <meta property="og:image" content="https://joddiy.github.io/assets/images/14108933.jpeg"/>
    
    
        <meta property="og:image" content="https://res.cloudinary.com/dzarsr3se/image/upload/v1588964038/blog/Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning/1_mfq4g7.png"/>
        <meta class="swiftype" name="image" data-type="enum" content="https://res.cloudinary.com/dzarsr3se/image/upload/v1588964038/blog/Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning/1_mfq4g7.png"/>
    
    
        <meta property="og:image" content="https://www.simplilearn.com/ice9/free_resources_article_thumb/Convolutional_Neural_Network_Tutorial.jpg"/>
        <meta class="swiftype" name="image" data-type="enum" content="https://www.simplilearn.com/ice9/free_resources_article_thumb/Convolutional_Neural_Network_Tutorial.jpg"/>
    
    
    <!--STYLES-->
    
<link rel="stylesheet" href="/assets/css/style-x8blglznjjnb9pnnwui5zw4h43ysufmsh1b0omicawm4vhqcutzqavokgpne.min.css">

    <!--STYLES END-->
    

    

    
        
            
<link rel="stylesheet" href="/assets/css/gitalk.css">

        
    
</head>

    <body>
        <div id="blog">
            <!-- Define author's picture -->


    
        
            
        
    

<header id="header" data-behavior="4">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a
            class="header-title-link"
            href="/"
            aria-label=""
        >
            Jz Blog
        </a>
    </div>
    
        
            <a
                class="header-right-picture "
                href="#about"
                aria-label="Open the link: /#about"
            >
        
        
            <img class="header-picture" src="/assets/images/14108933.jpeg" alt="Author&#39;s picture"/>
        
        </a>
    
</header>

            <!-- Define author's picture -->



        
    

<nav id="sidebar" data-behavior="4">
    <div class="sidebar-container">
        
            <div class="sidebar-profile">
                <a
                    href="/#about"
                    aria-label="Read more about the author"
                >
                    <img class="sidebar-profile-picture" src="/assets/images/14108933.jpeg" alt="Author&#39;s picture"/>
                </a>
                <h4 class="sidebar-profile-name">Joddiy Zhang</h4>
                
                    <h5 class="sidebar-profile-bio"><p><a href="mailto:&#x6a;&#x6f;&#x64;&#x64;&#105;&#x79;&#x7a;&#104;&#x61;&#x6e;&#x67;&#64;&#x67;&#109;&#x61;&#x69;&#x6c;&#x2e;&#x63;&#111;&#109;">&#x6a;&#x6f;&#x64;&#x64;&#105;&#x79;&#x7a;&#104;&#x61;&#x6e;&#x67;&#64;&#x67;&#109;&#x61;&#x69;&#x6c;&#x2e;&#x63;&#111;&#109;</a></p>
</h5>
                
            </div>
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/"
                            
                            rel="noopener"
                            title="Home"
                        >
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-categories"
                            
                            rel="noopener"
                            title="Categories"
                        >
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-tags"
                            
                            rel="noopener"
                            title="Tags"
                        >
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Tags</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-archives"
                            
                            rel="noopener"
                            title="Archives"
                        >
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="#about"
                            
                            rel="noopener"
                            title="About"
                        >
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://github.com/joddiy"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="GitHub"
                        >
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://scholar.google.com/citations?user=KH-xv38AAAAJ&hl=en&oi=sra"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="Google Scholar"
                        >
                        <i class="sidebar-button-icon fab fa-google" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Google Scholar</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://www.linkedin.com/in/joddiyzhang/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="LinkedIn"
                        >
                        <i class="sidebar-button-icon fab fa-linkedin" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">LinkedIn</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
        <div class="post-header-cover
                    text-center
                    post-header-cover--partial"
             style="background-image:url('https://www.simplilearn.com/ice9/free_resources_article_thumb/Convolutional_Neural_Network_Tutorial.jpg');"
             data-behavior="4">
            
                <div class="post-header main-content-wrap text-center">
    
        <h1 class="post-title">
            Understanding and Optimizing Packed Neural Network Training for Hyper-Parameter Tuning
        </h1>
    
    
        <div class="post-meta">
    <time datetime="2020-05-01T00:00:00+08:00">
	
		    May 01, 2020
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Research/">Research</a>


    
</div>

    
</div>

            
        </div>

            <div id="main" data-behavior="4"
                 class="hasCover
                        hasCoverMetaIn
                        ">
                
<article class="post">
    
    
    <div class="post-content markdown">
        <div class="main-content-wrap">
            <h1 id="Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning"><a href="#Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning" class="headerlink" title="Understanding and Optimizing Packed Neural Network Training for Hyper-Parameter Tuning"></a>Understanding and Optimizing Packed Neural Network Training for Hyper-Parameter Tuning</h1><h2 id="Subject"><a href="#Subject" class="headerlink" title="Subject"></a>Subject</h2><p>This paper studies jointly training multiple neural network models on a single GPU. Our research prototype is in TensorFlow, and we evaluate performance across different models (ResNet, MobileNet, DenseNet, and MLP) and training scenarios. </p>
<h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><ol>
<li>packing two models can bring up to 40% performance improvement over unpacked setups for a single training step and the improvement increases when packing more models; </li>
<li>the benefit of a pack primitive largely depends on a number of factors including memory capacity, chip architecture, neural network structure, and batch size; </li>
<li>there exists a trade-off between packing and unpacking when training multiple neural network models on limited resources; (4) a pack-based Hyperband is up to 2.7× faster than the original Hyperband training method in our experiment setting, with this improvement growing as memory size increases and subsequently the density of models packed.</li>
</ol>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><ol>
<li>It is increasingly the case that a diverse set of model training tasks share limited training resources. The long-running nature of these tasks and the large variation in their size and complexity makes efficient resource sharing a crucial concern. </li>
<li>These concerns are compounded by an extensive trial-and-error development process where parameters are tuned and architectures are tweaked that result in a large number of trial models to train.</li>
</ol>
<h3 id="Multi-tenant-training"><a href="#Multi-tenant-training" class="headerlink" title="Multi-tenant training"></a>Multi-tenant training</h3><h4 id="granularity-of-full-devices"><a href="#granularity-of-full-devices" class="headerlink" title="granularity of full devices"></a>granularity of full devices</h4><p>Such a policy can result in poor resource utilization due to its coarse granularity. If a training workload consists of a large number of small neural networks, allocating entire devices to these training tasks is wasteful and significantly delays any large model training.</p>
<h4 id="fine-grained-resource-sharing"><a href="#fine-grained-resource-sharing" class="headerlink" title="fine-grained resource sharing"></a>fine-grained resource sharing</h4><p>Unlike CPUs, the full virtualization of GPU resources is very nascent. In this setting, those parallel kernels would transfer and store multiple copies of the same training data on the device. </p>
<h3 id="Pack-models"><a href="#Pack-models" class="headerlink" title="Pack models"></a>Pack models</h3><p>A multiple static neural network architectures (e.g., ones that are typically used in Computer Vision) can be rewritten as a single concatenated network that preserves the input, output, and backpropagation semantics.</p>
<ol>
<li>facilitate partitioning of a single device</li>
<li>synchronize data processing threads on GPUs</li>
<li>collapse common variables in the computation graph.</li>
</ol>
<p>It is often the case during hyperparameter or neural architecture search that multiple similar networks are trained, and packed configurations can feed a single I&#x2F;O stream of training examples to each variant of the network. In contrast, an isolated sharing configuration may lead to duplicated work and wasted resources.</p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><ol>
<li>packing models together is not strictly beneficial, that packing models that are too different can result in excessive blocking and wasted work.</li>
<li>this paper studies the range of possible improvements (and&#x2F;or overheads) for using packed configurations. </li>
<li>design an algorithm that heuristically finds performant packing configurations</li>
</ol>
<h2 id="Background-1"><a href="#Background-1" class="headerlink" title="Background"></a>Background</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p><img src="https://res.cloudinary.com/dzarsr3se/image/upload/v1588964038/blog/Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning/1_mfq4g7.png" alt="P1"></p>
<p>Suppose, we are training two models on the same dataset to test if a small tweak in network architecture will improve performance. If the system could pack together models when compatible in size, then these redundant data streams can be fused together to improve performance.</p>
<h3 id="Basic-Framework-API"><a href="#Basic-Framework-API" class="headerlink" title="Basic Framework API"></a>Basic Framework API</h3><p>We desire a framework that can pack models together when possible and jointly optimize their respective computation graphs to reduce redundant work. </p>
<p>Each training task is identified by four inputs.</p>
<ol>
<li>Model.</li>
<li>Optimizer.</li>
<li>Batch Size.</li>
<li>Steps.</li>
</ol>
<p>Such a system requires three basic primitives load, pack, and free. </p>
<ol>
<li>load<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load(model, device)</span><br></pre></td></tr></table></figure></li>
<li>free<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">checkpt = free(model, device)</span><br></pre></td></tr></table></figure></li>
<li>pack<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">output1 = nn1(input1)</span><br><span class="line">output2 = nn2(input2)</span><br><span class="line">==&gt;</span><br><span class="line">[output1 output2] = nn12([input1, input2])</span><br></pre></td></tr></table></figure></li>
</ol>
<p>This pack operation is fully differentiable and preserves the correctness semantics of the two original networks. The models can be jointly trained.</p>
<p>If the models are too different the device may waste effort stalling on one model.</p>
<h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><h3 id="Basic-Packing"><a href="#Basic-Packing" class="headerlink" title="Basic Packing"></a>Basic Packing</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">resnet_out = ... #reference to resnet output </span><br><span class="line">mobilenet_out = ..#reference to mobilenet output </span><br><span class="line">packed_output = stack([resnet_out, mobilenet_out])</span><br></pre></td></tr></table></figure>
<p><img src="https://res.cloudinary.com/dzarsr3se/image/upload/v1588964039/blog/Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning/2_yjjdme.png" alt="P2"></p>
<p>If one of the models is significantly more complex than the others, it will block progress.</p>
<ol>
<li>Misaligned Batch Sizes</li>
</ol>
<p>Our method is to rewrite the packed model to include a dummy operation that pads models with the smaller batch size to match the larger ones in dimension. </p>
<p><img src="https://res.cloudinary.com/dzarsr3se/image/upload/v1588964038/blog/Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning/3_mozcet.png" alt="P3"></p>
<ol start="2">
<li>Misaligned Step Counts</li>
</ol>
<p>when the earliest model finishes, the packed model has to be check-pointed and reconfigured to only include the remaining models. This introduces a switching overhead of loading and freeing the device memory.</p>
<h3 id="Eliminating-Redundancy"><a href="#Eliminating-Redundancy" class="headerlink" title="Eliminating Redundancy"></a>Eliminating Redundancy</h3><p>Dimensional differences between the models or training differences between the models can lead to wasted work. </p>
<p>In this case, input1 and input2 refer to the same dataset. We can avoid transferring the batch multiple times by symbolically rewriting the network description to refer to the same input:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">output1 = nn_relu(input1)</span><br><span class="line">output2 = nn_sigmoid(input2)</span><br><span class="line">==&gt;</span><br><span class="line">[output1 output2] = nn12([input, input])</span><br></pre></td></tr></table></figure>

<p>When packing models that use the same preprocessing tasks, the pack primitive fuse the steam processing accordingly.</p>
<h2 id="Profiling-Model-Packing"><a href="#Profiling-Model-Packing" class="headerlink" title="Profiling Model Packing"></a>Profiling Model Packing</h2><p>As it stands, model packing leads to the following tradeoffs. Potential performance improvements include: </p>
<ol>
<li>performing computations (inference and backpropagation) in parallel if and when possible, </li>
<li>eliminating redundant data transfers when models are trained on the same dataset, </li>
<li>combining redudant computations including preprocessing.</li>
</ol>
<p>On the other hand, the potential overheads include: </p>
<ol>
<li>models that dominate the device resources and block the progress of the others, </li>
<li>padding misaligned batch sizes,</li>
<li>loading and unloading models with a differing number of training steps.</li>
</ol>
<p>In particular, we find that packing is most beneficial when the models jointly trained are similar and train on the same dataset.</p>
<h3 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h3><p>we define the improvement metric as follows:</p>
<p>[IMPV]</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Ts(Seq) = Ts(Model 1)+···+Ts(Model n)</span><br><span class="line">Ts(Pack) = Ts(Pack(Model 1,··· ,n))</span><br><span class="line"></span><br><span class="line">IMPV = (Ts(Seq)−Ts(Pack))/Ts(Seq)</span><br></pre></td></tr></table></figure>
<p>The switching overhead of training n models is defined as:</p>
<p>[SwOH]</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SwOH = Te(Seq)−Te(Model 1)···−Te(Model n)</span><br></pre></td></tr></table></figure>

<h3 id="Micro-Benchmarks"><a href="#Micro-Benchmarks" class="headerlink" title="Micro-Benchmarks"></a>Micro-Benchmarks</h3><p>Improvement of packing models when increasing number of models and batch size on GPU.</p>
<p><img src="https://res.cloudinary.com/dzarsr3se/image/upload/v1588964038/blog/Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning/4_aeswjs.png" alt="P4"></p>
<h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>We test more cases based on the five factors:</p>
<p><img src="https://res.cloudinary.com/dzarsr3se/image/upload/v1588964038/blog/Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning/T1_u9m0tn.png" alt="T1"></p>
<p>The best scenarios are where the same training data and same batch size are used. Over all the configurations, the pack primitive always brings benefits when we training models with same data or same batch size since it will reduces the data transfer. </p>
<p><img src="https://res.cloudinary.com/dzarsr3se/image/upload/v1588964038/blog/Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning/5_a4nf4v.png" alt="P5"></p>
<h3 id="Memory-Usage"><a href="#Memory-Usage" class="headerlink" title="Memory Usage"></a>Memory Usage</h3><p>MLP-3 model maintains the same as batch size goes up. For convolutional neural networks like ResNet, MobileNet and DenseNet, the GPU memory usage is proportional to the batch size.</p>
<p><img src="https://res.cloudinary.com/dzarsr3se/image/upload/v1588964038/blog/Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning/6_otfjyk.png" alt="P6"></p>
<h3 id="Switching-Overheads"><a href="#Switching-Overheads" class="headerlink" title="Switching Overheads"></a>Switching Overheads</h3><p>The switching overhead is minor compared to the overall training time.</p>
<p><img src="https://res.cloudinary.com/dzarsr3se/image/upload/v1588964037/blog/Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning/T2_yivti7.png" alt="T2"></p>
<h3 id="Packing-vs-CUDA-Parallelism"><a href="#Packing-vs-CUDA-Parallelism" class="headerlink" title="Packing vs CUDA Parallelism"></a>Packing vs CUDA Parallelism</h3><p>Although CUDA supports it, our results show that it is an inefficient technique. We find that in all but the simplest cases lead to an out-of-memory error: “failed to allocateXXXfromdevice:CUDA_ERROR_OUT_OF_MEMORY: out of memory”. </p>
<h2 id="Packing-Aware-Hyperparameter-Search"><a href="#Packing-Aware-Hyperparameter-Search" class="headerlink" title="Packing-Aware Hyperparameter Search"></a>Packing-Aware Hyperparameter Search</h2><h3 id="Hyperband-for-Hyperparameter-Search"><a href="#Hyperband-for-Hyperparameter-Search" class="headerlink" title="Hyperband for Hyperparameter Search"></a>Hyperband for Hyperparameter Search</h3><p>Hyperband works by repeatedly sampling random parameter configurations, partially training models with those configurations and discarding those configurations that do not seem promising. </p>
<p>Hyperband is very effective for parallel hyperparameter search in comparison to sequential algorithms such as Bayesian Optimization.</p>
<p>The algorithm is long-running since it consists of a large number of trial models&#x2F;configurations to run and each of them will occupy the entire GPU resource when running.</p>
<h3 id="Packed-Hyperband"><a href="#Packed-Hyperband" class="headerlink" title="Packed Hyperband"></a>Packed Hyperband</h3><p>Our pack primitive allows Hyperband to jointly train configurations when possible thereby reducing the overall training time. </p>
<p>The optimization problem is to search over all packable partitions to find the best possible configuratio. We call this primitive pack_opt(), which solves the search problem to produce feasible packing configuration. </p>
<h3 id="Heuristic-Solution"><a href="#Heuristic-Solution" class="headerlink" title="Heuristic Solution"></a>Heuristic Solution</h3><p>We design a nearest-neighbor based heuristic. The method randomly selects a single configuration as the centroid and packs all other configurations similar to it until the device runs out of memory. </p>
<p>For calculating the similarity, we map hyperparameter configurations to multidimensional feature space and measure the pairwise Euclid distance among all the configurations. </p>
<p>We take standard distance unit as 1, and compute the distance between any two configurations. For categorical hyperparameters like optimizer and activation, the distance is 0 if same and 1 if different, for numeric hyperparameters, we use the index to compute distance. </p>
<p><img src="https://res.cloudinary.com/dzarsr3se/image/upload/v1588964037/blog/Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning/T3_efcra4.png" alt="T3"></p>
<h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h3><p>A Random Pack Hyperband can save more time than Batch-size Pack Hyperband since it achieves a better GPU resource utilization. Our kNN strategy gets the best of both worlds: it finds the most beneficial packing opportunities while completely utilizing the available resources, and benefits are scalable when deployed in an environment with larger GPU resource.</p>
<p><img src="https://res.cloudinary.com/dzarsr3se/image/upload/v1588964037/blog/Understanding-and-Optimizing-Packed-Neural-Network-Training-for-Hyper-Parameter-Tuning/T4_gzjwyr.png" alt="T4"></p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>We focus on related work that studies placement in machine learning and hyperparameter tuning systems.</p>
<h3 id="Systems-for-Hyperparameter-Tuning"><a href="#Systems-for-Hyperparameter-Tuning" class="headerlink" title="Systems for Hyperparameter Tuning"></a>Systems for Hyperparameter Tuning</h3><ol>
<li>Google Vizier [3] exposes hyperparamter search as a service to its organization’s data scientists. Aggressive “scale-out” has been the main design principle of Vizier and similar systems.</li>
<li>A more controlled resource usage. CHOPT proposes a novel stop-and-go scheme that can take advantage of transient cloud based resources. </li>
<li>HyperSched, proposes a scheduling framework for hyperparameter tuning tasks when there are contended specialized resources. And, some work has been done on resource management [33] and pipeline re-use [18] in the non-deep learning setting.</li>
</ol>
<h3 id="Coarse-grained-Multi-tenancy"><a href="#Coarse-grained-Multi-tenancy" class="headerlink" title="Coarse-grained Multi-tenancy"></a>Coarse-grained Multi-tenancy</h3><p>dividing resources at the granularity of full devices</p>
<ol>
<li>One of the earliest works in this space was the TetriSched project, which considered how to specify and optimize locality constraints during process scheduling.</li>
<li>Project Philly analyzed a trace of machine learning workloads run on a large cluster of GPUs in Microsoft and indicated the trade-off between locality and GPU utilization.</li>
</ol>
<h3 id="Fine-grained-Multi-tenancy"><a href="#Fine-grained-Multi-tenancy" class="headerlink" title="Fine-grained Multi-tenancy"></a>Fine-grained Multi-tenancy</h3><p>There are a number of projects which attempt to achieve the fine granularity. This idea called packing.</p>
<ol>
<li>Gandiva is a cluster scheduling framework for deep learning jobs that provides primitives such as time-slicing and migration to schedule different jobs, it supports packing as well, implementing packing arrangements using a greedy algorithm without decent optimizations.</li>
<li>Narayanan et al. [26] discuss a packing idea that tries to combine various models and trains them together.</li>
</ol>
<p>However, this proposed packing method of layer fusion has limited application scenarios as it only focuses on models with the same batch size.</p>
<h3 id="Multi-tenancy-Systems"><a href="#Multi-tenancy-Systems" class="headerlink" title="Multi-tenancy Systems"></a>Multi-tenancy Systems</h3><ol>
<li>CROSSBOW</li>
<li>PipeDream</li>
<li>Ease.ml</li>
</ol>
<h2 id="Discussion-and-Limitations"><a href="#Discussion-and-Limitations" class="headerlink" title="Discussion and Limitations"></a>Discussion and Limitations</h2><p>Any distributed training and the regarding optimization is out of the scope of the paper. </p>

            


        </div>
    </div>
    <div id="post-footer" class="post-footer main-content-wrap">
        
            <div class="post-footer-tags">
                <span class="text-color-light text-small">TAGGED IN</span><br/>
                
    <a class="tag tag--primary tag--small t-none-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a> <a class="tag tag--primary tag--small t-none-link" href="/tags/Graph-Optimization/" rel="tag">Graph Optimization</a> <a class="tag tag--primary tag--small t-none-link" href="/tags/Hyper-Parameter-Tuning/" rel="tag">Hyper-Parameter Tuning</a>

            </div>
        
        
            <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2020/05/07/TF2-0-Function/"
                    data-tooltip="TensorFlow 2.0 Functions, not Sessions."
                    aria-label="PREVIOUS: TensorFlow 2.0 Functions, not Sessions."
                >
                    
                        <i class="fa fa-angle-left" aria-hidden="true"></i>
                        <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                    </a>
            </li>
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2018/12/18/dynamic-malware-research/"
                    data-tooltip="Dynamic Malware Detection Research"
                    aria-label="NEXT: Dynamic Malware Detection Research"
                >
                    
                        <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                        <i class="fa fa-angle-right" aria-hidden="true"></i>
                    </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a
                class="post-action-btn btn btn--default btn-open-shareoptions"
                href="#btn-open-shareoptions"
                aria-label="Share this post"
            >
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://joddiy.github.io/2020/05/01/Packed-Neural-Network-Training/"
                    title="Share on Facebook"
                    aria-label="Share on Facebook"
                >
                    <i class="fab fa-facebook" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://twitter.com/intent/tweet?text=https://joddiy.github.io/2020/05/01/Packed-Neural-Network-Training/"
                    title="Share on Twitter"
                    aria-label="Share on Twitter"
                >
                    <i class="fab fa-twitter" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://plus.google.com/share?url=https://joddiy.github.io/2020/05/01/Packed-Neural-Network-Training/"
                    title="Share on Google+"
                    aria-label="Share on Google+"
                >
                    <i class="fab fa-google-plus" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
                <li class="post-action">
                    <a
                        class="post-action-btn btn btn--default"
                        href="#gitalk"
                        aria-label="Leave a comment"
                    >
                        <i class="fa fa-comment"></i>
                    </a>
                </li>
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


        
        
            
                <div id="gitalk"></div>

            
        
    </div>
</article>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2024 Joddiy Zhang. All Rights Reserved.
    </span>
</footer>

            </div>
            
                <div id="bottom-bar" class="post-bottom-bar" data-behavior="4">
                    <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2020/05/07/TF2-0-Function/"
                    data-tooltip="TensorFlow 2.0 Functions, not Sessions."
                    aria-label="PREVIOUS: TensorFlow 2.0 Functions, not Sessions."
                >
                    
                        <i class="fa fa-angle-left" aria-hidden="true"></i>
                        <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                    </a>
            </li>
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/2018/12/18/dynamic-malware-research/"
                    data-tooltip="Dynamic Malware Detection Research"
                    aria-label="NEXT: Dynamic Malware Detection Research"
                >
                    
                        <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                        <i class="fa fa-angle-right" aria-hidden="true"></i>
                    </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a
                class="post-action-btn btn btn--default btn-open-shareoptions"
                href="#btn-open-shareoptions"
                aria-label="Share this post"
            >
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://joddiy.github.io/2020/05/01/Packed-Neural-Network-Training/"
                    title="Share on Facebook"
                    aria-label="Share on Facebook"
                >
                    <i class="fab fa-facebook" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://twitter.com/intent/tweet?text=https://joddiy.github.io/2020/05/01/Packed-Neural-Network-Training/"
                    title="Share on Twitter"
                    aria-label="Share on Twitter"
                >
                    <i class="fab fa-twitter" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://plus.google.com/share?url=https://joddiy.github.io/2020/05/01/Packed-Neural-Network-Training/"
                    title="Share on Google+"
                    aria-label="Share on Google+"
                >
                    <i class="fab fa-google-plus" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
                <li class="post-action">
                    <a
                        class="post-action-btn btn btn--default"
                        href="#gitalk"
                        aria-label="Leave a comment"
                    >
                        <i class="fa fa-comment"></i>
                    </a>
                </li>
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


                </div>
                
    <div id="share-options-bar" class="share-options-bar" data-behavior="4">
        <i id="btn-close-shareoptions" class="fa fa-times"></i>
        <ul class="share-options">
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://www.facebook.com/sharer/sharer.php?u=https://joddiy.github.io/2020/05/01/Packed-Neural-Network-Training/"
                        aria-label="Share on Facebook"
                    >
                        <i class="fab fa-facebook" aria-hidden="true"></i><span>Share on Facebook</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://twitter.com/intent/tweet?text=https://joddiy.github.io/2020/05/01/Packed-Neural-Network-Training/"
                        aria-label="Share on Twitter"
                    >
                        <i class="fab fa-twitter" aria-hidden="true"></i><span>Share on Twitter</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://plus.google.com/share?url=https://joddiy.github.io/2020/05/01/Packed-Neural-Network-Training/"
                        aria-label="Share on Google+"
                    >
                        <i class="fab fa-google-plus" aria-hidden="true"></i><span>Share on Google+</span>
                    </a>
                </li>
            
        </ul>
    </div>


            
        </div>
        


    
        
    

<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-times"></i>
        </div>
        
            <img id="about-card-picture" src="/assets/images/14108933.jpeg" alt="Author&#39;s picture"/>
        
            <h4 id="about-card-name">Joddiy Zhang</h4>
        
            <div id="about-card-bio"><p><a href="mailto:&#x6a;&#111;&#100;&#x64;&#x69;&#121;&#x7a;&#104;&#97;&#x6e;&#103;&#x40;&#x67;&#x6d;&#97;&#105;&#x6c;&#x2e;&#x63;&#x6f;&#109;">&#x6a;&#111;&#100;&#x64;&#x69;&#121;&#x7a;&#104;&#97;&#x6e;&#103;&#x40;&#x67;&#x6d;&#97;&#105;&#x6c;&#x2e;&#x63;&#x6f;&#109;</a></p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>Machine Learning Engineer</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker-alt"></i>
                <br/>
                Singapore
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->

<script src="/assets/js/script-sqrh47zm5nkjgifq4rx38uvns4r2rarrrvwuhjxiztyrddruca5ukl7nw6br.min.js"></script>

<!--SCRIPTS END-->


    
        
<script src="/assets/js/gitalk.js"></script>

        <script type="text/javascript">
          (function() {
            new Gitalk({
              clientID: 'bee9685b2dc9739b6bd5',
              clientSecret: 'a0c683f383a94fae6d021ab932f37f7e56899410',
              repo: 'joddiy.github.io',
              owner: 'joddiy',
              admin: ['joddiy'],
              id: '2020/05/01/Packed-Neural-Network-Training/',
              ...{"language":"en","perPage":10,"distractionFreeMode":false,"enableHotKey":true,"pagerDirection":"first"}
            }).render('gitalk')
          })()
        </script>
    




    </body>
</html>
